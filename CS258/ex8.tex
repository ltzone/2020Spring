%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%                 %%%%%%%%%%%%
%%%%%%%%%%%%%    EXERCISE 1   %%%%%%%%%%%%
%%%%%%%%%%%%%                 %%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{exercise}[Differential entropy]{Evaluate the differential entropy $h(X)=$ $-\int f \ln f$ for the following:
  \begin{enumerate}
    \item The exponential density, $f(x)=\lambda e^{-\lambda x}, x \geq 0$
    \item The Laplace density, $f(x)=\frac{1}{2} \lambda e^{-\lambda|x|}$
    \item The sum of $X_{1}$ and $X_{2},$ where $X_{1}$ and $X_{2}$ are  independent normal random variables with means $\mu_{i}$ and variances $\sigma_{i}^{2}, i= 1,2$
  \end{enumerate}}
  \begin{solution}
  \par{~}
  \begin{enumerate}
    \item \begin{equation}
      \begin{aligned}
        h(X) &= -\int _{0}^{\infty} \lambda e^{-\lambda x} \ln \lambda e^{-\lambda x} dx\\
        &= -\int _{0}^{\infty} \lambda e^{-\lambda x} (\ln \lambda  - \lambda x) dx\\
        &= \ln \lambda \int _{0}^{\infty} e^{-\lambda x}   d (- \lambda x) -  \lambda  \int _{0}^{\infty} x  d  e^{-\lambda x} \\
        &= \ln \lambda e^{-\lambda x}  \bigg| _{0} ^{\infty} - \lambda x e^{-\lambda x} \bigg| _{0} ^{\infty} - \int_{0}^{\infty} e^{-\lambda x} d (-\lambda x) \\
        &= - \ln \lambda + 1 = \ln \frac{e}{\lambda}
      \end{aligned}
    \end{equation}
    \item \begin{equation}
      \begin{aligned}
        h(X) &= - \int_{-\infty}^{+\infty} \frac{1}{2} \lambda e^{-\lambda |x|} \ln \frac{1}{2} \lambda e^{-\lambda |x|} dx \\
        &= - 2 \int_{0}^{+\infty} \frac{1}{2}\lambda e^{-\lambda x} \ln \frac{1}{2}\lambda e^{-\lambda x}  dx \\
        &= - \int_{0}^{+\infty} \lambda e^{-\lambda x} \ln \lambda e^{-\lambda x} dx +  \ln 2 \int_{0}^{\infty} \lambda e^{-\lambda x} dx \\
        &= \ln \frac{e}{\lambda} - \ln 2 \cdot e^{-\lambda x} \bigg| ^{\infty}_{0} \\
        &= \ln \frac{2e}{\lambda}
      \end{aligned}
    \end{equation}
    \item By condition we know that
    \begin{equation}
      X_1, X_2 \sim \mathcal{N}\left(
        \left[\begin{array}{cc}
          \mu_1 & \mu_2 
        \end{array}\right], 
        \left[\begin{array}{cc}
          \sigma_1^{2} & 0 \\
           0 & \sigma_2^{2}
        \end{array}\right]\right)
    \end{equation}
    Since $X_1 + X_2 = \left[\begin{array}
      {cc} X_1 & X_2
    \end{array}\right] \left[\begin{array}
      {c} 1 \\ 1
    \end{array}\right]$, it follows that $X_1 + X_2 \sim \mathcal{N} (\mu_1 + \mu_2, \sigma_{1}^{2} + \sigma_{2}^{2})$.
    \begin{equation}
      \begin{aligned}
        f(x) &= \frac{1}{\sqrt{2 \pi (\sigma_{1}^{2} + \sigma_{2}^{2})}} e^{-\frac{(x-(\mu_1 + \mu_2))^{2}}{2 (\sigma_{1}^{2} + \sigma_{2}^{2})}} \\        
        h(X)&=-\int f(x) \log f(x) d x \\
        &=-\int f(x) \log \frac{1}{\sqrt{2 \pi (\sigma_{1}^{2} + \sigma_{2}^{2})}}+f(x)\left(-\frac{(x-(\mu_1 + \mu_2))^{2}}{2 (\sigma_{1}^{2} + \sigma_{2}^{2})}\right) d x         
        \end{aligned}      
    \end{equation}
    By property of normal distribution, we have
    \begin{equation}
      \int f(x) d x=1 \text { and } \int(x-(\mu_1 + \mu_2))^{2} f(x) d x=\sigma_{1}^{2} + \sigma_{2}^{2}
    \end{equation}
    Hence, 
    \begin{equation}
      h(X) = \frac{1}{2} \log 2\pi (\sigma_{1}^{2}+\sigma_{2}^{2}) + \frac{1}{2}
    \end{equation}
  \end{enumerate}
  \end{solution}
  \label{ex8-1}
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%                 %%%%%%%%%%%%
%%%%%%%%%%%%%    EXERCISE 2   %%%%%%%%%%%%
%%%%%%%%%%%%%                 %%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{exercise}[Concavity of determinants]{ Let $K_{1}$ and $K_{2}$ be two symmetric nonnegative definite $n \times n$ matrices. Prove:
  $$
  \left|\lambda K_{1}+\bar{\lambda} K_{2}\right| \geq\left|K_{1}\right|^{\lambda}\left|K_{2}\right|^{\bar{\lambda}} \quad \text { for } 0 \leq \lambda \leq 1, \quad \bar{\lambda}=1-\lambda
  $$
  where $|K|$ denotes the determinant of $K .$ [Hint: Let $\mathbf{Z}=\mathbf{X}_{\theta}$ where $\mathbf{X}_{1} \sim N\left(0, K_{1}\right), \mathbf{X}_{2} \sim N\left(0, K_{2}\right)$ and $\theta=$ Bernoulli $(\lambda) .$ Then
  use $h(\mathbf{Z} | \theta) \leq h(\mathbf{Z}) .]$}
  \begin{proof}
  Let $Z = \theta X_1 + (1 - \theta) X_2$, where $\theta,X_1$ and $X_2$ are independent, $\mathbf{X}_{1} \sim N\left(0, K_{1}\right), \mathbf{X}_{2} \sim N\left(0, K_{2}\right)$ and $\theta=$ Bernoulli $(\lambda) .$ Note that for every entry in the covariance matrix of $Z$,
  \begin{equation}
    \begin{aligned}
      cov(Z_i,Z_j) &= E(Z_i Z_j) E((\theta X_{1i} + \bar{\theta} X_{2i})(\theta X_{1j} + \bar{\theta} X_{2j})) \\
      &= E(\theta^2) E X_{1i} X_{1j} + E(\bar{\theta}^2) E X_{2i} X_{2j} \\
      &= \lambda cov(X_{1i},X_{1j}) + \bar{\lambda} cov (X_{2i},X_{2j})
    \end{aligned}
  \end{equation}
  Hence we have $K_Z = \lambda K_{X_1} + \bar{\lambda} K_{X_2}$. Also note that $EZ = \mathbf{0}$. By Theorem 8.6.5 [Cover] we have that 
  \begin{equation}
    h(Z) \le \frac{1}{2} \log (2\pi e)^n \left| K_Z \right| = \frac{1}{2} \log (2\pi e)^n \left|  \lambda K_{1} + \bar{\lambda} K_{2} \right|
    \label{eqn:1}
  \end{equation}
  By the formula for entropy of a multivariate normal distribution, we have that
  \begin{equation}
    \begin{aligned}
      h(X_1) &\le \frac{1}{2} \log (2\pi e)^n \left| K_1 \right| \\
      h(X_2) &\le \frac{1}{2} \log (2\pi e)^n \left| K_2 \right| \\
      \Rightarrow h(Z|\theta) &= \lambda h(X_1) + \bar{\lambda}h(X_2) \\
      &=  \frac{\lambda}{2} \log (2\pi e)^n \left| K_1 \right| + \frac{\bar{\lambda}}{2} \log (2\pi e)^n \left| K_2 \right| \\
      &= \frac{1}{2} \log (2\pi e)^n \left| K_{1} \right|^{\lambda} \left| K_{2} \right|^{\bar{\lambda}}
    \end{aligned}
    \label{eqn:2}
  \end{equation}
  Note that $h(Z|\theta) \le h(Z)$. The result follows from Equation \ref{eqn:1} and \ref{eqn:2}.
  \end{proof}
  \label{ex8-2}
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%                 %%%%%%%%%%%%
%%%%%%%%%%%%%    EXERCISE 3   %%%%%%%%%%%%
%%%%%%%%%%%%%                 %%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{exercise}[Uniformly distributed noise]{ Let the input random variable $X$ to a channel be uniformly distributed over the interval $-\frac{1}{2} \leq x \leq+\frac{1}{2}$ Let the output of the channel be $Y=X+Z,$ where the noise random variable is uniformly distributed over the interval $-a / 2 \leq z \leq$ $+a / 2$
  \begin{enumerate}
    \item Find $I(X ; Y)$ as a function of $a$
    \item For $a=1$ find the capacity of the channel when the input $X$ is peak-limited; that is, the range of $X$ is limited to $-\frac{1}{2} \leq x \leq$ $+\frac{1}{2} .$ What probability distribution on $X$ maximizes the mutual information $I(X ; Y) ?$
    \item (Optional) Find the capacity of the channel for all values of $a$, again assuming that the range of $X$ is limited to $-\frac{1}{2} \leq x \leq+\frac{1}{2}$
  \end{enumerate} }
  \begin{solution}
  \par{~}
  \begin{enumerate}
    \item We first calculate the distribution of $Y$.
    \begin{equation}
      p_Y (y) = \int_{-\frac{a}{2}}^{\frac{a}{2}} \frac{1}{a} \mathbf{1}_{\left\{ y - \frac{1}{2} \le z \le y + \frac{1}{2} \right\}} d z
    \end{equation}
    If $a\le 1$, we have that
    \begin{equation}
      p_Y (y) = \begin{cases}
        \frac{1}{a} \left( y + \frac{a+1}{2}\right)  & -\frac{a+1}{2} \le y \le \frac{a-1}{2} \\
        1 & \frac{a-1}{2} < y \le \frac{1-a}{2} \\
        \frac{1}{a} \left(-y + \frac{a+1}{2}\right) & \frac{1-a}{2} < y \le \frac{a+1}{2}
      \end{cases}
    \end{equation}
    If $a > 1$, we have that
    \begin{equation}
      p_Y (y) = \begin{cases}
        \frac{1}{a} \left( y + \frac{a+1}{2}\right)  & -\frac{a+1}{2} \le y \le \frac{1-a}{2} \\
        \frac{1}{a} & \frac{1-a}{2} < y \le \frac{a-1}{2} \\
        \frac{1}{a} \left(-y + \frac{a+1}{2}\right) & \frac{a-1}{2} < y \le \frac{a+1}{2}
      \end{cases}
    \end{equation}
    For $a \le 1$,
    \begin{equation}
      \begin{aligned}
        I(X;Y) &= h(Y) - h(Y|X) = h(Y) - h(Z) = h(Y) - \ln a \\
        &= \int_{-\frac{a+1}{2}}^{\frac{a+1}{2}} p(y) \ln p(y) dy - \ln a \\
        &= - 2 \int_{0}^{\frac{1-a}{2}} 1 \ln 1 dy - 2 \int_{\frac{1-a}{2}}^{\frac{1+a}{2}} \frac{1}{a} (-y + \frac{a+1}{2}) \log \frac{1}{a} (-y + \frac{a+1}{2}) dy - \ln a \\
        &= 0 - 2a \int_{0}^{1} t \ln t dt - \ln a \quad ( t \triangleq \frac{1}{a}\left(-y + \frac{a+1}{2})\right) \\
        &= - 2a \left( \frac{1}{2} t^2 \ln t - \frac{1}{4} t^2 \right) \bigg|_{0}^{1} - \ln a \\
        &= \frac{a}{2} - \ln a
      \end{aligned}
    \end{equation}
    For $a > 1$,
    \begin{equation}
      \begin{aligned}
        I(X;Y) &= h(Y) - h(Y|X) = h(Y) - h(Z) = h(Y) - \ln a \\
        &= \int_{-\frac{a+1}{2}}^{\frac{a+1}{2}} p(y) \ln p(y) dy - \ln a \\
        &= - 2 \int_{0}^{\frac{a-1}{2}} \frac{1}{a} \ln \frac{1}{a} dy - 2 \int_{\frac{a-1}{2}}^{\frac{1+a}{2}} \frac{1}{a} (-y + \frac{a+1}{2}) \log \frac{1}{a} (-y + \frac{a+1}{2}) dy - \ln a \\
        &= \frac{a-1}{a} \ln a - 2a \int_{0}^{\frac{1}{a}} t \ln t dt - \ln a \quad ( t \triangleq \frac{1}{a}\left(-y + \frac{a+1}{2})\right) \\
        &= - \frac{1}{a} \ln a - 2a \left( \frac{1}{2} t^2 \ln t - \frac{1}{4} t^2 \right) \bigg|_{0}^{\frac{1}{a}}\\
        &= \frac{1}{2a}
      \end{aligned}
    \end{equation}
    \item Since $X$ and $Z$ are both limited to $\left[-\frac{1}{2},\frac{1}{2}\right]$, $Y$ is limited to $\left[-1,1\right]$. By the Example 12.2.4 in [Cover], the maximal differential entropy $h(Y)$ is $\ln 2$, which can be obtained when $Y$ is uniformly distributed, and thus $p(X = -\frac{1}{2}) = p(X = \frac{1}{2}) = \frac{1}{2}$.
    \begin{equation}
      I(X;Y) = h(Y) - h(Y|X) = h(Y) - h(Z) = h(Y) - \ln 1 \le \ln 2 \text{ nats}
    \end{equation}
    The capacity is $\ln 2$ nats $= 1$ bit.
    \item If $a = \frac{1}{k}, k\in \mathbf{N}$, then we can construct $X$ to be uniformly distributed on $\left\{-\frac{1}{2}, -\frac{1}{2} + \frac{1}{k}, \ldots,\frac{1}{2} \right\}$, with $Y$ uniformly distributed on $\left[-\frac{1}{2}-\frac{1}{2k}, -\frac{1}{2}+\frac{1}{2k} \right]$. In this case, the maximal mutual information can be obtained. $C = \log \left(1+ \frac{1}{k}\right)$.
  \end{enumerate}
  \end{solution}
  \label{ex8-3}
\end{exercise}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%                 %%%%%%%%%%%%
%%%%%%%%%%%%%    EXERCISE 4   %%%%%%%%%%%%
%%%%%%%%%%%%%                 %%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{exercise}[Channel with uniformly distributed noise]{ Consider a additive channel whose input alphabet $\mathcal{X}=\{0,\pm 1,\pm 2\}$ and whose output $Y=X+Z,$ where $Z$ is distributed uniformly over the interval $[-1,1] .$ Thus, the input of the channel is a discrete random variable, whereas the output is continuous. Calculate the capacity $C=$ $\max _{p(x)} I(X ; Y)$ of this channel.}
  \begin{solution}
  Note that
  \begin{equation}
    I(X;Y) = h(Y) - h(Y|X) = h(Y) -h(Z) = h(Y) - \log 2
  \end{equation}
  Note that the support set of $Y$ is $\left[-3,3\right]$.By the Example 12.2.4 in [Cover], the maximal differential entropy $h(Y)$ is $\log 6$, which can be obtained when $p(X=-2) = p(X=0) = p(X=2) = \frac{1}{3}$, and $Y$ is uniformly distributed. The maximal mutual information is $\log 3$.
  \end{solution}
  \label{ex8-4}
\end{exercise}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%                 %%%%%%%%%%%%
%%%%%%%%%%%%%    EXERCISE 5   %%%%%%%%%%%%
%%%%%%%%%%%%%                 %%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{lemma}[Conditional Expectation of Two Normal Random Variables] {Let $X$ and $Y$ be jointly Gaussian with variances $\sigma_1^2$ ,$\sigma_2^2$ and correlation coefficient $\rho$. We have that $E(X|Y) = \frac{\sigma_1 \rho}{\sigma_2} Y$.}
  \begin{proof}
    We calculate the conditional probability density of $p(x|y)$
    \begin{equation}
      \begin{aligned}
        p(x|y) &= \frac{p(x,y)}{p(y)} \\
        &= \frac{\frac{1}{2\pi \sqrt{1-\rho^2}\sigma_1\sigma_2} \operatorname{exp}\left\{-\frac{1}{2(1-\rho^2)}\left[\frac{x^2}{\sigma_1^2}-2\rho\frac{xy}{\sigma_1\sigma_2}+\frac{y^2}{\sigma_2^2}\right]\right\}}{\frac{1}{\sqrt{2\pi}\sigma_2}\operatorname{exp} \left\{ - \frac{1}{2} \frac{y^2}{\sigma_2^2}\right\} }  \\
        &= \frac{1}{\sqrt{2\pi (1-\rho^2)}} \operatorname{exp} \left\{ -\frac{1}{2(1-\rho^2)}\left[\frac{x^2}{\sigma_1^2}-2\rho\frac{xy}{\sigma_1\sigma_2}+\frac{\rho^2 y^2}{\sigma_2^2}\right] -\frac{1}{2} \frac{y^2}{\sigma_2^2}  +\frac{1}{2} \frac{y^2}{\sigma_2^2}   \right\} \\
        &= \frac{1}{\sqrt{2\pi (1-\rho^2)}} \operatorname{exp} \left\{ -\frac{1}{2(1-\rho^2)\sigma_1^2}\left(x - \frac{\sigma_1 \rho y}{\sigma_2}\right)   \right\}
      \end{aligned}
    \end{equation}
    With $Y$ given, the mean value is $\frac{\sigma_1 \rho}{\sigma_2} Y$
  \end{proof}
\end{lemma}


\begin{exercise}[Gaussian mutual information]{Suppose that $(X, Y, Z)$ are jointly Gaussian and that $X \rightarrow Y \rightarrow Z$ forms a Markov chain. Let $X$ and $Y$ have correlation coefficient $\rho_{1}$ and let $Y$ and $Z$ have correlation coefficient $\rho_{2} .$ Find $I(X ; Z)$}
  \begin{solution} By the formula of mutual information between correlated Gaussian random variables, we have
    \begin{equation}
      I(X;Z) = -\frac{1}{2} \log (1-\rho_{xz} ^2)
    \end{equation}

  With the lemma above, we can derive $\rho_{xz}$ as follows.

  \begin{equation}
      \begin{aligned}
      \rho_{x z} &=\frac{\mathrm{E}\{X Z\}}{\sigma_{x} \sigma_{z}} \\
      &=\frac{\mathrm{E}\{\mathrm{E}\{X Z | Y\}\}}{\sigma_{x} \sigma_{z}} &\quad {\text{Nested Expectation}} \\
      &=\frac{\mathrm{E}\{\mathrm{E}\{X | Y\} \mathrm{E}\{Z | Y\}\}}{\sigma_{x} \sigma_{z}} &\quad {\text{Markov Chains}}\\
      &=\frac{\mathrm{E}\left\{\left(\frac{\sigma_{x} \rho_{x y}}{\sigma_{y}} Y\right)\left(\frac{\sigma_{z} \rho_{z x}}{\sigma_{y}} Y\right)\right\}}{\sigma_{x} \sigma_{z}} &\quad {\text{Apply the Lemma}} \\
      &=\rho_{x y} \rho_{z y}
      \end{aligned}
  \end{equation}
  Hence $I(X;Z) = -\frac{1}{2} \log (1 -\rho_{x y}^2 \rho_{z y}^2 )$
  \end{solution}
  \label{ex8-5}
\end{exercise}