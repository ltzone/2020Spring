@article{shannon1948mathematical,
  title={A mathematical theory of communication},
  author={Shannon, Claude E},
  journal={Bell system technical journal},
  volume={27},
  number={3},
  pages={379--423},
  year={1948},
  publisher={Wiley Online Library}
}

@book{yeung2008information,
  title={Information theory and network coding},
  author={Yeung, Raymond W},
  year={2008},
  publisher={Springer Science \& Business Media}
}

@book{yeung2012first,
  title={A first course in information theory},
  author={Yeung, Raymond W},
  year={2012},
  publisher={Springer Science \& Business Media}
}

@book{belyaev1959many,
  title={The Many Body Problem},
  author={Belyaev, Spartak T and Bloch, Claude and Bohm, David and Brueckner, Keith A and de Dominicis, Cirano and Huang, Kerson and Hugenholtz, Nicolaas Marinus and Lipkin, Harry J and Lynton, Ernest Albert and Mottelson, Ben Roy and others},
  year={1959}
}

@article{Dayan2001Theoretical,
  title={Theoretical Neuroscience: Computational and Mathematical Modeling of Neural Systems},
  author={Dayan, P. and Abbott, L F},
  year={2001},
 keywords={arteriovenous malformation;dura;surgery},
 abstract={Theoretical neuroscience provides a quantitative basis for describing what nervous systems do, determining how they function, and uncovering the general principles by which they operate. This text introduces the basic mathematical and computational methods of theoretical neuroscience and presents applications in a variety of areas including vision, sensory-motor integration, development, learning, and memory.},
}

@article{Rieke1996Spikes,
  title={Spikes, Exploring the Neural Code},
  author={Rieke, Fred and Warland, David and Steveninck, Rob Ruyter and Bialek, William},
  year={1996},
 abstract={Spikes : exploring the neural code Fred Rieke ... [et al.] (Computational neuroscience) MIT Press, 1999 paperback ed.},
}

@article{Williams2010Nonnegative,
  title={Nonnegative Decomposition of Multivariate Information},
  author={Williams, Paul L. and Beer, Randall D.},
  year={2010},
 keywords={Computer Science - Information Theory;Mathematical Physics;Physics - Biological Physics;Physics - Data Analysis;Statistics and Probability;Quantitative Biology - Neurons and Cognition;Quantitative Biology - Quantitative Methods;94A15},
 abstract={Of the various attempts to generalize information theory to multiple variables, the most widely utilized, interaction information, suffers from the problem that it is sometimes negative. Here we reconsider from first principles the general structure of the information that a set of sources provides about a given variable. We begin with a new definition of redundancy as the minimum information that any source provides about each possible outcome of the variable, averaged over all possible outcomes. We then show how this measure of redundancy induces a lattice over sets of sources that clarifies the general structure of multivariate information. Finally, we use this redundancy lattice to propose a definition of partial information atoms that exhaustively decompose the Shannon information in a multivariate system in terms of the redundancy between synergies of subsets of the sources. Unlike interaction information, the atoms of our partial information decomposition are never negative and always support a clear interpretation as informational quantities. Our analysis also demonstrates how the negativity of interaction information can be explained by its confounding of redundancy and synergy.},
}

@article{stanley1997enumerative,
  title={Enumerative combinatorics, vol. 1 Cambridge University Press Cambridge},
  author={Stanley, RP},
  journal={England zbMATH},
  year={1997}
}