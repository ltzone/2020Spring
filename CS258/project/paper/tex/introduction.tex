\section{Introduction}

% 研究意义，前人工作
% 研究方法，主要创新点
% 论文组织结构


\qquad Developing from investigation in physics, engineering and mathematics, information theory originated from Shannon's pioneering research of the reliability and coding in communication system. It has successfully become a universal tool for analyzing complex systems, and has been applied in many fields such as genetics, physics, neuroscience, machine learning and so on. Shannon’s mutual information is by far the most widely used concept from the information theory \cite{shannon1948mathematical}, and Shannon’s information measures refer to entropy, conditional entropy, mutual information, and conditional mutual information, which are the most significant measures of information in information theory. 


The relations between the entropies, conditional entropies and mutual information for two random variables X and Y can be summarized by a variation of the Venn diagram. With the help of such diagram, the relations can be depicted intuitively and vividly, thus reducing the difficulty of readers' understanding of the knowledge. This one-to-one correspondence between Shannon’s information measures and set theory is not just a coincidence for two random variables. Actually, Shannon’s information measures for any n $\geq$ 2 random variables all have a set-theoretic structure.

Using a few basic concepts in measure theory such as fields, atoms and signed measure $\mu$, a theory “I-measure” which establishes a one-to-one correspondence between Shannon’s information measures and set theory in full generality has been developed \cite{yeung2008information,yeung2012first}. Then manipulations of Shannon’s information measures can be viewed as set operations, thus allowing the rich suite of tools in set theory to be used in information theory. Moreover, the structure of Shannon’s information measures can easily be visualized by means of an information diagram if four or fewer random variables are involved. The use of information diagrams simplifies many difficult proofs in information theory problems. More importantly, these results, which may be difficult to discover in the first place, can easily be obtained by inspection of an information diagram.

Until now, most of the work of information theory only deals with the simplest case: the information about another variable provided by one variable. By contrast, most challenging scientific problems, such as many-body problems in physics \cite{belyaev1959many}, and population coding in neuroscience \cite{Dayan2001Theoretical,Rieke1996Spikes}, involve understanding the structure of interactions between three or more variables. To generalize information theory to multivariate interactions, interaction information was proposed as a measure of the amount of information bound in a set of variables, rather than the amount of information existing in any subset of those variables.  So entropy and mutual information correspond to the first- and second-order interaction information, while interaction information and its third-order, fourth-order and higher-order variants provide a method to represent the structure of multivariate information. However, the widespread use of interactive information is largely hindered by "odd" and "unfortunate" attributes. For three or more variables, interaction information can be negative, whose meaning is usually unclear for information as it is commonly understood.

Then a new perspective on the structure of multivariate information has been formulated.\cite{Williams2010Nonnegative} Considering the general structure of the information that a set of sources provide about a given variable, a new definition of redundancy was proposed, which can be used to exhaustively decompose the Shannon information in a multivariate system into partial information atoms.

Here we use the above-mentioned method of I-measure, aiming to establish a one-to-one correspondence between partial information measures and the set theory. With this correspondence, manipulations of partial information measures can be viewed as set operations. What’s more, the structure of partial information measures can be easily visualized by a partial information diagram. And we hope that this can simplify the deduction and  proofs in information theory problems.
