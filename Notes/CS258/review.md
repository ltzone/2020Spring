[toc]

# 0304 Entropy (1)

## Entropy: Definition

### Notation:

- Let $X$ be a _discrete random variable_(ç¦»æ•£éšæœºå˜é‡) with alphabet(å­—æ¯è¡¨/æ ·æœ¬ç©ºé—´) $\mathcal{X}$ and **probability mass function** $p(x)=Pr(X=x), x\in \mathcal{X}$. (abbrev. $p_{\mathcal{X}}(x)$)

- The entropy of $X$ is defined by $$H(X)=-\sum_{x \in \mathcal{X}} p(x) \log p(x)$$

- Remark:
    - $0 \log{0} \rightarrow 0$
    - æˆ‘ä»¬æœ‰æ—¶ä¹Ÿç›´æ¥ç”¨æ¦‚ç‡åˆ†å¸ƒå‡½æ•°è¡¨ç¤ºå­—æ¯è¡¨,å› ä¸º$H(X)$ä»…ä¸$p(x)$æœ‰å…³
    - $H(X) \geq 0$
    - å½“$X$å‡åŒ€åˆ†å¸ƒæ—¶,$H(X)=\log{|X|}$,åœ¨ç¦»æ•£æƒ…å†µä¸‹ç†µæ˜¯æœ€å¤§çš„
    - $H_{b}(X)=\log _{b}{a} H_{a}(X)$
        - å¯¹æ•°åº•å–$e$æ—¶,entropy is measured in nats
        - å¯¹æ•°å–$2$æ—¶,entropy is measured in bits
    - æœ¬è¯¾ç¨‹ä¸­,ä¸»è¦è®¨è®ºæœ‰é™å­—æ¯è¡¨çš„é—®é¢˜

### Examples:

- Binary entropy function 
$$\text { Let } X=\left\{\begin{array}{ll}1 & \text { with probablity } p \\ 0 & \text { with probability } 1-p\end{array}\right.$$
$$H(X)=-p \log p-(1-p) \log (1-p)$$
> ![](img/0304-1.png)


- ç”¨æœŸæœ›çš„å½¢å¼è¡¨è¾¾ç†µ, ç†µæ˜¯éšæœºå˜é‡$\log \frac{1}{p(X)}$çš„æœŸæœ›
$$\begin{aligned} E_{p} g(X) &=\sum_{x \in x} g(x) p(x) \\ H(X) &=E_{p} \log \frac{1}{p(X)} \end{aligned}$$

### Properties

> **Theorem** For any discrete random variable $X$ $0 \leq H(X) \leq \log |X|$
> **Pf.** éè´Ÿ,trivial
> Note. $f(x)=-x \log x$ is concave in $x$ (by second-derivative). and that $\sum_{X} p(x)=1$.
> By applying the concavity of $f(x)$, 
$$\frac{1}{|\mathcal{X}|} \sum_{x \in \chi}-p(x) \log p(x) \leq-\frac{1}{|\mathcal{X}|} \log \frac{\sum_{x} p(x)}{|x|}=\frac{1}{|x|} \log |\mathcal{X}|$$
>> Recall: Concavity.
>> $$\sum_{i} p_{i} f\left(x_{i}\right) \leq f\left(\sum_{i} p_{i} x_{i}\right)$$

> **Lemma: å‡åŒ€åˆ†å¸ƒæœ€å¤§åŒ–ç¦»æ•£ç†µ** equality holds iff $p(x)=1 /|\mathcal{X}|$

## More Entropies

- æˆ‘ä»¬ä¼šå®šä¹‰æ›´å¤šç†µ, æ¡ä»¶ç†µ/è”åˆç†µ...
- ç†µçš„å®šä¹‰åªä¸æ¦‚ç‡å¯†åº¦æœ‰å…³, å’Œå­—æ¯è¡¨çš„å–å€¼å…·ä½“æƒ…å†µæ— å…³. 
- å¯¹å¤šä¸ªéšæœºå˜é‡, æˆ‘ä»¬å¯ä»¥å®šä¹‰
    - è”åˆåˆ†å¸ƒ $p\left(x_{i}, x_{j}\right)$
    - æ¡ä»¶åˆ†å¸ƒ $p\left(x_{i} | \dots\right)$
    - éƒ½å¯ä»¥è®¡ç®—å‡ºç†µ
- æ¦‚ç‡è®ºä¸­åŸºæœ¬å®šå¾‹
    - Chain Rule $p\left(x_{1}, x_{2}, \ldots, x_{n}\right)=p\left(x_{n}\right) p\left(x_{n-1} | x_{n}\right) \ldots p\left(x_{1} | x_{2}, \ldots, x_{n-1}\right)$
    - Bayesian Rule $p(y) p(x | y)=p(x) p(y | x)$
    - è¿™äº›åŸºæœ¬å‡†åˆ™çš„å­˜åœ¨, è¡¨æ˜ç†µä¹Ÿå¯èƒ½å­˜åœ¨ç‰¹æ®Šçš„ç»“æ„

### Joint Entropy

Facts: å¤šä¸ªéšæœºå˜é‡çš„å­—æ¯è¡¨å¯ä»¥ç»„åˆæˆä¸€ä¸ªå­—æ¯è¡¨

> **Definition** The joint entropy $H(X,Y)$ of a pair of discrete random variable $(X,Y)$ with joint distribution $p(x,y)$ is defined as $$H(X, Y)=-\sum_{x \in X} \sum_{y \in Y} p(x, y) \log p(x, y)$$

Propositions:
1. $H(X,X)=H(X)$, å¯ä»¥ç†è§£æˆæœ¬è´¨ä¸Šæ˜¯åŒä¸€ä»¶äº‹, åªæ˜¯å¤šæ¬¡å®éªŒè€Œå·²
2. $H(X,Y)=H(Y,X)$
3. è”åˆç†µä¹Ÿå¯ä»¥å†™æˆè”åˆæœŸæœ›çš„å½¢å¼ $$H\left(X_{1}, X_{2}, \ldots, X_{n}\right)=-\sum p\left(x_{1}, x_{2}, \ldots, x_{n}\right) \log p\left(x_{1}, x_{2}, \ldots, x_{n}\right)=-E \log p\left(X_{1}, \ldots, X_{n}\right)$$

### Conditional Entropy

ä¸¤ç§è®¡ç®—æ–¹å¼


- å…ˆå¯¹fixed $X$ç®—æ¡ä»¶ç†µ, å†å¯¹æ‰€æœ‰æ¡ä»¶ç†µåŠ æƒæ±‚å’Œ.
    - Entropy for $p(Y|X=x)$ $$H(Y | X=x)=\sum_{y}-p(y | X=x) \log p(y | X=x)=- E \log p(Y | X=x)$$
    - Entropy for $p(Y|X)$
    $$H(Y|X) = \sum_{x} p(x) H(Y|X=x)$$

- ä¹Ÿå¯ä»¥é€šè¿‡ç›´æ¥æ ¹æ®ä»¥ä¸‹æ¨å¯¼, ç›´æ¥è®¡ç®—$\log p(Y | X)$çš„æœŸæœ›. $$\begin{aligned} H(Y | X) &=\sum_{x \in \mathcal{X}} p(x) H(Y | X=x) \\ &=-\sum_{x \in \mathcal{X}} p(x) \sum_{y \in \mathcal{Y}} p(y | x) \log p(y | x) \\ &=-\sum_{x \in X} \sum_{y \in y} p(x, y) \log p(y | x) \\ &=-E \log p(Y | X) \end{aligned}$$

> **Proposition** $H(Y | X) \leq H(Y)$
> ç›´è§‚ç†è§£, æ¡ä»¶ç†µ(åœ¨Xå·²çŸ¥çš„æƒ…å†µä¸‹,Yçš„ä¸ç¡®å®šåº¦)ä¼šæ¯”åŸå§‹ç³»ç»Ÿçš„ç†µè¦ä½, æ¡ä»¶é™ä½äº†ç³»ç»Ÿçš„ä¸ç¡®å®šåº¦.

> **Remark** Exampleä¸­ä¸¤ä¸ªæœ‰æ„æ€çš„ç»“è®º, åç»­ç»™å‡ºè¯æ˜
> $$\begin{array}{c}\boldsymbol{H}(X | \boldsymbol{Y}) \neq \boldsymbol{H}(\boldsymbol{Y} | \boldsymbol{X}) \\ \boldsymbol{H}(\boldsymbol{X} | \boldsymbol{Y})+\boldsymbol{H}(\boldsymbol{Y})=\boldsymbol{H}(\boldsymbol{Y} | \boldsymbol{X})+\boldsymbol{H}(\boldsymbol{X})=\boldsymbol{H}(\boldsymbol{X}, \boldsymbol{Y})\end{array}$$
> ç›´è§‚ç†è§£, ä¸¤ä»¶äº‹çš„å…ˆåå‘ç”Ÿçš„ä¸ç¡®å®šæ€§ä¸å…·æœ‰å¯¹ç§°æ€§, ä¸¤ä»¶äº‹çš„ä¸ç¡®å®šæ€§ä¹‹å’Œå¯ä»¥ç†è§£ä¸ºä¸¤ä»¶äº‹(å¸¦æ¡ä»¶)å…ˆåå‘ç”Ÿçš„ä¸ç¡®å®šæ€§ä¹‹å’Œ.

### Chain Rule

Recall: æ¦‚ç‡è®ºä¸­, $p(x, y)=p(x | y) p(y)=p(y | x) p(x)$. å› æ­¤æˆ‘ä»¬æœ‰$\log p(x, y)=\log p(x | y)+\log p(y)=\log p(y | x)+\log p(x)$.

è€ƒè™‘ä¸ŠèŠ‚ä¸­å®šä¹‰çš„æ¡ä»¶ç†µ, $$\begin{aligned} & E-\log p(x, y) \\=& E-\log p(x | y)+E-\log p(y) \\=& E-\log p(y | x)+E-\log p(x) \end{aligned}$$

> **Theorem: Chain Rule** $H(X, Y)=H(Y)+H(X | Y)=H(X)+H(Y | X)$

**Proposition**
1. å¦‚æœXå’ŒYç‹¬ç«‹,é‚£ä¹ˆ$H(X,Y)=H(X)+H(Y)$.
2. å¦‚æœXæ˜¯å…³äºYçš„å‡½æ•°,é‚£ä¹ˆ$H(X,Y)=H(Y)$.
3. è´å¶æ–¯å…¬å¼: $H(X, Y | Z)=H(X | Z)+H(Y | X, Z)$
    **Pf.** Note $$ p(x,z)p(y|x,z)=p(x,y,z) = p(z)p(x,y|z)$$
   and that $$p(x,z)=p(x|z)p(z)$$
   it follows that $p(x, y | z)=p(x | z) p(y | x, z)$.



### Venn Diagram

æˆ‘ä»¬å¦‚ä½•é«˜æ•ˆåœ°æ•´ç†ä¿¡æ¯é‡çš„å…³ç³»?

> $H(X, Y)=H(Y)+H(X | Y)=H(X)+H(Y | X)$
> ![](img/0304-2.png)


### Zero Entropy

å¦‚æœéšæœºå˜é‡çš„æ¡ä»¶ç†µä¸º0, é‚£ä¹ˆYæ˜¯Xçš„ä¸€ä¸ªå‡½æ•° (i.e., for all $x$ with $p(x)>0$, there is only one possible value of $y$ with $p(x,y)>0$.

Zero Entropyåœ¨ç½‘ç»œåˆ†æ/äººå·¥æ™ºèƒ½çš„æ¨ç†ä¸Šå…·æœ‰å¾ˆå¤§çš„åº”ç”¨. å½“æˆ‘ä»¬é‡åˆ°æ¡ä»¶å…³ç³»æ—¶, å¯ä»¥è€ƒè™‘ç”¨è¿™ç§æ–¹å¼è§£å†³å®ƒ.

**Pf.** By condition we have $$H(Y|X) = \sum_{x} p(x) H(Y|X=x) = 0$$
Note that $p(x)>0$, thus for any $x$, we have $H(Y|X=x)=0$.

It follows that when $x$ is determined, the distribution of $Y$ is a single value.


# 0309 Entropy (2)


## Relative Entropy

ç›¸å¯¹ç†µ, åº¦é‡ä¸¤ç§åˆ†å¸ƒä¹‹é—´çš„è·ç¦» (K-L distance), å‡å®š$p(x)$, $q(x)$å…·æœ‰ç›¸åŒçš„ç»´æ•°(over the same alphabet $\mathcal{X}$), æˆ‘ä»¬æœ‰
$$\begin{aligned} D(p \| q)=& \sum_{x \in \mathcal{X}} p(x) \log \frac{p(x)}{q(x)} \\ &=E_{p} \log \frac{p(X)}{q(X)} \end{aligned}$$

æ³¨æ„, ä»…å¯¹pæ±‚æœŸæœ›, æˆ‘ä»¬è¿˜æœ‰ä»¥ä¸‹è§„å®šæˆ–æ€§è´¨
- $0 \log _{0}^{0}=0,0 \log \frac{0}{q}=0, p \log \frac{p}{0}=\infty$
- è‹¥å­˜åœ¨äº‹ä»¶x, ä½¿å¾—$p(x)>0$ä¸”$q(x)=0$, é‚£ä¹ˆ$D(p \| q)=\infty$
- $D(p \| q) \geq 0$
- $D(p \| q)=E_{p}(-\log q(x))-E_{p}(-\log p(x))=E_{p}(-\log q(x))-H(p)$, ç›¸å¯¹ç†µå¯å†™ä½œä¸€ä¸ªå…³äºpåˆ†å¸ƒçš„æœŸæœ›å‡å»ä¸€ä¸ªpçš„ç†µçš„å½¢å¼

### Relative Entropy is NOT Metric
> A metric d: $X,Y \mapsto R^{+}$ between two distributions should satifsy
> - $d(X, Y) \geq 0$
> - $d(X, Y) \equiv d(Y, X)$
> - $d(X, Y)=0$ if and only if $X=Y$
> - $d(X, Y)+d(Y, Z) \geq d(X, Z)$

- Euclidean distance is a metric
- KL distance is not a metric
  - $D(p\| p)= 0$
  - but $D(p|\ q) \neq D(q|\ p)$
  - distance but not metric
- **Variational Distance(å·®åˆ†/å˜åˆ†è·ç¦»)** between $p$ and $q$ is denoted as
$$
V(p, q)=\sum_{x \in \mathcal{X}}|p(x)-q(x)|
$$
- Pinsker's Inequality: ç›¸å¯¹ç†µæ˜¯å·®åˆ†è·ç¦»çš„ä¸€ä¸ªä¸Šç•Œ
$$
D(p \| q) \geq \frac{1}{2 \ln 2} V^{2}(p, q)
$$

### Conditional Relative Entropy
æ¡ä»¶ç›¸å¯¹ç†µ = è®¡ç®—ä¸¤ç§å•ä¸ªæ¡ä»¶æ¦‚ç‡åˆ†å¸ƒç›¸å¯¹ç†µ, å†å¯¹på–å¹³å‡. è¯¥è®¡ç®—æ–¹æ³•ä¹Ÿå¯æ”¹å†™æˆæœŸæœ›çš„å½¢å¼

$$\begin{aligned}
D(p(y | x) \| q(y | x)) &=\sum_{x} p(x) \sum_{y} p(y | x) \log \frac{p(y | x)}{q(y | x)} \\
&=\sum_{x} \sum_{y} p(x) p(y | x) \log \frac{p(y | x)}{q(y | x)} \\
&=E_{p(x, y)} \log \frac{p(Y | X)}{q(Y | X)}
\end{aligned}$$

æ€§è´¨: Chain Rule $D(p(x, y) \| q(x, y))=D(p(x) \| q(x))+D(p(y | x) \| q(y | x))$.
**Proof.** By Definition
$$\begin{array}{l}
D(p(x, y) \| q(x, y))=\sum_{x} \sum_{y} p(x, y) \log \frac{p(x, y)}{q(x, y)}=\sum_{x} \sum_{y} p(x, y) \log \frac{p(x) p(y | x)}{q(x) q(y | x)} \\
=\sum_{x} \sum_{y} p(x, y)\left(\log \frac{p(x)}{q(x)}+\log \frac{p(y | x)}{q(y | x)}\right)
\end{array}$$

## Mutual Information
$$
\begin{aligned} I(X ; Y) &=\sum_{x} \sum_{y} p(x, y) \log \frac{p(x, y)}{p(x) p(y)} \\ &=D(p(x, y) \| p(x) p(y)) \\ &=E_{p(x, y)} \log \frac{p(X, Y)}{p(X) p(Y)} \end{aligned}
$$

äº’ä¿¡æ¯çš„æ€§è´¨:
- $I(X;Y)=I(Y;X)$
- $I(X;X)=H(X)$ éšæœºå˜é‡çš„ç†µå°±æ˜¯å®ƒæœ¬èº«äº’ä¿¡æ¯çš„å€¼
- $X$å’Œ$Y$ç‹¬ç«‹, $I(X;Y)=0$. æ²¡æœ‰å…³è”çš„éšæœºå˜é‡äº’ä¿¡æ¯ä¸º0
- æ³¨æ„Notation: $I(X;Y)$ $H(X,Y)$
- æˆ‘ä»¬é€šå¸¸ç”¨å…³ç³»å¼è®¡ç®—ä¿¡æ¯é‡

### Mutual Information and Entropy

 ![A1](img/0304-2.png)

$$\begin{array}{c}I(X ; Y)=H(X)-H(X | Y) \\I(X ; Y)=H(Y)-H(Y | X) \\I(X ; Y)=H(X)+H(Y)-H(X, Y) \\I(X ; Y)=I(Y ; X) \\I(X ; X)=H(X)\end{array}$$

Note: é¢ç§¯å¯èƒ½ä¸ºè´Ÿ, æ‰€ä»¥$I(X;Y)$éƒ¨åˆ†å¿…ä¸å¯å°‘
**Proof.** Recall $p(X, Y)=p(X) p(Y | X)=p(Y) p(X | Y)$
To Prove $I(X ; Y)=H(X)+H(Y)-H(X, Y)$, Since 
$$\log \frac{p(X, Y)}{p(X) p(Y)}=-\log p(X)-\log p(Y)+\log p(X, Y)$$
By taking Expectation of $p(x,y)$ and using $E(X_1 +X_2) = E(X_1) + E(X_2)$, we get $I(X ; Y)=H(X)+H(Y)-H(X, Y)$

Proposition:
If $X$ and $Y$ are independent, then
$$H(X,Y)=H(X)+H(Y)$$
$$I(X;Y) = H(X)+H(Y)-H(X,Y)$$
or vice versa.

### Conditional Mutual Information
$$\begin{aligned}
I(X ; Y | Z) &=H(X | Z)-H(X | Y, Z) \\
&=E_{p(x, y, z)} \log \frac{p(X, Y | Z)}{p(X | Z) p(Y | Z)}
\end{aligned}$$

> Chain Rule is a decomposition

$$I\left(X_{1}, X_{2}, \ldots, X_{n} ; Y\right)=\sum_{i=1}^{n} I\left(X_{i} ;  Y | X_{i-1}, X_{i-2}, \ldots, X_{1}\right)$$

**Proof.** $I\left(X_{1}, X_{2}, \ldots, X_{n} ; Y\right)=H\left(X_{1}, \ldots, X_{n}\right)-H\left(X_{1}, \ldots, X_{n} | Y\right)$
Apply Chain Rule for entropy respectively, then bind the $\sum$ together, rewrite in the mutual information form.



![relation](img/0309-1.png)

ä¿¡æ¯åº¦é‡çš„å¥½å¤„æ˜¯ç”¨å•ä¸€å€¼é¿å…éšç€éšæœºå˜é‡çš„å¢å¤š,æ ·æœ¬ç©ºé—´æŒ‡æ•°å‹çš„ä¸Šå‡


## Propositions About Information Quantities

### Nonnegative Mutual Information
> **Information inequality** Let $p(x),q(x),x\in X$ be two probability mass functions. Then 
> $$D(p\| q)\ge 0$$
> with equality iff $p(x)=q(x)$ for all $x$.

**Proof.**
- By convexity: $$-D(p \| q)=\Sigma p \log \frac{q}{p} \leq \log \Sigma p \frac{q}{p}=\log \Sigma q \leq \log 1=0$$
- Using $\log x \le x - 1$when $x>0$ $$-D(p \| q)=\Sigma p \log \frac{q}{p} \leq \Sigma p\left(\frac{q}{p}-1\right)=\Sigma q-\Sigma p \leq 0$$
  $\sum q$å¯èƒ½å–ä¸åˆ°1, å› ä¸º$p=0$è€Œ$q\neq0$æ—¶ç›¸å¯¹ç†µè¢«å®šä¹‰ä¸º0, æ­¤æ—¶è¿™ä¸€é¡¹qå°±è¢«æ¶ˆå»äº†. å®é™…å‚ä¸è¿ç®—çš„$q$ä¹‹å’Œ$\le 1$


### More Properties
- $D(p \| q)=0$ iff $p(x)=q(x)$
- $I(X;Y) \ge 0$, with equality iff $X$ and $Y$ are independent
- $D(p(y|x)\| q(y|x))\ge 0$ with equality iff $p(y|x)=q(y|x)$ for all $x$ and $y$ such that $p(x)>0$
- $I(X;Y|Z)\ge 0$ with equality iff $X$ and $Y$ are conditionally independent given $Z$.
- Let $u(x)=\frac{1}{|x|}$ be the uniform probability mass fuction over $X$, and let $p(x)$ be the probability mass function for $X$, Then
  $$0\le D(p\| u)= \log |\mathcal{X}|-H(X)$$
- (Conditioning reduces entropy) (Information can't hurt)
  $$H(X|Y) \le H(X)$$
  with equality iff $X$ and $Y$ are independent
  hint: relation with $I(X;Y)$

# 0311 Entropy (3)

## Independence Bound on Entropy

From intuition to math expression.
Let $X_1,X_2,\cdots,X_n$ be drawn according to $p(x_1,x_2,\cdots,x_n)$. Then
$$H\left(X_{1}, X_{2}, \ldots, X_{n}\right) \leq \sum_{i=1}^{n} H\left(X_{i}\right)$$
with equality if and only if the $X_i$ are independent.

**Pf.** by chain rule and conditioning reduces entropy
$$H\left(X_{1}, X_{2}, \ldots, X_{n}\right)=\sum_{i=1}^{n} H\left(X_{i} | X_{i-1}, \ldots, X_{1}\right) \leq \sum_{i=1}^{n} H\left(X_{i}\right)$$


## Markov Chain
$$ p(x,y,z) = p(x)p(y|x)p(z|y) \text{denoted as} X \rightarrow Y\rightarrow Z$$
i.e. $p(z|y,x)=p(z|y)$

Prop about markov chain
-  $X \rightarrow Y \rightarrow Z$ iff $X$ and $Z$ are conditionally independent given $Y$.
- æ—¶é—´å¯é€† $X \iff Y \iff Z$
  - an easy interpretation is that in the mutual information $I(X ; Z | Y)$, $X$ and $Z$ can be switched.
- ä»¿å°„ if$Z=f(Y)$ then $X \rightarrow Y \rightarrow Z$.
- ä½“ç°åœ¨ä¿¡æ¯åº¦é‡ä¸Š, if $X \rightarrow Y\rightarrow Z$, then $I(X;Z|Y)=0$ i.e. $Y$ and $Z$ are conditionally independent given $Y$.
  **Pf.** from the probability formula we have
  $$I(X ; Z | Y )=E_{p(x, y, z)} \log \frac{p(X, Y | Z)}{p(X | Z) p(Y | Z)}$$

## Data Processing Inequality
é©¬å°”ç§‘å¤«ç³»ç»Ÿçš„ä¿¡æ¯æ˜¯å¦‚ä½•æ¼”åŒ–çš„? é©¬å°”å¯å¤«é“¾å®é™…ä¸Šç›¸å½“äºæ•°æ®åˆ†æ­¥å¤„ç†çš„è¿‡ç¨‹
Theorem : If  $X \rightarrow Y \rightarrow Z$, Then $I(X;Y)\ge I(X;Z)$
ä¿¡æ¯å¤„ç†å¾—è¶Šå¤š,ä¿¡æ¯ä¸¢å¤±çš„è¶Šå¤š
**Pf.** $I(X;Y,Z)=I(X;Z)+I(X;Y|Z)=I(X;Y)+I(X;Z|Y)$
Since $I(X;Z|Y)=0$, we have
$$I(X;Z)+I(X;Y|Z)=I(X;Y)$$

- In particular, if $Z=g(Y)$, then $I(X;Y)\ge I(X;g(Y))$
- Collary: If  $X \rightarrow Y \rightarrow Z$, $I(X;Y|Z)\le I(X;Y)$, å¯¹ä¸‰ä¸ªéšæœºå˜é‡è€Œè¨€, **æ¡ä»¶äº’ä¿¡æ¯ä¸ä¸€å®šå°äºç­‰äºäº’ä¿¡æ¯(ä¸æ¡ä»¶ç†µä¸åŒ)**

### I(X;Y;Z)
æœ‰å…³ä¸Šé¢Remarkçš„ä¸€ä¸ªåä¾‹:

Assume $X,Y$ are two independent random variables uniformly distributed on {0,1}.
  $$Z=X+Y (mod 2)$$
  We can find that $I(X;Y|Z)> I(X;Y)$. 

ä»é—®é¢˜ä¸­, $X,Y,Z$ä»»æ„ä¸¤ä¸ªéƒ½èƒ½å†³å®šå‰©ä¸‹ä¸€ä¸ª, åˆ†å¸ƒéƒ½ç›¸åŒ, ä¸”ä¸¤ä¸¤ç›¸äº’ç‹¬ç«‹(by def $p(X,Z)=p(X)p(Z)$).

$$\begin{aligned}
I(X ; Y | Z) &=H(X | Z)-H(X | Y, Z) \\
&=H(X | Z) \\
&=H(X) \\
&=1
\end{aligned}$$
$$1=I(X;Y|Z)>I(X;Y)=0$$
**Intuition:** å½“ä½ çŸ¥é“$Z$ä¹‹å,$X$å’Œ$Y$ä¹‹é—´å¯ä»¥è§£å‡ºæ›´å¤šä¿¡æ¯å‡ºæ¥. ç†µå°±ä¸å…·å¤‡è¿™ä¸€æ€§è´¨.
**Denote:** $I(X;Y;Z)=I(X;Y)-I(X;Y|Z)$ ä»…ä»…æ˜¯ç¬¦å·æ„ä¹‰, ä¸å…·æœ‰äº’ä¿¡æ¯çš„æ„ä¹‰, å› ä¸ºå®ƒå¯èƒ½å°äº0.

## Information Diagram

### 2RVs to more

| 2RVs | 3RVs| 4RVs
| --| --| --|
|![](img/0304-2.png) | ![](img/0311-1.png)| ![](img/0311-2.png)
| **Virtual Circles** | Area are all nonnegative except $I(X;Y;Z)$| All areas can be expressed in combinations of (conditional) entropy/mutual info  |

Circles are not representing any entities, for independent variables, unintersected circles are not allowed, since values can be negative.
Only items like $H(X|Y)$, $I(X;Y|Z)$ are nonnegative
Reference: Ch. 3, Information Theory and Network Coding, R. W. Yeung

### Markov Chain

| $X\rightarrow Y\rightarrow Z$ | $X_1\rightarrow \cdots \rightarrow X_n$ |
|--|--|
|![](img/0311-3.png) | ![](img/0311-4.png)|
|ç”¨åŠåœ†è¡¨ç¤º,ä¿è¯ç›¸äº’ç›¸äº¤,**ä¿è¯æ¯å—éè´Ÿ**  | æ›´ä¸€èˆ¬çš„æƒ…å†µ, nä¸ªç›¸äº’ç›¸äº¤çš„åŠåœ†|
|å…±6å—,**ä¸‰è€…äº’ä¿¡æ¯=Xä¸Zçš„äº’ä¿¡æ¯** |ä¿è¯ç¬¬ä¸€ä¸ªå’Œæœ€åä¸€ä¸ªæœ‰æ˜æ˜¾ç›¸äº¤ |

### Examples: Use Info Diagram to Prove Inequalities

![](img/0311-5.png)

$$\begin{array}{c}
H(X, Y, Z) \leq \frac{H(X, Y)+H(Y, Z)+H(Z, X)}{2} \leq H(X)+H(Y)+H(Z) \\
H(X | Y, Z)+H(Y | X, Z)+H(Z | X, Y) \leq \frac{H(X, Y | Z)+H(Y, Z | X)+H(Z, X | Y)}{2} \leq H(X, Y, Z)
\end{array}$$
With graphical interpretation:
$$a+c+e \le \frac{(a+b+c)+(c+d+e)+(a+f+e)}{2} \le a+b+\cdot +g $$
***Note, some areas can be negative! Take signals into account***

## Practical Examples

### Causality(å› æœæ¨æ–­)
æˆ‘ä»¬å°†ç³»ç»Ÿä¸­çš„å› ç´ ç”¨å›¾è¡¨ç¤º,è®¡ç®—ä¿¡æ¯é‡,å†™å‡ºä¿¡æ¯é‡ä¹‹é—´çš„ç›¸äº’å…³ç³»,æ¨å¯¼ä¿¡æ¯ä¹‹é—´æ˜¯å¦å…·æœ‰ä¸€å®šå…³ç³»
e.g. Given: $X \perp Y | Z$ and $X \perp Z$ and Prove: $X \perp Y$

### Perfect Secrecy(å®Œç¾å®‰å…¨æ¨¡å‹)
![](img/0311-6.png)
æ˜æ–‡ç”±ç§˜å¯†å’Œå¯†é’¥ç”Ÿæˆ: $H(Y|X,Z)=0$
æ¥æ”¶ç«¯å¯ä»¥é€šè¿‡æ˜æ–‡å’Œå¯†é’¥ç”Ÿæˆ:$H(X|Y,Z)=0$
æˆ‘ä»¬å¯ä»¥ç”±æ­¤æ¨å‡º: $I(X;Y)\ge H(X)-H(Z)$
å‡è®¾çªƒå¬è€…ä¸ç§˜å¯†ä¹‹é—´æ¯«æ— å…³è”$I(X;Y) = 0$
é‚£ä¹ˆå¦‚æœè¦ä½¿$I(X;Y)=0$, æˆ‘ä»¬éœ€è¦$H(X)\le H(Z)$, å³ä¿¡æ¯é•¿åº¦å°äºå¯†é’¥é•¿åº¦.


## Fano's Inequality: Estimation

### Background

- Suppose that we wish to estimate a random variable ğ‘‹with a distribution $ğ‘(ğ‘¥)$.
- We observe a random variable $ğ‘Œ$ that is related to ğ‘‹by the conditional distribution $ğ‘(ğ‘¦|ğ‘¥)$.
- From ğ‘Œ, we calculate a function $ğ‘”(ğ‘Œ)=\hat{X}$, where $\hat{X}$ ğ‘‹is an estimate of ğ‘‹and takes on values in $\hat{X}$.
  - We will not restrict the alphabet $\hat{X}$ to be equal to ğ‘‹, and we will also allow the function ğ‘”(ğ‘Œ)to be random.
- We wish to bound the probability that $\hat{X}$â‰ ğ‘‹. We observe that ğ‘‹â†’ğ‘Œâ†’$\hat{X}$ forms a Markov chain. Define the probability of error
  $P_e=Pr(\hat{X} \neq X)$
- When ğ»(ğ‘‹|ğ‘Œ)=0, we know that $P_e=0$. How about $ğ»(ğ‘‹|ğ‘Œ)$,as $P_e\rightarrow 0$?

> **Theorem (Fanoâ€™s Inequality)** For any estimator $\hat{X}$ such that $X \rightarrow Y \rightarrow \hat{X}$ with $P_e = \Pr(\hat{X}\neq X)$ we have
> $$H\left(P_{e}\right)+P_{e} \log |\mathcal{X}| \geq H(X | \hat{X}) \geqslant H(X | Y)$$
> Or can be weakened to
> $$1+P_{e} \log |\mathcal{X}| \geqslant H(X | Y) \text{ or } P_{e} \geqslant \frac{H(X| Y)-1}{\log |x|}$$
> 
> åè€…æ˜¯data-processing ä¸ç­‰å¼, å‰è€…æ˜¯æ³•è¯ºä¸ç­‰å¼çš„æ ¸å¿ƒéƒ¨åˆ†.

### Proof

Define an error random variable 
$$E=\left\{\begin{array}{ll}
0, & \text { if } \hat{X}=X \\
1, & \text { if } \hat{X} \neq X
\end{array}\right.$$
Then
$$\begin{aligned}
H(E, X | \hat{X}) &=H(X | \hat{X})+H(E | X, \bar{X}) \\
&=H(E | \hat{X})+H(X | E, \hat{X})
\end{aligned}$$

- é©¬å°”å¯å¤«é“¾implies$H(E | X, \hat{X})=0$
- ç¬¬äºŒæ­¥,  $H(X|\hat{X},E=1)<=H(x)=H(P_e)$, ç†µæ°¸è¿œå°äºå­—æ¯è¡¨çš„å¯¹æ•°å€¼
- æ­¤å¤–, $H(X | E, \hat{X}) \leq P_{e} \log |x|$ å› ä¸º
$$\begin{aligned}
H(X | E, \hat{X}) &=\operatorname{Pr}(E=0) H(X | \hat{X}, E=0)+\operatorname{Pr}(E=1) H(X | \hat{X}, E=1) \\
& \leq\left(1-P_{e}\right) 0+P_{e} \log |\mathcal{X}|
\end{aligned}$$

**Corollary** Let $P_{e}=\operatorname{Pr}(X \neq \hat{X})$, and let $\hat{X}: y \rightarrow x$; then $H\left(P_{e}\right)+P_{e} \log (|x|-1) \geq H(X | Y)$, ç”±äºå·²çŸ¥$X$å’Œ$\hat{X}$ä¸ç­‰, åœ¨ä¼°è®¡æ—¶, ç†µçš„ä¸Šç•Œå¯ä»¥è°ƒå° (corollary)

ç›´è§‚ç†è§£:
$P_e \rightarrow 0$ implies $H(P_e) \rightarrow 0$ implies $H(X|Y)\rightarrow 0$ é”™è¯¯ç‡è¶‹å‘äº0æ—¶, Xå’ŒYçš„å…³ç³»è¶‹å‘ç¡®å®š.

Recall: binary entropy function
$H(p)=-plogp-(1-p)log(1-p)$ å®é™…æ˜¯ç®€å†™çš„è®°å·, è®¡ç®—çš„æ˜¯ä¸¤ç‚¹åˆ†å¸ƒçš„ç†µ

## Convexity/ Concavity of Information Measures

**Log Sum Inequality** for nonnegative $a_1, \cdots, a_n$ and $b_1, \cdots, b_n$,
$$\sum_{i=1}^{n} a_{i} \log \frac{a_{i}}{b_{i}} \geq\left(\sum_{i=1}^{n} a_{i}\right) \log \frac{\sum_{i=1}^{n} a_{i}}{\sum_{i=1}^{n} b_{i}}$$
with equality if and only if $\frac{a_i}{b_i}=const$.
Pf. by moving $\left(\sum_{i=1}^{n} a_{i}\right)$ to the left, the coefficient can be regarded as a probability distribution.

Corollaries:
- Concavity of $H(P)$
- $(X,Y)~p(x,y)=p(x)p(y|x)$, then $I(X;Y)$is a concave function of $p(x)$ for fixed $p(y|x)$ and a convex function of$p(y|x)$ for fixed $p(x)$ 
  - Note given $p(x)$, $p(y|x) \rightarrow p(x,y) \rightarrowp(y)$
- Convexity of relative entropy. $D(p\|q)$is a convex function for pair $(p,q)$.
- å¯ä»¥æŠŠ $p(x), p(y)$ç­‰çœ‹ä½œé«˜ç»´ç©ºé—´ä¸Šçš„ä¸€ä¸ªç‚¹, è€Œä¸æ˜¯æ¦‚ç‡åˆ†å¸ƒ. for $X ={1, 2,3...,n}$, define $\mathbb{x}_1 = p_1, \mathbb{x}_2= p_2, ..$


# 0316 AEP


## Law of Large Numbers

éšæœºå˜é‡çš„æ”¶æ•›æ€§
1. **In probability** if for every $\epsilon > 0$, $\Pr\{|X_n-X|>\epsilon \} \rightarrow 0$
2. **In mean square** if $E(X_n-X)^2 \rightarrow 0$
3. **With probability 1**(almost surely) if $\Pr\{\lim_{n\rightarrow \infty} X_n = X\}$
Note $(2)\rightarrow(1)$ $(3)\rightarrow(1)$, proof by Malkov and Chebshev.

å¼ºå¤§æ•°å®šå¾‹: For i.i.d random variables, $\bar{X_n} \rightarrow E(X_1)$ with probability 1.
å¼±å¤§æ•°å®šå¾‹: For i.i.d random variables, $\bar{X_n} \rightarrow E(X_1)$ in probability.

## AEP (Asymptotic Equipartition Property)
æ¸è¿›å‡åˆ†æ€§, å¤§æ•°å®šå¾‹åœ¨ä¿¡æ¯è®ºä¸­çš„ä½“ç°.
**Theorem** If $X_{1}, X_{2}, \dots$ are i.i.d. $\sim p(x)$, then
$$\begin{aligned}
-\frac{1}{n} \log p\left(X_{1}, X_{2}, \ldots, X_{n}\right) &=-\frac{1}{n} \sum_{i} \log p\left(X_{i}\right) \\
& \rightarrow-E \log p(X) \text { in probability } \\
&=H(X)
\end{aligned}$$

**Proof.** 
AEPåº”ç”¨äºæ•°æ®å‹ç¼©ç®—æ³•ä¸­, æˆ‘ä»¬ç”¨æé™çš„è¯­è¨€å†™å‡ºè¯æ˜:

$$\begin{aligned}
H(X)-\epsilon & \leq-\frac{1}{n} \log p\left(X_{1}, X_{2}, \ldots, X_{n}\right) \leq H(X)+\epsilon \\
2^{-n(H(X)+\epsilon)} & \leq p\left(X_{1}, X_{2}, \ldots, X_{n}\right) \leq 2^{-n(H(X)-\epsilon)} \Rightarrow A_{\epsilon}^{(n)}
\end{aligned}$$
Reason:
- Functions of independent random variables are also independent random variables.
- Since the $X_i$ are i.i.d., so are $\log p\left(X_{i}\right)$
- By the weak law of large numbers


å°†æé™çš„å®šä¹‰å±•å¼€, æˆ‘ä»¬å¯ä»¥é€šè¿‡å¦‚ä¸‹æ–¹å¼å¯¼å‡ºå¹¶å®šä¹‰å…¸å‹é›†

$$H(X)-\epsilon \leq-\frac{1}{n} \log p\left(X_{1}, X_{2}, \ldots, X_{n}\right) \leq H(X)+\epsilon$$
ä¹Ÿå³
$$2^{-n(H(X)+\epsilon)} \leq p\left(X_{1}, X_{2}, \dots, X_{n}\right) \leq 2^{-n(H(X)-\epsilon)} \Rightarrow A_{\epsilon}^{(n)}$$
å¯¼å‡ºå…¸å‹é›†çš„æ¦‚å¿µ.



## Typical Set
**Definition**æ ·æœ¬ç©ºé—´ä¸­,æ‰€æœ‰æ»¡è¶³$2^{-n(H(X)+\epsilon)} \leq p\left(X_{1}, X_{2}, \ldots, X_{n}\right) \leq 2^{-n(H(X)-\epsilon)}$çš„$\left(x_{1}, x_{2}, \ldots, x_{n}\right)$

**Properties**
1. If $\left(x_{1}, x_{2}, \ldots, x_{n}\right) \in A_{e}^{(n)},$ then $H(X)-\epsilon \leq-\frac{1}{n} \log p\left(x_{1}, x_{2}, \ldots, x_{n}\right) \leq H(X)+\epsilon$
   **Proof.** è§ä¸Šä¸€èŠ‚çš„æ¦‚å¿µå¯¼å‡º
2. å…¸å‹é›†ä¸­æ‰€æœ‰å…ƒç´ æ¦‚ç‡ä¹‹å’Œæ¥è¿‘äº1. $\operatorname{Pr}\left\{A_{\epsilon}^{(n)}\right\} \geq 1-\epsilon$ for $n$ sufficiently large.
   **Proof.** ç”±AEP, æˆ‘ä»¬çŸ¥é“ç»™å®šä»»æ„$\epsilon$, å¯¹ä»»æ„$\delta > 0$, å­˜åœ¨$n_0$,å¯¹ä»»æ„$n>n_0$, 
   $$\Pr\left\{\left| -\frac{1}{n} \log p\left(X_{1}, X_{2}, \ldots, X_{n}\right)-H(X) \right| > \epsilon\right\} < \delta$$
   equivalently,
   $$\operatorname{Pr}\left\{\left|-\frac{1}{n} \log p\left(X_{1}, X_{2}, \ldots, X_{n}\right)-H(X)\right|<\epsilon\right\}>1-\delta$$
   æ³¨æ„å…¸å‹é›†çš„å®šä¹‰, æœ‰
   $$\operatorname{Pr}\left\{A_{\epsilon}^{(n)}\right\}= \operatorname{Pr}\left\{\left|-\frac{1}{n} \log p\left(X_{1}, X_{2}, \ldots, X_{n}\right)-H(X)\right|<\epsilon\right\}$$
   Setting $\delta=\epsilon$,
   $$\operatorname{Pr}\left\{A_{\epsilon}^{(n)}\right\} \geq 1-\epsilon$$
3. å…¸å‹é›†çš„å¤§å°å­˜åœ¨ä¸Šç•Œ:$\left|A_{e}^{(n)}\right| \leq 2^{n(H(X)+\epsilon)},$ where $|A|$ denotes the number of elements in the set $A$
   **Proof.**
   $$\begin{aligned}
    1 &=\sum_{x \in \mathcal{X}^{n}} p(x) \\
    & \geq \sum_{x \in A_{\epsilon}^{(n)}} p(x) \\
    & \geq \sum_{x \in A_{\epsilon}^{(n)}} 2^{-n(H(X)+\epsilon)} \\
    &=2^{-n(H(X)+\epsilon)}\left|A_{e}^{(n)}\right|_{(X)}
    \end{aligned}$$
    Thus, $\left|A_{\epsilon}^{(n)}\right| \leq 2^{n(H(X)+\epsilon)}$
    è¿›ä¸€æ­¥, æˆ‘ä»¬å¯ä»¥è¯´æ˜
    $$\frac{\left|\boldsymbol{A}_{\epsilon}^{(\boldsymbol{n})}\right|}{\left|x^{n}\right|} \leq \mathbf{2}^{n(\boldsymbol{H}(\boldsymbol{X})-\log |\boldsymbol{x}|)} \rightarrow \mathbf{0}$$
    (åœ¨Xéå‡åŒ€åˆ†å¸ƒæ—¶,$H(X)<\log{|X|}$)
    Recall:æ€§è´¨2å®è´¨è¯´æ˜äº†$\operatorname{Pr}\left(x^{n}\right) \approx \operatorname{Pr}\left(A_{\epsilon}^{(n)}\right)$, ç»“åˆè¿™ä¸¤ç‚¹æˆ‘ä»¬å¯ä»¥è·å¾—å¦‚å›¾æ‰€ç¤ºçš„å…¸å‹é›†çš„ç›´è§‚ç†è§£.
    ![](img/0316-1.png)
4. $\left|A_{\epsilon}^{(n)}\right| \geq(1-\epsilon) 2^{n(H(X)-\epsilon)}$ for $n$ sufficiently large.
   **Proof.** For sufficiently large $n, \operatorname{Pr}\left\{A_{\epsilon}^{(n)}\right\}>1-\epsilon,$ so that
   $$\begin{aligned}
    & 1-\epsilon<\operatorname{Pr}\left\{A_{e}^{(n)}\right\} \\
    \leq & \sum_{x \in A_{\epsilon}^{(n)}} 2^{-n(H(X)-\epsilon)} \\
    =& 2^{-n(H(X)-e)}\left|A_{e}^{(n)}\right|
    \end{aligned}$$
    Thus $\left|A_{\epsilon}^{(n)}\right| \geq(1-\epsilon) 2^{n(H(X)-\epsilon)}$

### High Probability Set
ç»™å‡ºæ›´å®½æ³›çš„å®šä¹‰, é«˜æ¦‚ç‡é›†
**Definition** For each $n=1,2, \ldots,$ let $B_{\delta}^{(n)} \subseteq x^{n}$ be the smallest set with
$$\operatorname{Pr}\left\{B_{\delta}^{(n)}\right\} \geq 1-\delta$$

**Theorem** é«˜æ¦‚ç‡é›†å…ƒç´ å¤§å°çš„ä¸‹ç•Œ Let $X_{1}, X_{2}, \ldots, X_{n}$ be i.i.d $\sim p(x) .$ For $\delta<\frac{1}{2}$ and any $\delta^{\prime}>0,$ if $\operatorname{Pr}\left\{B_{\delta}^{(n)}\right\} \geq 1-\delta,$ then 
$$\frac{1}{n} \log \left|B_{\delta}^{(n)}\right|>H-\delta^{\prime}$$
for $n$ sufficiently large.

é«˜æ¦‚ç‡é›†å’Œ$2^{nH}$æ˜¯åŒé˜¶çš„, æˆ‘ä»¬å¯ä»¥è¯´å…¸å‹é›†å¯èƒ½æ˜¯æœ€å°çš„é«˜æ¦‚ç‡é›†.

**Intuition** As $A_{\epsilon}^{(n)}$ has $2^{n(H \pm \epsilon)}$ elements, $\left|B_{\delta}^{(n)}\right|$ and $\left|A_{\epsilon}^{(n)}\right|$ are equal to the first order in the exponent
Idea: é«˜æ¦‚ç‡é›†å’Œå…¸å‹é›†çš„æ¦‚ç‡åˆ†å¸ƒçš„äº¤é›†åº”è¯¥ä¹Ÿæ˜¯å¾ˆå¤§çš„, å¦åˆ™ç¼ºå¤±ä¼šæ¯”è¾ƒä¸¥é‡, ä¼šä¸1æœ‰æ˜æ˜¾çš„å·®è·. å› æ­¤æˆ‘ä»¬çš„è¯æ˜å°±æ˜¯ç ”ç©¶$\operatorname{Pr}\left(A_{\epsilon}^{(n)} \cap B_{\delta}^{(n)}\right)$

**Proof.** For any two sets $A, B,$ if $\operatorname{Pr}(A) \geq 1-\epsilon_{1} \operatorname{Pr}(B) \geq 1-\epsilon_{2},$ then $\operatorname{Pr}(A \cap B)>1-\epsilon_{1}-\epsilon_{2}$ é¦–å…ˆä»æ¦‚ç‡æ„ä¹‰ä¸Šå¾—åˆ°ä¸€ä¸ªtrivialçš„ç»“è®º

$$\begin{array}{l}
1-\epsilon-\delta \leq \operatorname{Pr}\left(A_{\epsilon}^{(n)} \cap B_{\delta}^{(n)}\right)=\sum_{A_{\epsilon}^{(n)} \cap B_{\delta}^{(n)}} p\left(x^{n}\right) \leq \sum_{A_{\epsilon}^{(n)} \cap B_{\delta}^{(n)}} 2^{-n(H-\epsilon)} \\ 
=\left|A_{\epsilon}^{(n)} \cap B_{\delta}^{(n)}\right| 2^{-n(H-\epsilon)} \leq\left|B_{\delta}^{(n)}\right| 2^{-n(H-\epsilon)}
\end{array}$$
å¾—å‡ºç»“è®º: é«˜æ¦‚ç‡é›†å¿…å®šå æ®äº†å…¸å‹é›†å¤§éƒ¨åˆ†çš„ç©ºé—´
$$\left|B_{\delta}^{(n)}\right| \geq\left|A_{\epsilon}^{(n)} \cap B_{\delta}^{(n)}\right| \geq 2^{n(H-\epsilon)}(1-\epsilon-\delta)$$

## Data Compression
æ•°æ®æº(iid) $X^n = (X_1,...,X_n)$ ----> Encoder ----m bits --> Decoder --> $\hat{X}^n$

### Problem Formulation
- Source: $X_{1}, X_{2}, \ldots,$ are i.i.d. $\sim p(X) .$æ•°æ®æº, iidçš„å‡è®¾è™½ç„¶æœ‰æ—¶å¯¹å®é™…é—®é¢˜è¿‡äºå¼º, ä½†ç†è®ºä¸Šæ˜¯éœ€è¦çš„
- Source sequences: $X^{n}=\left(X_{1}, \ldots, X_{n}\right)$ denotes the $n$ -tuple that represents a sequence of $n$ source symbols
- Alphabet: $x=\{1,2, \ldots,|x|\}-$ the possible values that each $X_{i}$ can take on 
- Encoder and decoder are a pair of functions $f, g$ such that $$f: x \rightarrow\{0,1\}^{*}$ and $g:\{0,1\}^{*} \rightarrow x$$ 
- Probability of error $P_{e}=P\left(X^{n} \neq \bar{X}^{n}\right)$ æˆ‘ä»¬é€šè¿‡è§£ç å™¨è·å¾—ä¿¡æ¯, å¸Œæœ›è§£ç åçš„é”™è¯¯ç‡èƒ½å¤Ÿåœ¨nå¾ˆå¤§æ—¶,æ— ç©·è¶‹å‘äº0
  - If $P_{e}=0,$ "lossless" æ— æŸç¼–ç , otherwise "lossy" æœ‰æŸç¼–ç 
- The rate of a scheme ç ç‡: $R=\frac{m}{n}(R=\log |X| \text { is trivial! })$ nä¸ªéšæœºå˜é‡ç”¨mä¸ªç æ¥ç¼–, è¿™é‡Œçš„Rä¸ä¸€å®šæ˜¯æœ€ä¼˜çš„, æˆ‘ä»¬å¸Œæœ›æ‰¾åˆ°å°½å¯èƒ½å°çš„R.(åŠå…¶å¯¹åº”çš„encoder,decoder)
ToDo: Find an encoder and decoder pair such that $P_{e} \rightarrow 0$, as $n \rightarrow \infty$

### Procedure

![](img/0316-2.png)
å®é™…ä¸­, æˆ‘ä»¬æ²¡æœ‰å¿…è¦ç»™æ¯ä¸ªæ ·æœ¬ç›¸åŒçš„ç¼–ç é•¿åº¦, æˆ‘ä»¬è¦å¯¹æ ·æœ¬ç©ºé—´è¿›è¡Œåˆ’åˆ†. æ¯”å¦‚, å¯¹å…¸å‹é›†ç­‰é«˜æ¦‚ç‡é›†åŒºåˆ†å¼€æ¥
- éå…¸å‹é›†ä¸­, æˆ‘ä»¬è‡³å°‘éœ€è¦ $n\log|\mathcal{X}| + 1 + 1$ 1ä¸ªbitå‡‘æ•´,å¦ä¸€ä¸ªbitç”¨ä»¥åŒºåˆ†éå…¸å‹é›†
- å…¸å‹é›†éœ€è¦ $n(H+\epsilon) + 1 + 1$åŒºåˆ†

Divide and conquer: $x^{n} \in A_{\epsilon}^{(n)}$ and $x^{n} \notin A_{\epsilon}^{(n)}$
- $x^{n} \in A_{\epsilon}^{(n)}:$
  - since there are $\leq 2^{n(H+\epsilon)}$ sequences in $A_{\epsilon}^{(n)},$ the indexing requires no more than $n(H+$ $\left.e^{\epsilon}\right)+1$ bits. $[\text { The extra bit may be necessary because } n(H+\epsilon)$ may not be an integer.] 
- $x^{n} \notin A_{\epsilon}^{(n)}:$
  - Slimilarly, we can index each sequence not in $A_{\epsilon}^{(n)}$ by using not more than $n \log |X|+1$ bits.
- To deal with overlap in the $\{0,1\}$ sequences, ä½†è¿™æ ·çš„ç¼–ç å¯èƒ½ä¼šå¸¦æ¥å†²çª, æ¯”å¦‚{0,0},{0,0}, æ‰€ä»¥æˆ‘ä»¬åœ¨èµ·å§‹ä½ç½®å†åŠ ä¸€ä½è¿›è¡ŒåŒºåˆ†
  - We prefix all these sequences by a 0 , giving a total length of $\leq n(H+\epsilon)+2$ bits to represent each sequence in $A_{\epsilon}^{(n)}$
  - Prefixing these indices by 1 , we have a code for all the sequences in $X^{n}$.

### Analysis

åˆ†æä¸€ä¸‹æœŸæœ›é•¿åº¦.
$$\begin{aligned}
E\left(l\left(X^{n}\right)\right) &=\sum_{x^{n}} p\left(x^{n}\right) l\left(x^{n}\right) \\
&=\sum_{x^{n} \in A_{\epsilon}^{(n)}} p\left(x^{n}\right) l\left(x^{n}\right)+\sum_{x^{n} \in A_{\epsilon}^{(n)}} p\left(x^{n}\right) l\left(x^{n}\right) \\
& \leq \sum_{x^{n} \in A_{\epsilon}^{(n)}} p\left(x^{n}\right)(n(H+\epsilon)+2)+\sum_{x^{n} \in A_{\epsilon}^{(n)}} p\left(x^{n}\right)(n \log |x|+2) \\
&=\operatorname{Pr}\left\{A_{\epsilon}^{(n)}\right\}(n(H+\epsilon)+2)+\operatorname{Pr}\left\{\left(A_{\epsilon}^{(n)}\right)^{c}\right\}(n \log |x|+2) \\
& \leq n(H+\epsilon)+\epsilon n(\log |x|)+2 \\
&=n\left(H+\epsilon^{\prime}\right) \text{ (}\epsilon^{\prime}\text{æ˜¯å¦ä¸€ä¸ªåŒæ ·ä¸€é˜¶çš„æ— ç©·å°é‡)}
\end{aligned}$$

å¯¼å‡ºå¹³å‡é•¿åº¦çš„æœŸæœ›å€¼æ˜¯ä»¥$H(X)$ä¸ºä¸Šç•Œçš„
$$E\left[\frac{1}{n} l\left(X^{n}\right)\right] \leq H(X)+\epsilon$$

Thus, we can represent sequences $X^n$ using $nH(X)$ bits on the average. æ‰èƒ½ä¿è¯æ¢å¤

ä¸ºäº†è¯´æ˜è¿™ä¸€ç‚¹, æˆ‘ä»¬è¿˜è¦è¯´æ˜H(X)æ˜¯æœ€å°çš„ç ç‡
**Converse** For any scheme with rate $r<H(X), P_{e} \rightarrow 1$ ä¸ä»…ä¸è¶‹å‘äº0, è€Œä¸”ç›´æ¥è¶‹å‘äº1
**Proof.** Let $r=H(X)-\epsilon .$ For any scheme with rate $r,$ it can encode at most $2^{n r}$ different symbols in $\mathcal{X}^{n} .$ The correct decoding probability is $\approx 2^{n r} 2^{-n H}=2^{-n(H-r)} \rightarrow 0$
Thus, $P_e \rightarrow 1$

# 0318 Entropy Rate


AEPç ”ç©¶äº†ç‹¬ç«‹åŒåˆ†å¸ƒçš„éšæœºå˜é‡åˆ—, æˆ‘ä»¬å¸Œæœ›å¾—åˆ°æ›´å¹¿æ³›çš„ç»“è®º.

## Stochastic Process


### Introduction
A stochastic process $\left\{X_{i}\right\}$ is an indexed sequence of random variables.

ä¸€ä¸ªä¾‹å­: èµŒå¾’çš„ç ´äº§.
- ä¸‹ä¸€å±€çš„è¾“èµ¢æ¦‚ç‡ä¸ä¸Šä¸€å±€æ— å…³
- pæ¦‚ç‡å¾—1å…ƒ,1-pæ¦‚ç‡ä¸¢ä¸€å…ƒ
- å¾—åˆ° $X_{i+1}=X_{i} \pm 1$
- Thus $X_{i}$ 's are not i.i.d.

### Stationary Process
**Definition** A stochastic process is said to be
stationary (ç¨³æ€) if the **joint distribution** of _**any subset**_ **of the sequence of random variables is invariant** with respect to shifts in the time
index; that is, ä»»æ„éšæœºå˜é‡çš„åˆ†å¸ƒæŒ‰ç…§æ—¶é—´å¹³ç§»,å¾—åˆ°çš„è”åˆæ¦‚ç‡åˆ†å¸ƒæ˜¯ä¸å˜çš„.
$$\begin{aligned}
& \operatorname{Pr}\left\{X_{1}=x_{1}, X_{2}=x_{2}, \ldots, X_{n}=x_{n}\right\} \\
=& \operatorname{Pr}\left\{X_{1+l}=x_{1}, X_{2+l}=x_{2}, \ldots, X_{n+l}=x_{n}\right\}
\end{aligned}$$
for every $n$ and every shift $l$ and for all $x_{1}, x_{2}, \ldots, x_{n} \in X$

**æ€§è´¨**:
1. å¹³ç§»ä¸å˜æ€§:
   - $p\left(X_{1}\right)=p\left(X_{2}\right)=\cdots=p\left(X_{n}\right)$
   - $p\left(X_{1}, X_{3}\right)=p\left(X_{2}, X_{4}\right) \dots$
2. é«˜æ–¯è¿‡ç¨‹æ˜¯ä¸€ä¸ªç¨³æ€è¿‡ç¨‹
3. è¾¾åˆ°ç¨³å®šçŠ¶æ€åçš„é©¬å°”å¯å¤«é“¾, åŒåˆ†å¸ƒè€Œä¸ç‹¬ç«‹
4. æˆ‘ä»¬çš„å®šä¹‰ä¸­åªè¯´æ˜äº†ç¨³æ€çš„åˆ†å¸ƒ, è¿™æ˜¯å¼ºç¨³æ€, ä¸ä¹‹ç›¸å¯¹åº”çš„æ˜¯å¼±ç¨³æ€. åœ¨ä½¿ç”¨æ—¶, ä¸¤è€…æ²¡æœ‰ä¸¥æ ¼çš„æ¨å‡ºå…³ç³»

é€šè¿‡ç¨³æ€åˆ†å¸ƒçš„å®šä¹‰, æˆ‘ä»¬å¯ä»¥è¯æ˜ä¸€äº›ç»“è®º.
**Theorem** æ—¶é—´å•å‘æ€§, Time's arrow. Let $\left\{X_{i}\right\}_{i=-\infty}^{\infty}$ be a stationary stochastic process. Prove that
$$H\left(X_{0} | X_{-1}, X_{-2}, \dots, X_{-n}\right)=H\left(X_{0} | X_{1}, X_{2}, \dots, X_{n}\right)$$
å³æ—¶é—´ä¸Šçš„å‰åè¡ŒåŠ¨å¯¹ç³»ç»Ÿæ²¡æœ‰å½±å“.

**Proof.** å¹³ç§»ä¸å˜æ€§, é¦–å…ˆæœ‰ç›¸åŒåˆ†å¸ƒ$H\left(X_{-n}, \dots, X_{0}\right)=H\left(X_{0}, \dots, X_{n}\right)$, $H\left(X_{-n}, \ldots, X_{-1}\right)=H\left(X_{1}, \ldots, X_{n}\right)$. ç›¸å‡å¾—è¯.


### Markov Chain

The Markov chain is said to be **time invariant** if the conditional probability $p\left(x_{n+1} | x_{n}\right)$ does not depend on $n ;$ that is, for $n=1,2, \ldots$
$$\operatorname{Pr}\left\{X_{n+1}=b | X_{-} n=a\right\}=\operatorname{Pr}\left\{X_{2}=b | X_{1}=a\right\} \quad \text{for all } a, b \in X$$
We will assume that the Markov chain **is time invariant unless otherwise stated**
A time-invariant Markov chain is characterized by its initial state =and a probability transition matrix $P=\left[P_{i j}\right], i, j \in\{1,2, \ldots, m\},$ where
$$P_{i j}=\operatorname{Pr}\left\{X_{n+1}=j | X_{n}=i\right\}$$
Example:
- Gambler's ruin
- Random Walk

### Stationary Distribution of MC
- By the definition of stationary, a Markov chain is stationary iff $p\left(X_{n+1}\right)=p\left(X_{n}\right)$
- If the probability mass function at time $n$ is $p\left(x_{n}\right)$, then
$$
p\left(x_{n+1}\right)=\sum_{x_{n}} p\left(x_{n}\right) P_{x_{n} x_{n+1}} \text { or } x^{T} P=x^{T}
$$
- If the initial state of a Markov chain is drawn according to a stationary distribution, the Markov chain is stationary
- Example: Consider a two state Markov chain with a probability transition matrix
$$P=\left[\begin{array}{cc}1-\alpha & \alpha \\ \beta & 1-\beta\end{array}\right]$$

$$\left(\mu_{1}, \mu_{2}\right)\left[\begin{array}{cc}
1-\alpha & \alpha \\
\beta & 1-\beta
\end{array}\right]=\left(\mu_{1}, \mu_{2}\right)$$

å¦è§£, å¯¹å°è§„æ¨¡çš„ç½‘ç»œ, ç”±æœ€å¤§æµ-æœ€å°å‰²å®šç†, è€ƒè™‘ä»»æ„å‰²é›†ä¹‹é—´çš„æµè¿›ä¸æµå‡ºæ¦‚ç‡ä¸º0, For stationary distribution, the net probability flow across any cut set in zero
$$\mu_{1} \alpha=\mu_{2} \beta$$
$$\begin{array}{c}
\mu_{1}+\mu_{2}=1 \\
\mu_{1}=\frac{\beta}{\alpha+\beta} \text { and } \mu_{2}=\frac{\alpha}{\alpha+\beta}
\end{array}$$


## Entropy Rate
å¯¹å¤æ‚ç³»ç»Ÿ, æˆ‘ä»¬éš¾ä»¥ç”¨ä¸€ä¸ªæ—¶åˆ»éšæœºå˜é‡çš„ç†µ, æˆ‘ä»¬å¸Œæœ›æè¿°ç†µçš„æ¼”åŒ–å½¢å¼. æˆ‘ä»¬å–ç³»ç»Ÿè”åˆç†µçš„æé™.
**Definition** The entropy rate of a stochastic process $\left\{X_{i}\right\}$ is defined by
$$
H(x)=\lim _{n \rightarrow \infty} \frac{1}{n} H\left(X_{1}, X_{2}, \ldots, X_{n}\right)
$$
when the limits exists(ç†µç‡ä¹Ÿå¯èƒ½ä¸å­˜åœ¨)

è®¡ç®—æ–¹å¼: 
$$H\left(X_{n}, \dots, X_{1}\right)=\sum_{i=1}^{n} H\left(X_{i} | X_{i-1}, \dots, X_{1}\right)$$
For $H\left(X_{i} | X_{i-1}, \ldots, X_{1}\right),$ we now need to make clear of
- the existence of 
  $$\lim _{n \rightarrow \infty} H\left(X_{n} | X_{n-1}, \ldots, X_{1}\right)$$
- In a series $\left\{a_{n}\right\},$ if $a_{n} \rightarrow a,$ the existence of
$$
\lim _{n \rightarrow \infty} \frac{1}{n} \sum_{i=1}^{n} a_{i}
$$


### H'(X)

å¯¹ç¨³æ€éšæœºè¿‡ç¨‹, æˆ‘ä»¬æœ‰å¦‚ä¸‹æ€§è´¨: 
**Theorem** For a stationary stochastic process, $H\left(X_{n} | X_{n-1}, \ldots, X_{1}\right)$ is nonincreasing in $n$ and has a limit.
**Proof.** 
$$\begin{aligned}
& H\left(X_{n+1} | X_{n}, \ldots, X_{1}\right) \\
\leq & H\left(X_{n+1} | X_{n}, \ldots, X_{2}\right) \\
=& H\left(X_{n} | X_{n-1}, \ldots, X_{1}\right) \\
H &\left(X_{n} | X_{n-1}, \ldots, X_{1}\right) \geq 0
\end{aligned}$$
æ ¹æ®æ•°åˆ—æé™çš„ç»“è®º(MCT): since $\left\{H\left(X_{n} | X_{n-1}, \ldots, X_{1}\right)\right\}$ is nonincreasing and $H\left(X_{n} | X_{n-1}, \ldots, X_{1}\right) \geq 0,$ the limit exists.

**Define**
$$H^{\prime}(x)=\lim _{n \rightarrow \infty} H\left(X_{n} | X_{n-1}, X_{n-2}, \ldots, X_{1}\right)$$
we have The limits $H^{\prime}(x)$ exists

### Cesaro Mean
Recall in Calculus, 
$$\text { If } a_{n} \rightarrow a \text { and } b_{n}=\frac{1}{n} \sum_{i=1}^{n} a_{i}, \text { then } b_{n} \rightarrow a$$
**Proof.**
Let $\epsilon>0 .$ since $a_{n} \rightarrow a$, there exists a number $N(\epsilon)$ such that $\left|a_{n}-a\right| \leq \epsilon$ for all $n \geq N(\epsilon) .$ Hence
$$\begin{aligned}
\left|b_{n}-a\right| &=\left|\frac{1}{n} \sum_{i=1}^{n}\left(a_{i}-a\right)\right| \\
& \leq \frac{1}{n} \sum_{i=1}^{n}\left|\left(a_{i}-a\right)\right| \\
& \leq \frac{1}{n} \sum_{i=1}^{N(c)}\left|a_{i}-a\right|+\frac{n-N(\epsilon)}{n} \epsilon \\
& \leq \frac{1}{n} \sum_{i=1}^{N(\epsilon)}\left|a_{i}-a\right|+\epsilon
\end{aligned}$$
Thus, $\left|b_{n}-a\right| \leq \epsilon^{\prime},$ for all $n \geq N(\epsilon)$.

### Entropy Rate for Stationary Process

**Theorem.** For a **stationary stochastic process**, the limits in $H(X)$ and $H^{\prime}(X)$ exist and are equal:
$$
H(x)=H^{\prime}(x)
$$
![](img/0318-1.png)

### Entropy Rate for Markov Chain

For a _**stationary**_ **Markov chain**, the entropy rate is given by
$$
\begin{aligned}
H(x) &=H^{\prime}(x)=\lim H\left(X_{n} | X_{n-1}, \ldots, X_{1}\right)=\lim H\left(X_{n} | X_{n-1}\right) \\
&=H\left(X_{2} | X_{1}\right)
\end{aligned}
$$
where the conditional entropy is calculated using the given stationary distribution.

Recall that the stationary distribution $\mu$ is the solution of the equations
$$
\mu_{j}=\sum_{i} \mu_{i} P_{i j} \text { for all } j
$$

**Theorem** Let $\left\{X_{i}\right\}$ be a stationary Markov chain with stationary distribution $\mu$ and transition matrix $P .$
Let $X_{1} \sim \mu$. Then the entropy rate is
$$
H(x)=-\sum_{i j} \mu_{i} P_{i j} \log P_{i j}
$$
**Proof.** 
$$H(x)=H\left(X_{2} | X_{1}\right)=\sum_{i} p\left(x_{i}\right) H\left(X_{2} | X_{1}=x_{i}\right)=\sum_{i} \mu_{i}\left(\sum_{j}-P_{i j} \log P_{i j}\right)$$

### Example: Random Walk

Undirected graph with weight $W_{i j} \geq 0$ and $W_{i j}=W_{j i}$
æˆ‘ä»¬è®¡ç®—å¦‚ä¸‹å‚é‡, è§£é¢˜æ€è·¯:
$$
\begin{array}{c}
P_{i j}=W_{i j} / \sum_{k} W_{i k} \\
W_{i}=\sum_{j} w_{i j}
\end{array}
$$
$W=\sum_{i} \frac{W_{i}}{2}$
The stationary distribution is
$$
\mu_{i}=\frac{W_{i}}{2 W}
$$
Verify it by $\mu P=\mu$
$$
\begin{array}{l}
H(x)=H\left(X_{2} | X_{1}\right) \\
=H\left(\ldots \frac{W_{i j}}{2 W}, \ldots\right)-H\left(\ldots, \frac{W_{i}}{2 W}, \ldots\right)
\end{array}
$$

## Second Law of Thermodynamics

Intuition: æˆ‘ä»¬è§‚æµ‹çš„çŠ¶æ€ä¼šå—åˆ°åŸºæœ¬åŸåˆ™çš„æ§åˆ¶. æˆ‘ä»¬å°†ç‰©ç†è¿‡ç¨‹æŠ½è±¡æˆé©¬å°”å¯å¤«é“¾, åŸºæœ¬åŸç†åˆ™æŠ½è±¡æˆè½¬ç§»çŸ©é˜µ

- One of the basic laws of physics, the second law of thermodynamics, states that the entropy of an isolated system is nondecreasing.
- We model the isolated system as a **Markov chain with transitions obeying the physical laws governing the system**.
  - Implicit in this assumption is the notion of an overall state of the system and the fact that knowing the present state, the future of the system is independent of the past.

Some Results
- Relative entropy $\mathrm{D}\left(\mu_{\mathrm{n}} \| \mu_{\mathrm{n}}^{\prime}\right)$ decreases with $n$ 
  (Pf. by ç›¸å¯¹ç†µé“¾å¼æ³•åˆ™, å› ä¸ºè½¬ç§»çŸ©é˜µæ˜¯ä¸å˜çš„, ç³»ç»Ÿä¼šè¶‹äºç¨³å®š)
- The conditional entropy $H\left(X_{n} | X_{1}\right)$ increases with $n$ for a stationary Markov process
- Shuffles increase entropy: $H(TX) \geq H(X)$ 

Reference: Neri Merhav (2010), â€œStatistical Physics and Information Theory,â€ Foundations and TrendsÂ® in Communications and Information Theory

## Extension: Functions of Markov Chains
æœ¬èŠ‚çš„é‡ç‚¹æ˜¯ç†µç‡çš„æ€§è´¨, æ­¤å¤„ä»‹ç»ä¸€ä¸ªç®—æ³•, äº†è§£å³å¯.

é—®é¢˜èƒŒæ™¯: æˆ‘ä»¬é€šè¿‡ä¸€äº›ç°è±¡çŸ¥é“äº†ä¸€äº›çœ‹ä¸è§ç‰©è´¨çš„å­˜åœ¨. æˆ‘ä»¬ç”¨$X$è¡¨ç¤ºçœ‹ä¸åˆ°çš„ç°è±¡, ä»–ä»¬é€šè¿‡ä¸€äº›ç‰©ç†æ³•åˆ™, $\Phi$ ç”Ÿæˆäº†ç°è±¡$Y$. å¦‚æœ$X$æ˜¯ç¨³æ€çš„, æˆ‘ä»¬å¦‚ä½•è®¡ç®—è§‚æµ‹ç»“æœçš„ç†µç‡?

> $$
> \begin{array}{ccccc}
> X_{1} & X_{2} & \ldots & X_{n} & \ldots \\
> \downarrow & \downarrow & \ldots & \downarrow & \ldots \\
> Y_{1}=\phi\left(X_{1}\right) & Y_{2}=\phi\left(X_{2}\right) & \ldots & Y_{n}=\phi\left(X_{n}\right) & \ldots
> \end{array}
> $$
> Let $X_{1}, X_{2}, \ldots, X_{n}, \ldots$ be a stationary Markov chain, and let $Y_{i}=\phi\left(X_{i}\right)$ be a process each term of which is a function of the corresponding state in the Markov chain. What is the entropy rate of $H(y)$ ?

- $\left\{Y_{i}\right\}:$ A very special case of hidden Markov model (HMM) éšé©¬å°”ç§‘å¤«æ¨¡å‹, åœ¨ä¿¡å·å¤„ç†ç­‰æƒ…å†µä¸‹ååˆ†å¸¸è§
- $\left\{Y_{i}\right\}$ is not a Markov chain in general å¹¶ä¸ä¸€å®šæ˜¯é©¬å°”å¯å¤«é“¾, å› ä¸ºYå–å†³äºå•ä¸ªXçš„å–å€¼. æˆ‘ä»¬æ˜¯æ— æ³•ç›´æ¥å¾—åˆ°Yä¹‹é—´çš„çŠ¶æ€è½¬ç§»çŸ©é˜µ.
- $\left\{X_{i}\right\}$ is stationary $\Rightarrow\left\{Y_{i}\right\}$ is stationary, ä½†ç¨³æ€åˆ†å¸ƒæ˜¯è‚¯å®šçš„, ç†µç‡è¿˜æ˜¯å¯ä»¥é€šè¿‡æ±‚æ¡ä»¶ç†µçš„æé™è·å¾—
$$
H(\boldsymbol{y})=\lim _{\boldsymbol{n} \rightarrow \infty} \boldsymbol{H}\left(\boldsymbol{Y}_{\boldsymbol{n}} | \boldsymbol{Y}_{\boldsymbol{n}-\mathbf{1}}, \ldots, \boldsymbol{Y}_{\mathbf{1}}\right)
$$
- Drawback: Hard to ensure the convergence by $n$
- Solution: We have already known that $H\left(Y_{n} | Y_{n-1}, \ldots, Y_{1}\right)$ is lower bounded by $H(Y)$ å·²çŸ¥ç†µç‡æ˜¯æ¡ä»¶ç†µçš„ä¸‹ç•Œ.
  - Find a lower bound for $H(y)$ which is close to $H\left(Y_{n} | Y_{n-1}, \ldots, Y_{1}\right)$
- Let's have a look at $X_{1}$
  - $X_{1}$ contains much information about $Y_{n}$ as $Y_{1}, Y_{0}, Y_{-1}, \ldots \ldots$
$$
\boldsymbol{H}\left(\boldsymbol{Y}_{\boldsymbol{n}} | \boldsymbol{Y}_{\boldsymbol{n}-\mathbf{1}}, \ldots, \boldsymbol{Y}_{\mathbf{1}}, \boldsymbol{X}_{\mathbf{1}}\right)
$$
$\left(Y_{1} \text { could be ignored }\right)$ è¿™æ˜¯æ²¡é—®é¢˜çš„, å› ä¸º$Y_1$æ˜¯$X_1$çš„å‡½æ•°.


**Theorem.** If $X_{1}, X_{2}, \ldots, X_{n}$ form a stationary Markov chain, and $Y_{i}=\phi\left(X_{i}\right),$ then
$$
H\left(Y_{n} | Y_{n-1}, \ldots, Y_{1}, X_{1}\right) \leq H(y) \leq H\left(Y_{n} | Y_{n-1}, \ldots, Y_{1}\right)
$$
and $\lim {\boldsymbol{H}\left(\boldsymbol{Y}_{\boldsymbol{n}} | \boldsymbol{Y}_{\boldsymbol{n}-\mathbf{1}}, \ldots, \boldsymbol{Y}_{\mathbf{1}}, \boldsymbol{X}_{\mathbf{1}}\right)}=\boldsymbol{H}(\boldsymbol{y})=\lim \boldsymbol{H}\left(\boldsymbol{Y}_{\boldsymbol{n}} | \boldsymbol{Y}_{\boldsymbol{n}-\mathbf{1}}, \ldots, \boldsymbol{Y}_{\mathbf{1}}\right)$ æˆ‘ä»¬å¸Œæœ›è¿ç”¨å¤¹é€¼å®šç†.

**Proof.** 
1. (handled) Yçš„æ¡ä»¶ç†µæ˜¯ç†µç‡çš„ä¸Šç•Œ
2. å¸¦$X_1$çš„æ¡ä»¶ç†µæ˜¯ç†µç‡çš„ä¸‹ç•Œ
   è¿ç”¨é©¬å°”å¯å¤«é“¾çš„æ€§è´¨æ·»åŠ è´Ÿé¡¹, æŒªå»æ¡ä»¶ç†µå¢å¤§, åº”ç”¨å¹³ç§»ä¸å˜æ€§, å‘ç°é€’å‡, è¿ç”¨MCT
$$\begin{aligned}
& H\left(Y_{n} | Y_{n-1}, \ldots, Y_{2}, X_{1}\right) \\
=& H\left(Y_{n} | Y_{n-1}, Y_{2}, Y_{1}, X_{1}\right) \\
=& H\left(Y_{n} | Y_{n-1}, \ldots, Y_{1}, X_{1}, X_{0}, X_{-1}, \ldots, X_{-k}\right) \\
=& H\left(Y_{n} | Y_{n-1}, \ldots, Y_{1}, X_{1}, X_{0}, X_{-1}, \ldots, X_{-k}, Y_{0}, \ldots, Y_{-k}\right) \\
\leq & H\left(Y_{n} | Y_{n-1}, \ldots, Y_{1}, Y_{0}, \ldots, Y_{-k}\right) \\
=& H\left(Y_{n+k+1} | Y_{n+k}, \ldots, Y_{1}\right) \\
k & \rightarrow \infty \\
& H\left(Y_{n} | Y_{n-1}, \ldots, Y_{2}, X_{1}\right) \leq H(\mathcal{Y})
\end{aligned}$$
3. å¸¦$X_1$æ¡ä»¶ä¸‹, éšç€$n$çš„å¢å¤§, ä¸ç­‰å¼ä¸¤è¾¹ç†µæ— é™æ¥è¿‘, å³ç ”ç©¶äº’ä¿¡æ¯è¶‹å‘äº0. 
$$H\left(Y_{n} | Y_{n-1}, \ldots, Y_{1}\right)-H\left(Y_{n} | Y_{n-1}, \ldots, Y_{1}, X_{1}\right) = I\left(X_{1} ; Y_{n} | Y_{n-1}, \ldots, Y_{1}\right)$$
é¦–å…ˆæ³¨æ„åˆ°äº’ä¿¡æ¯å°äºç†µ
$$
I\left(X_{1} ; Y_{1}, Y_{2}, \ldots, Y_{n}\right) \leq H\left(X_{1}\right)
$$
è¿™åœ¨æé™æ¡ä»¶ä¸‹ä¹Ÿæˆç«‹, è¿ç”¨é“¾å¼æ³•åˆ™å±•å¼€
$$\begin{aligned}
H\left(X_{1}\right) &\geq \lim _{n \rightarrow \infty} I\left(X_{1} ; Y_{1}, Y_{2}, \ldots, Y_{n}\right)\\
&=\lim _{n \rightarrow \infty} \sum_{i=1}^{n} I\left(X_{1} ; Y_{i} | Y_{i-1}, \ldots, Y_{1}\right)\\
&=\sum_{i=1}^{\infty} I\left(X_{1} ; Y_{i} | Y_{i-1}, \ldots, Y_{1}\right)\\
\end{aligned}$$
æ— ç©·çº§æ•°, æ¯ä¸€é¡¹éƒ½æ˜¯æ­£çš„, æé™å­˜åœ¨, é‚£ä¹ˆå¯¹nè¶³å¤Ÿå¤§çº§æ•°é¡¹è¶‹å‘äº0.
$$\begin{aligned}
&\begin{array}{c}
I\left(X_{1} ; Y_{n} | Y_{n-1}, \ldots, Y_{2}, Y_{1}\right) \rightarrow 0 \\
\| \\
\left(Y_{n} | Y_{n-1}, \ldots, Y_{2}, Y_{1}\right)-H\left(Y_{n} | Y_{n-1}, \ldots, Y_{1}, X_{1}\right)
\end{array}
\end{aligned}$$

# 0323 Data Compression (1)


## Example of Codes

Let $X$ be a random variable with the following distribution and codeword assignment:
$$\begin{array}{ll}
\operatorname{Pr}(X=1)=1 / 2, & \text { codeword } C(1)=0 \\
\operatorname{Pr}(X=2)=1 / 4, & \text { codeword } C(2)=10 \\
\operatorname{Pr}(X=3)=1 / 8, & \text { codeword } C(3)=110 \\
\operatorname{Pr}(X=4)=1 / 8, & \text { codeword } C(4)=111
\end{array}$$

- Without of loss of generality, we can assume that the $D$ -ary alphabet is $\mathcal{D}=$ $\{0,1, \ldots, D-1\}$. äºŒè¿›åˆ¶ä¸­,D=2
- ä¿¡æºç¼–ç  $C$ for a random variable $X$ is a mapping from $X$ to $D^{*}$, the set of finite-length strings of symbols from a D-ary (Då…ƒç»„) alphabet.
- Let $C(x)$ denote the codeword corresponding to $x$ and let $l(x)$ denote the length of $C(x)$
- The expected length $L(C)$ of a source code $C(x)$ for a random variable $X$ with probability mass function $p(x)$ is given by
$$L(C)=\sum_{x \in X} p(x) l(x)$$
  - What is min $L(C)$
  - How to construct such an optimal code
- Recall åœ¨AEPä¸­,ç ç‡å¤§äºç­‰äºç†µ

## Nonsingular Code
- ç¼–ç ç³»ç»Ÿä¸å¸Œæœ›ä¸¤ä¸ªå­—æ¯æœ‰åŒæ ·çš„ç¼–ç . A code is said to be nonsingular if every element of the range of $X$ maps into $a$ different string in $D^{*} ;$ that is,
$$x \neq x^{\prime} \Rightarrow C(x) \neq C\left(x^{\prime}\right)$$
- å®šä¹‰ç¼–ç çš„è¿æ¥ The extension $C^{*}$ of a code $C$ is the mapping from finite length strings of $X$ to finite-length strings of $D,$ defined by
  $$C\left(x_{1} x_{2} \ldots x_{n}\right)=C\left(x_{1}\right) C\left(x_{2}\right) \ldots C\left(x_{n}\right)$$

å”¯ä¸€å¯è§£ç , åœ¨æ‰©å±•æ˜¯éå¥‡å¼‚çš„æƒ…å†µä¸‹, ç¼–ç æ˜¯å¯è§£ç çš„. A code is called uniquely decodable if its extension is nonsingular.
- In other words, any encoded string in a uniquely decodable code has only one possible source string producing it.

åœ¨å¯¹ç³»ç»Ÿè¦æ±‚æ›´é«˜çš„æƒ…å†µä¸‹, æˆ‘ä»¬å¸Œæœ›å®æ—¶çš„è§£ç , 
- å‰ç¼€ç : ä»»æ„ç¼–ç éƒ½ä¸æ˜¯å¦ä¸€ä¸ªç çš„å‰ç¼€
- åç¼€ç : ä»»æ„ç¼–ç éƒ½ä¸æ˜¯å¦ä¸€ä¸ªç çš„åç¼€
How to construct?\

### Prefix Code

ç”±å‰ç¼€ç çš„æ€§è´¨, æˆ‘ä»¬å¯ä»¥ç”¨ä¸€äº›è‰¯å¥½çš„ç»“æ„æè¿°ç¼–ç æ–¹å¼.

å¦‚, æˆ‘ä»¬ç”¨ä¸‰å‰æ ‘è¡¨ç¤ºä¸‰è¿›åˆ¶çš„ç åˆ¶.
![](img/0323-1.png)

æˆ‘ä»¬ä¹Ÿå¯ä»¥ç”¨åŒºé—´æ¥è¡¨ç¤º.
![](img/0323-2.png)
å„ä¸ªç åˆ¶å¯¹åº”çš„å·¦é—­å³å¼€åŒºé—´æ˜¯$[0,1)$çš„ä¸€ä¸ªäº’ä¸ç›¸äº¤çš„åˆ†å‰².

### Kraft Inequality
ä»æ•°å­¦ä¸Šè§£å†³äº†å‰ç¼€ç å­˜åœ¨çš„ä¸€ä¸ªé‡è¦æ¡ä»¶.
(Kraft Inequality 1949 ) For any instantaneous code (prefix code) over an alphabet of size
$D,$ the codeword lengths $l_{1}, l_{2}, \ldots, l_{m}$ must satisfy the inequality 
$$\sum_{i=1}^{m} D^{-l_{i}} \leqq 1$$
Conversely, given a set of codeword lengths that satisfy this inequality, there exists an instgntgneous code with these word lengths.

ä»»æ„ä¸¤ä¸ªç åˆ¶å¯¹åº”çš„è·¯å¾„æ˜¯äº’ç›¸ä¸èƒ½è¦†ç›–çš„. å½“æˆ‘é€‰å®šä¸€ä¸ªè·¯å¾„ä¹‹å, æˆ‘å°±ä¸å¯èƒ½å†å°†è·¯å¾„çš„ç»ˆç‚¹å±•å¼€(å¶å­è½¬æ¢æˆå­æ ‘).

ä»»æ„ä¸¤ä¸ªç åˆ¶å¯¹åº”çš„è·¯å¾„æ˜¯äº’ç›¸ä¸èƒ½è¦†ç›–çš„. å½“æˆ‘é€‰å®šä¸€ä¸ªè·¯å¾„ä¹‹å, æˆ‘å°±ä¸å¯èƒ½å†å°†è·¯å¾„çš„ç»ˆç‚¹å±•å¼€(å¶å­è½¬æ¢æˆå­æ ‘).



Assume $l_{1} \leq l_{2} \ldots \leq l_{m}$ (The maximum depth is $\left.l_{m}\right)$

- For $l_{i},$ it "occupied" a subtree in size $D^{l_{m}-l_{i}}$ å®ƒæŠŠæ¥ä¸‹æ¥çš„ç©ºé—´è¦†ç›–äº†

- The aggregate size of subtrees $$ \sum_{i=1}^{m} D^{l_{m}-l_{i}} $$

- å¯¹Då‰æ ‘, åœ¨$l_m$å±‚æœ‰å¿…è¦æ¡ä»¶ $$ \sum_{i=1}^{m} D^{l_{m}-l_{i}} \leq D^{l_{m}} \Rightarrow \text { "only } i f^{\prime \prime} $$

- "if": mathematical induction åˆ©ç”¨å¯¹mè¿›è¡Œæ•°å­¦å½’çº³æ³•è¯æ˜å……åˆ†æ€§

- m-1: 

   $$ \sum_{i=1}^{m-1} D^{l_{m}-l_{i}} < D_{l_m} $$

- ä¸€å®šå­˜åœ¨æ²¡æœ‰è¦†ç›–çš„m

### Extended Kraft Inequality

(Extended Kraft Inequality) For any countably infinite set of codewords that form a prefix code, the codeword lengths satisfy the extended Kraft inequality,
$$\sum_{i=1}^{\infty} D^{-l_{i}} \leq 1$$
Conversely, given any $l_{1}, l_{2}, \ldots$ satisfying the extended Kraft inequality, we can construct a prefix code with these codeword lengths.

è¯æ˜ä¸Š, æ ‘çš„è¡¨ç¤ºä¸å†é€‚ç”¨. æˆ‘ä»¬ç”¨åŒºé—´çš„æ–¹æ³•.

å¿…è¦æ€§:

Let the D-ary alphabet be $\{0,1, \ldots, D-1\} .$ Consider the $i$ th codeword $y_{1} y_{2} \ldots y_{l_{i}}$
Let $0 . y_{1} y_{2} \ldots y_{l_{i}}$ be the real number given by the D-ary expansion
$$0 . y_{1} y_{2} \dots y_{l_{i}}=\sum_{j=1}^{l_{i}} y_{l_{j}} D^{-j}$$

This codeword corresponds to the interval
$$
\left[0 . y_{1} y_{2} \dots y_{i_{i}}, 0 . y_{1} y_{2} \dots y_{l_{i}}+\frac{1}{D^{l_{i}}}\right)
$$
- This is a subinterval of the unit interval [0,1
- By the prefix condition, these intervals are disjoint.

å……åˆ†æ€§:
å°†$l_1,l_2$é€’å¢æ’åº, ä»å·¦å¾€å³ä¾æ¬¡åˆ‡å‰²[0,1]åŒºé—´, å°±å¯ä»¥æ„é€ å‰ç¼€ç .



## Optimal Codes

### Problem Formulation
Kraft inequality gives a mathematical expression on the existence of prefix code The problem of finding the prefix code with the minimum expected length could be formulated as a standard optimization problem
$$
\min L=\sum p_{i} l_{i}
$$
such that $\sum D^{-l_{i}} \leq 1$

è€ƒè™‘åˆ°$l_i$æ˜¯æ•´æ•°è¾ƒä¸ºå¤æ‚, æˆ‘ä»¬é¦–å…ˆè€ƒè™‘å®æ•°çš„æƒ…å†µ

By Lagrange, their gradient vectors are parallel $\nabla f(X)=\lambda \nabla g$

### Solution

The Lagrange multipliers
$$J=\sum p_{i} l_{i}+\lambda\left(\sum D^{-l_{i}}-1\right)$$
Differentiating with respect to $l_{i},$ we obtain
$$
\frac{\partial J}{\partial l_{i}}=p_{i}-\lambda D^{-l_{i}} \log _{e} D
$$
Setting the derivatives to $0,$ we obtain
$$
D^{-l_{i}}=\frac{p_{i}}{\lambda \log _{e} D}
$$
åˆå› ä¸º$\sum D^{-l_i} = 1$

Substituting this in the constraint to find $\lambda$, we find $\lambda=1 / \log _{e} D,$ and hence
$$
\boldsymbol{p}_{i}=\boldsymbol{D}^{-\boldsymbol{l}_{i}}
$$
yielding optimal code lengths,
$$
l_{i}^{*}=-\log _{D} p_{i}
$$
This noninteger choice of codeword lengths yields expected codeword length
$$
L^{*}=\Sigma p_{i} l_{i}^{*}=\Sigma-p_{i} \log p_{i}=H_{D}(X)
$$
ä¸ä¸€å®šæ˜¯æ•´æ•°å–å¾—åˆ°, ä½†è¿™å¯ä»¥æ˜¯ä¸€ä¸ªä¸‹ç•Œ.
In general, $H_{D}(X)$ cannot be attained
$$
\boldsymbol{L}^{*} \geq \boldsymbol{H}_{\boldsymbol{D}}(\boldsymbol{X})
$$

ç„¶è€Œ, æˆ‘ä»¬é€šè¿‡æ¨å¯¼å¾—çŸ¥, åœ¨æœ€ä¼˜æƒ…å†µä¸‹, æœ‰ä¸‹é¢çš„å…³é”®å…³ç³».
$$\begin{aligned}
&p_{i}=D^{-l_{i}}\\
&l_{i}=-\log p_{i}
\end{aligned}$$



### Bounds

è¿›ä¸€æ­¥, ç åˆ¶åœ¨æœ€ä¼˜æƒ…å†µä¸‹çš„å¹³å‡æƒ…å†µèƒ½æ»¡è¶³ä»¥ä¸‹ä¸ç­‰å¼, å¾—åˆ°æœ€ä¼˜ç¼–ç çš„ä¸Šç•Œæ˜¯ç†µ+1bit
Let $l_{1}^{*}, l_{2}^{*}, \ldots, l_{m}^{*}$ be optimal codeword lengths for a source distribution $\mathbf{p}$ and $D$ - ary alphabet, and let $L^{*}$ be the associated expected length of an optimal code $\left(L^{*}=\sum p_{i} l_{i}^{*}\right)$ Then
$$H_{D}(X) \leq L^{*}<H_{D}(X)+1$$

**Proof.**
å‘ä¸Šå–æ•´
Recall that $p_{i}=D^{-l_{i}}$ and $l_{i}=-\log _{\mathrm{D}} p_{i}$
since $\log _{D} \frac{1}{p_{i}}$ may not equal to an integer, we round it up to give integer word-length assignments,
$$
l_{i}=\left|\log _{D} \frac{1}{p_{i}}\right| \Rightarrow \text { Shamon codes }
$$
æˆ‘ä»¬å¯ä»¥éªŒè¯æ–°çš„ç¼–ç é•¿åº¦æ»¡è¶³Kraftä¸ç­‰å¼(ç”±prefixåŒºé—´ä¸ç›¸äº¤çš„æ€§è´¨ä¿è¯)
Check $l_{i}^{\prime}$ s satisfying Kraft inequality.
$$
\log _{D} \frac{1}{p_{i}} \leq l_{i}<\log _{D} \frac{1}{p_{i}}+1
$$
Take expectations
$$
H_{D}(X) \leq L<H_{D}(X)+1
$$
ä¸‹èŠ‚è¯¾æˆ‘ä»¬ä¼šç»§ç»­ä»‹ç»èƒ½å¦æ¶ˆé™¤ä¸€ä¸ªbit

## Approach the limit
å°†nä¸ªéšæœºå˜é‡ä¸€åŒç»Ÿä¸€å¤„ç†
Encode $n$ symbols $X_{1}, X_{2}, \ldots, X_{n}$ on $X$ together, where $X_{i}^{\prime} s$ are i.i.d $\sim p(x)$ Denote the alphabet by $\mathcal{X}^{n}$æ•´ä½“å­—æ¯è¡¨, the expected codeword length by $L_{n},$ the length of codeword associated with $\left(x_{1}, x_{2}, \ldots, x_{n}\right)$ by $l\left(x_{1}, x_{2}, \ldots, x_{n}\right)$
$$
L_{n}=\frac{1}{n} \sum p\left(x_{1}, x_{2}, \ldots, x_{n}\right) l\left(x_{1}, x_{2}, \ldots, x_{n}\right)=\frac{1}{n} E l\left(X_{1}, X_{2}, \ldots, X_{n}\right)
$$
æˆ‘ä»¬è®¡ç®—çš„æ˜¯per symbol,æ‰€ä»¥è®°å¾—é™¤ä»¥n
Treat $X_{1}, X_{2}, \ldots, X_{n}$ as a whole and apply the lower bound aforementioned
$$
H\left(X_{1}, X_{2}, \ldots, X_{n}\right) \leq E l\left(X_{1}, X_{2}, \ldots, X_{n}\right)<H\left(X_{1}, X_{2}, \ldots, X_{n}\right)+1
$$
since $X_{i}^{\prime} s$ are i.i.d, $H\left(X_{1}, X_{2}, \ldots, X_{n}\right)=n H(X)$
$$
H(X) \leq L_{n} \leq H(X)+\frac{1}{n}
$$

å®šç†: åŒºå—ç¼–ç 
(Theorem.) The minimum expected codeword length per symbol statisfies
$$
\frac{H\left(X_{1}, X_{2}, \ldots, X_{n}\right)}{n} \leq L^{*}<\frac{H\left(X_{1}, X_{2}, \ldots, X_{n}\right)}{n}+\frac{1}{n}
$$
Moreover, if $X_{1}, X_{2}, \ldots, X_{n}$ is a stationary stochastic process,
$\rightarrow 11$

å½“nå¾ˆå¤§æ—¶, æœŸæœ›ä»¥ç†µç‡ä¸ºæé™, é€šè¿‡è¿™ç§æ–¹å¼, æˆ‘ä»¬å¯ä»¥é€šè¿‡é€¼è¿‘çš„æ–¹å¼æŠŠ+1bitå»æ‰, ä½†åå¤„æ˜¯å­—æ¯è¡¨å¤ªå¤§äº†$|\mathcal{X}|^n$ç åˆ¶æ•°é‡æ˜¯æŒ‡æ•°çº§çš„. ä½†è‡³å°‘, ç³»ç»Ÿçš„ç†µç‡æ˜¯ç¼–ç é—®é¢˜çš„æé™.

## Wrong Code

What happens to the expected description length if the code is designed for the wrong distribution $(q(x)) .$ For example, the wrong distribution may be the best estimate that we can make of the unknown true distribution.

Recall ç›¸å¯¹ç†µè¡¡é‡éšæœºå˜é‡ä¹‹é—´çš„è·ç¦». 

(Wrong code) The expected length under $p(x)$ of the code assignment $l(x)=\log \frac{1}{q(x)}$ satisfies
$$
H(p)+D(p \| q) \leq E_{p} l(x)<H(p)+D(p \| q)+1
$$

$D(p \| q)$ æ˜¯æˆ‘ä»¬ä¼°è®¡äº§ç”Ÿåå·®çš„æƒ©ç½šé¡¹, è¿™æ˜¯éš¾ä»¥é¿å…çš„, ä½†æˆ‘ä»¬å¯ä»¥ç”¨æ•°å€¼é‡åŒ–.

$$\begin{aligned}
E l(x) &=\sum_{x} p(x)\left[\log \frac{1}{q(x)}\right] \\
&<\sum_{x} p(x)\left(\log \frac{1}{q(x)}+1\right) \\
&=\sum_{x} p(x) \log \frac{p(x)}{q(x)} \frac{1}{p(x)}+1 \\
&=\sum_{x} p(x) \log \frac{p(x)}{q(x)}+\sum_{x} p(x) \log \frac{1}{p(x)}+1 \\
&=D(p \| q)+H(p)+1
\end{aligned}$$

## Kraft Inequality For Uniquely Decodable Codes
æˆ‘ä»¬è¿›ä¸€æ­¥è¯´æ˜, Kraftä¸ç­‰å¼å¯ä»¥æè¿°ä»»æ„å”¯ä¸€å¯è§£ç çš„ç¼–ç æ–¹å¼, å› æ­¤æˆ‘ä»¬ä»…ç ”ç©¶å‰ç¼€ç å°±å¤Ÿäº†.
å¯¹ä»»æ„å¯è§£ç çš„ç¼–ç æ–¹å¼ï¼Œ Kraftéƒ½æˆç«‹ã€‚æ¨è®ºï¼Œå¯¹ä»»æ„æ»¡è¶³çš„$l_i$ï¼Œå¦‚æœæ²¡æœ‰å…¶ä»–ç‰¹æ®Šè¦æ±‚ï¼Œå‰ç¼€ç å°±å¤Ÿäº†ã€‚
(McMillan) The codeword lengths of **any uniquely decodable** D-ary code must satisfy the Kraft inequality $\sum D^{-l_{i}} \leq 1$
Conversely, given a set of codeword lengths that satisfy this inequality, it is possible to construct a uniquely decodable code with these codeword lengths.

- Consider $C^{k} = C(x_1,\ldots,x_k)$, the $k$ th extension of the code (i.e., the code formed by the concatenation of $k$ repetitions of the given uniquely decodable code $C$ ). è€ƒè™‘æ¯ä¸€ç§ç¼–ç æ–¹å¼çš„kæ¬¡æ‰©å±•ï¼Œè¿åœ¨ä¸€èµ·ä½œä¸ºæ–°çš„ç åˆ¶ã€‚
- By the definition of unique decodability, the $k$ th extension of the code is nonsingular. ç”±äºå¯ä»¥è§£ç ï¼Œé‚£ä¹ˆkæ¬¡æ‰©å±•ä¹Ÿæ˜¯å¯è§£ç çš„ã€‚
- since there are only $D^{n}$ different D-ary strings of length $n,$ unique decodability implies that the number of code sequences of length $n$ in the $k$ th extension of the code must be no greater than $D^{n}$.è€ƒè™‘Då…ƒå­—ç¬¦ä¸²ä¸­ï¼Œæœ€å¤šæœ‰$D^{n}$ä¸åŒçš„å­—ç¬¦ä¸²ï¼Œå”¯ä¸€å¯è§£ç æ€§æ„å‘³ç€kæ¬¡æ‰©å±•ç åˆ¶çš„é•¿åº¦ä¸èƒ½è¶…è¿‡$D^{n}$

ä»¥ä¸Šä¸‰ä¸ªç»“è®ºæœ‰åŠ©äºæˆ‘ä»¬è¿›ä¸€æ­¥çš„è¯æ˜ã€‚

- Let the codeword lengths of the symbols $x \in X$ be denoted by $l(x) .$ For the extension code, the length of the code sequence is $l\left(x_{1}, x_{2}, \ldots, x_{k}\right)=\sum_{i=1}^{k} l\left(x_{i}\right)$ ç”±äºæ‰©å±•æ˜¯ç›´æ¥ç›¸è¿
- The inequality we wish to prove is $\sum_{x \in X} D^{-l(x)} \leq 1$ 
- Consider the $k$ th power of this quantity
  $$
  \begin{aligned}
  \left(\sum_{x \in X} D^{-l(x)}\right)^{k} &=\sum_{x_{1} \in X} \sum_{x_{2} \in X} \ldots \sum_{x_{k} \in X} D^{-l\left(x_{1}\right)} D^{-l\left(x_{2}\right)} \ldots D^{-l\left(x_{k}\right)} \\
  &=\sum_{x_{1}, x_{2}, \ldots, x_{k} \in X^{k}} D^{-l\left(x_{1}\right)} D^{-l\left(x_{2}\right)} \ldots D^{-l\left(x_{k}\right)} \\
  &=\sum_{x^{k} \in X^{k}} D^{-l\left(x^{k}\right)}=\sum_{m=1}^{k l_{\max }} a(m) D^{-m} \\
  & \leq \sum_{m=1}^{k l_{\max }} D^{m} D^{-m}=k l_{\max }
  \end{aligned}
  $$
  ç¬¬ä¸€è¡Œï¼Œå±•å¼€ï¼Œç¬¬äºŒè¡Œï¼Œåˆå¹¶ï¼Œç¬¬ä¸‰è¡Œï¼Œå¯¹ä¸åŒçš„kï¼Œ$l(x_k)$å¯èƒ½æ˜¯ç›¸åŒçš„ï¼Œæˆ‘ä»¬åˆå¹¶åŒç±»é¡¹ã€‚
  - $a(m)$ç”¨åˆ°äº†ç”Ÿæˆå‡½æ•°çš„ä¸€äº›æ€§è´¨ï¼Œå³é•¿åº¦ä¸ºmçš„ç¼–ç æ–¹å¼æ‰€å¯¹åº”çš„æ•°ç›®ã€‚æ¦‚å¿µä¸Šç±»ä¼¼äºŒé¡¹å¼å®šç†ä¸­çš„ç»„åˆæ•°ã€‚
  - $l \max$ is the maximum codeword length and $a(m)$ is the number of source sequences $x^{k}$ mapping into codewords of length $m$
  - ç”±äºæˆ‘ä»¬è€ƒè™‘çš„æ˜¯kæ¬¡æ‰©å±•ï¼Œæœ€é•¿é•¿åº¦æ˜¯$kl_{max}$ï¼Œ
  - $a(m)\le D^m$æ˜¯æˆ‘ä»¬ä¹‹å‰è¯æ˜äº†çš„ç»“è®º
  - ç”±äºkä»»å–ï¼Œæé™å¯ä»¥è¶‹å‘äº1ã€‚
  $$\sum_{x \in X} D^{-l(x)} \leq\left(k l_{\max }\right)^{\frac{1}{k}} \rightarrow 1, \text { as } k \rightarrow \infty$$
ä»¥ä¸Šæ˜¯å¿…è¦æ€§çš„è¯æ˜ï¼Œå……åˆ†æ€§çš„è¯æ˜ä¸å”¯ä¸€å¯åˆ†è§£ç ç±»ä¼¼ã€‚
ideaï¼škæ¬¡æ‰©å±•åˆšå¥½å¯¹åº”äº$\left(\sum_{x \in X} D^{-l(x)}\right)^{k}$

å› æ­¤ï¼ŒKraftå¯ä»¥çº¦æŸæ‰€æœ‰å¯è§£ç çš„æ–¹å¼
## Summary

![](img/0322-3.png)

æœ¬èŠ‚æˆ‘ä»¬ä»‹ç»äº†æœ€ä¼˜ç¼–ç çš„å­˜åœ¨æ€§, ä¸‹èŠ‚è¯¾æˆ‘ä»¬ä»‹ç»æœ€ä¼˜ç¼–ç çš„ç®—æ³•.

### Rate and Capacity

- The **rate** $R$ of $(M, n)$ code is
  $$
  R=\frac{\log M}{n} \text { bits per transmission. }
  $$
  å› ä¸ºä¿¡æ¯Mä¸€å®šèƒ½ç”¨é•¿åº¦ä¸º$\log M$çš„ç è¿›è¡Œç¼–ç , å¯ä»¥é€šè¿‡næ¬¡ä¼ è¾“.
- A rate $R$ is said to be **achievable** if there exists a sequence of $\left(2^{n R}, n\right)$ codes such that **the maximal probability of error $\lambda(n)$ tends to 0** as $n \rightarrow \infty$
  ç ç‡å¯å–, if æˆ‘ä»¬èƒ½æ‰¾åˆ°ä¸€ä¸ªç¼–ç , ä½¿**æœ€å¤§**é”™è¯¯æ¦‚ç‡,åœ¨ç é•¿å¾ˆé•¿æ—¶å¯ä»¥è¶‹å‘äº0.
- The capacity of a channel is the supremum of all achievable rates.
  ä¿¡é“å®¹é‡: æ‰€æœ‰å¯å–ç ç‡çš„ä¸Šç¡®ç•Œ.

> (Channel coding theorem) For a **discrete memoryless channel**, all rates below capacity $C$ are achievable. Specifically, for every rate $R<C,$ there exists a sequence of $\left(2^{n R}, n\right)$ codes with maximum probability of error $\lambda^{(n)} \rightarrow 0 .$ Conversely, any sequence of $\left(2^{n R}, n\right)$ codes with $\lambda^{(n)} \rightarrow 0$ must have $R \leq C$

å³,æˆ‘ä»¬è¦è¯æ˜$C=\max _{p(x)} I(X ; Y)$
å°±è¦è¯æ˜ä¸¤ç‚¹
1. Achievability
   For any $r<C,$ there exists an $\left(2^{n r}, n\right)$ code
2. Converse
   For any $r>c, \lambda_{e}>0$

## Joint Typicality
å®šä¹‰è”åˆå…¸å‹é›†.

Roughly speaking, we decode a channel output $Y^{n}$ as the $i$ th index if the codeword $X^{n}(i)$ is "jointly typical" with the received signal $Y^{n}$
ç°åœ¨æˆ‘ä»¬æœ‰ä¸¤ç»„éšæœºå˜é‡, The set $A_{\epsilon}^{(n)}$ of jointly typical sequences è”åˆå…¸å‹é›†åºåˆ— $\left\{\left(x^{n}, y^{n}\right)\right\}$ with respect to the distribution $p(x, y)$ is the set of n-sequences with empirical entropies $\epsilon$ -close to the true entropies:
$$
\begin{aligned}
A_{\epsilon}^{(n)}=\left\{\left(x^{n}, y^{n}\right) \in X^{n} \times Y^{n}:\right.&-\frac{1}{n} \log p\left(x^{n}\right)-H(X) |<\epsilon \\
&\left|-\frac{1}{n} \log p\left(y^{n}\right)-H(Y)\right|<\epsilon \\
&\left.\left|-\frac{1}{n} \log p\left(x^{n}, y^{n}\right)-H(X, Y)\right|<\epsilon\right\}
\end{aligned}
$$
æˆ‘ä»¬è¦æ±‚, å…¸å‹é›†å…ƒç´ è¦æ±‚æ»¡è¶³å•ä¸ªå…ƒç´ åœ¨å…¸å‹é›†ä¸­, åˆèµ·æ¥çœ‹ä¹Ÿè¦åœ¨å…¸å‹é›†ä¸ç­‰å¼ä¸­.

æ³¨æ„, ä¸€èˆ¬æƒ…å†µä¸‹,$X^{n} \in A_{\epsilon}^{(n)}, Y^{n} \in A_{\epsilon}^{(n)}$ cannot imply $\left(X^{n}, Y^{n}\right) \in A_{\epsilon}^{(n)}$

æœ‰äº†å…¸å‹é›†, é¡ºç†æˆç« æˆ‘ä»¬è¦è¯æ˜è”åˆAEP.

- $\operatorname{Pr}\left(\left(X^{n}, Y^{n}\right) \in A_{\epsilon}^{(n)}\right) \rightarrow 1 \text { as } n \rightarrow \infty$
- $\left|A_{\epsilon}^{(n)}\right| \leq 2^{n(H(X, Y)+\epsilon)}$ è¯æ˜ç±»ä¼¼å•ä¸ªéšæœºå˜é‡çš„AEP
- $\left|A_{\epsilon}^{(n)}\right| \geq (1-\epsilon) 2^{n(H(X, Y)-\epsilon)}$ è¯æ˜ç±»ä¼¼å•ä¸ªéšæœºå˜é‡çš„AEP
- $\text { If }\left(\tilde{X}^{n}, \tilde{Y}^{n}\right) \sim p\left(x^{n}\right) p\left(y^{n}\right), \text { then }$ 
  $$(1-\epsilon) 2^{-n(I(X, Y)+3 \epsilon)} \leq \operatorname{Pr}\left(\left(\tilde{X}^{n}, \tilde{Y}^{n}\right) \in A_{\epsilon}^{(n)}\right) \leq 2^{-n(I(X, Y)-3 \epsilon)}$$
  (å³å¯¹å…¸å‹é›†ä¸­ç‰¹æ®Šçš„$X_n,Y_n$, ä¸Šä¸‹ç•Œç”¨äº’ä¿¡æ¯è¡¡é‡, è€Œä¸æ˜¯ç†µ)

æˆ‘ä»¬ä¸»è¦è¯æ˜ç¬¬ä¸‰ä¸ªæ€§è´¨.
å®šä¹‰å±•å¼€, æ ¹æ®å…¸å‹é›†å¤§å°æ”¾ç¼©. å³ä¾§å¾—è¯
$$\begin{aligned}
\operatorname{Pr}\left(\left(\tilde{X}^{n}, \tilde{Y}^{n}\right) \in A_{\epsilon}^{(n)}\right)=& \sum_{\left(x^{n}, y^{n}\right) \in A_{\varepsilon}^{(n)}} p\left(x^{n}\right) p\left(y^{n}\right) \\
\leq & 2^{n(H(X, Y)+\epsilon)} 2^{-n(H(X)-\epsilon)} 2^{-n(H(Y)-\epsilon)}=2^{-n(I(X, Y)+3 \epsilon)}
\end{aligned}$$

å·¦ä¾§, å…ˆè¯æ€§è´¨3
$$1-\epsilon \leq \operatorname{Pr}\left(A_{\epsilon}^{(n)}\right)=\sum_{\left(x^{n}, y^{n}\right) \in A_{\epsilon}^{(n)}} p\left(x^{n}, y^{n}\right) \leq\left|A_{\epsilon}^{(n)}\right| 2^{-n(H(X, Y)-\epsilon)}$$

ç”¨ç±»ä¼¼çš„æ–¹æ³•, 
$$\begin{array}{c}
\operatorname{Pr}\left(\left(\tilde{X}^{n}, \tilde{Y}^{n}\right) \in A_{\epsilon}^{(n)}\right)=\sum_{\left(x^{n}, y^{n}\right) \in A_{\varepsilon}^{(n)}} p\left(x^{n}\right) p\left(y^{n}\right) \\
\geq(1-\epsilon) 2^{n(H(X, Y)-\epsilon)} 2^{-n(H(X)+\epsilon)} 2^{-n(H(Y)+\epsilon)}=(1-\epsilon) 2^{-n(I(X ; Y)+3 \epsilon)}
\end{array}$$

ä¸ºä»€ä¹ˆä¿¡é“ç¼–ç ä¸­ä¼šå‡ºç°äº’ä¿¡æ¯? 
![](img/04-08-11-31-40.png)
å› ä¸ºåœ¨æ‰€æœ‰$X^n$,$Y^n$ä¸­,èƒ½å¤Ÿç¬¦åˆå…¸å‹é›†æ€§è´¨3çš„å¿…é¡»ç”¨äº’ä¿¡æ¯è¡¡é‡.
$$\frac{2^{n H(X . Y)}}{2^{n H(X)} 2^{n H(Y)}}=2^{-n I(X: Y)}$$

## Intuition for Channel Capacity

![](img/04-08-11-33-15.png)
- Xä¸Yçš„å…³ç³»ç±»ä¼¼$X^n$é©±åŠ¨äº†$Y^n$, ç”±äºå™ªå£°çš„ä½œç”¨, $X^n$çš„ç‚¹å˜æˆäº†$Y^n$çš„åŒºåŸŸ.
- é‚£æˆ‘ä»¬æœ€å¥½å¸Œæœ›, no two $X$ sequences produce the same $Y$ output sequence.
  - é‚£æ¯”è¾ƒå¥½çš„åšæ³•å°±æ˜¯æŠŠY^nçš„æŠ•å½±åˆ†æˆè¶Šæ¥è¶Šå¤šçš„ä¸ç›¸äº¤çš„å°é›†åˆ.
- ä»å…¸å‹é›†çš„è§’åº¦, å½“$X^n$ç»™å®šçš„æƒ…å†µä¸‹, ä¹Ÿå°±å¯¹åº”çš„ä¼šæœ‰ $2^{nH(Y|X)}$çš„$Y^n$ä¸ªåºåˆ—å¯èƒ½ä¼šæ„æˆå…¸å‹æ€§.
- ç”±äº$Y^n$çš„å…¸å‹æ€§, æˆ‘ä»¬æœ€å¤šèƒ½æ‰¾åˆ°äº’ä¸ç›¸äº¤çš„é›†åˆæ•°ç›®å°±æ˜¯, $2^{n(H(Y)-H(Y | X))}=2^{n I(X ; Y)}$
- å› æ­¤, ä¼˜åŒ–ç›®æ ‡æ˜¯æœ€å¤§åŒ– $I(X ; Y)$



# 0413 Channel Capacity (3)


## Coverse Proof Special Case: Zero-Error Codes

![](img/04-13-23-05-10.png)

æˆ‘ä»¬å¸Œæœ›è¯æ˜ï¼Œå¯¹äºä»»ä½•åˆæ ¼çš„ç¼–ç æ–¹æ¡ˆï¼Œç ç‡è¦å°äºç­‰äºä¿¡é“å®¹é‡ã€‚

å…ˆè€ƒè™‘ä¸€ä¸ªç‰¹æ®Šæƒ…å†µï¼š$Y^{n}$å¯ä»¥å®Œç¾åœ°æ¢å¤å‡º$W$ã€‚å³$H(W|Y^{n})=0$.

The outline of the proof of the converse is most clearly motivated by going through the argument when absolutely no errors are allowed.
$$
\begin{aligned}
n R=H(W) &=H\left(W | Y^{n}\right)_{=0}+I\left(W ; Y^{n}\right) \\
&=I\left(W ; Y^{n}\right) \\
& \leq I\left(X^{n} ; Y^{n}\right) \quad\left(W \rightarrow X^{n} \rightarrow Y^{n}\right) \\
& \leq \sum_{i} I\left(X_{i} ; Y_{i}\right) \\
& \leq n C \\
& R \leq C
\end{aligned}
$$
In general, $H\left(W | Y^{n}\right)>0:$ Fano's inequality

$$\begin{aligned}
I\left(X^{n} ; Y^{n}\right) &=H\left(Y^{n}\right)-H\left(Y^{n} | X^{n}\right)=H\left(Y^{n}\right)-\sum H\left(Y_{i} | X_{i}\right) \\
& \leq \Sigma H\left(Y_{i}\right)-\Sigma H\left(Y_{i} | X_{i}\right)=\sum I\left(X_{i} ; Y_{i}\right)
\end{aligned}$$

æ³¨æ„ï¼Œåœ¨è¿™ä¸ªé—®é¢˜ï¼ˆDMCï¼‰ä¸­ï¼Œ$Y_i$ä¸$X_i$ç›¸äº’ç‹¬ç«‹ï¼Œä½†ä¸ä»£è¡¨$Y_i$ä¹‹é—´ç›¸äº’ç‹¬ç«‹ã€‚

ä¸Šé¢çš„è¯æ˜ä¸­ï¼Œæˆ‘ä»¬åŠ å¼ºçš„æ¡ä»¶ç®€åŒ–äº†ä¸ç­‰å¼çš„è¯æ˜ï¼Œin general$H(W|Y^{n})>0$ï¼Œæˆ‘ä»¬å°±è¦ç”¨åˆ°Fano's Inequalityã€‚

## Coverse Proof: Channel Coding Theorem


![](img/04-13-08-13-31.png)

æˆ‘ä»¬å‡è®¾é”™è¯¯æ¦‚ç‡ç‡è¶‹å°ï¼Œå³æ ¹æ®fanoä¸ç­‰å¼ï¼Œ$H(W | W) \leq 1+P_{e}^{(n)} n R$

$$\begin{aligned}
n R &=H(W) \\
&=H(W | \widehat{W})+I(W ; \widehat{W}) \\
& \leq 1+P_{\epsilon}^{(n)} n R+I(W ; \widehat{W}) \\
& \leq 1+P_{\epsilon}^{(n)} n R+I\left(X^{n} ; Y^{n}\right) \quad \text{é©¬å°”å¯å¤«é“¾æ€§è´¨ï¼Œæ•°æ®å¤„ç†ä¸ç­‰å¼}\\
& \leq 1+P_{\epsilon}^{(n)} n R+n C
\end{aligned}$$

å› æ­¤æˆ‘ä»¬æœ‰

$$R \leq P_{\epsilon}^{(n)} R+\frac{1}{n}+c \rightarrow C$$

åè¯æ³•ï¼Œå¦‚æœ$R>C$ï¼Œåˆ™$P_{\epsilon}^{(n)}$ä¸ä¼šè¶‹å‘äº0.
$$P_{\epsilon}^{(n)} \geq 1-\frac{c}{R}-\frac{1}{n R}>0 \text { as } R>C$$

æ¥ä¸‹æ¥æˆ‘ä»¬è¯æ˜ï¼Œå¯¹ä»»æ„å°äº$C$çš„ç¼–ç ï¼Œæˆ‘ä»¬éƒ½èƒ½æ‰¾åˆ°ä¸€ç§ç¼–ç ï¼ˆå¯è¾¾æ€§ï¼‰


## Achievability

### Code Construction
![](img/04-13-08-23-27.png)
ç¼–ç çš„æ„é€ ï¼šå®é™…ä¸Šå°±æ˜¯éšæœºç”Ÿæˆç æœ¬çŸ©é˜µ$\mathcal{C}$ã€‚
è¡Œï¼šæ¶ˆæ¯ï¼Œåˆ—ï¼šç¼–ç ã€‚

Fix $p(x) .$ Generate $a\left(2^{n R}, n\right)$ code at random according to $p(x)$
$$
p\left(x^{n}\right)=\prod_{i=1}^{n} p\left(x_{i}\right)
$$

The probability the we generate a particular code $C$ is å¯¹ä»»æ„ç¼–ç ï¼Œç”Ÿæˆè¯¥ç¼–ç ï¼ˆç æœ¬ï¼‰çš„æ¦‚ç‡æ˜¯ $\operatorname{Pr}(C)=\prod_{w=1}^{2^{n R}} \prod_{i=1}^{n} p\left(x_{i}(w)\right)$
The code $C$ will be shared both by the sender and receiver, both know $p(y|x)$.

A message $W$ is chosen according to a uniform distributionæš‚æ—¶å¿½ç•¥$nR$ä¸æ˜¯æ•´æ•°çš„é—®é¢˜ã€‚ $\operatorname{Pr}(W=w)=2^{-n R}, w=1,2, \ldots, 2^{n R}$
The $w$ th codeword $X^{n}(w)$ is sent over the channel The receiver receives a sequence $Y^{n}$ according to the distribution
$$
P\left(y^{n} | x^{n}(w)\right)=\prod_{i=1}^{N} p\left(y_{i} | x_{i}(w)\right)
$$

### Joint Decoding
å¦‚ä½•è§£ç ï¼Œæˆ‘ä»¬è¦ç”¨è”åˆAEPè§£ç ã€‚

The receiver guess which message was sent. In jointly typical decoding, the receiver declares that the index $\widehat{W}$ was sent if the following conditions are satisfied:
- $\left(X^{n}(\widehat{W}), Y^{n}\right)$ is jointly typical å­˜åœ¨
- There is no other index $W^{\prime} \neq W,$ such that $\left(X^{n}\left(W^{\prime}\right), Y^{n}\right) \in A_{\epsilon}^{(n)}$ å”¯ä¸€

If no such $\hat{W}$ exists or if there is more than one such, an error is declared. (We may assume that the receiver **outputs a dummy index such as 0** in this case.)
Let $\mathcal{E}$ be the event $\{\widehat{W} \neq W\}$

**We need to show that**
$$
\operatorname{Pr}(\mathcal{E}) \rightarrow \mathbf{0}
$$

## $\operatorname{Pr}(\mathcal{E}) \rightarrow 0$

> Main idea: If we could prove that for all the codebook (all the possible $C$ ), the average $\operatorname{Pr}(\varepsilon) \leq \epsilon_{i}$  **then the error probability of the best code** (one of $C^{\prime}$s $\leq \epsilon$ï¼‰

We let $W$ be drawn according to a uniform distribution over $\left\{1,2, \ldots, 2^{n R}\right\}$ and use jointly typical decoding $\hat{W}\left(y^{n}\right)$

Let $\mathcal{E}=\left\{\hat{W}\left(y^{n}\right) \neq W\right\}$ denote the error event

We will calculate the average probability of error, averaged over all codewords in the codebook, and averaged over all codebooks æ¯ä¸€ä¸ªç æœ¬ã€æ¯ä¸€ä¸ªç åˆ¶ä¸Šçš„å¹³å‡é”™è¯¯ç‡ã€‚
$$
\begin{aligned}
&\operatorname{Pr}(\varepsilon)=\sum_{c} \operatorname{Pr}(C) P_{e}^{(n)}(C) \text{\quad å®šä¹‰å±•å¼€} \\
&=\sum_{c} \operatorname{Pr}(C) \frac{1}{2^{n R}} \sum_{w=1}^{2^{n R}} \lambda_{w}(C) \text{\quad ç åˆ¶å‡åŒ€åˆ†å¸ƒ}\\
&=\frac{1}{2^{n R}} \sum_{w=1}^{2^{n R}} \sum_{c} \operatorname{Pr}(C) \lambda_{w}(C) \text{\quad æ±‚å’Œäº¤æ¢ä½ç½®}
\end{aligned}
$$

æˆ‘ä»¬åˆ†æç¬¬äºŒä¸ªæ±‚å’Œè¡¨è¾¾å¼ã€‚
$$\sum_{C} \operatorname{Pr}(C) \lambda_{1}(C)=\operatorname{Pr}(\mathcal{E} | W=1)$$
æ˜¯åœ¨ä¼ é€’ä¿¡æ¯1çš„æƒ…å†µä¸‹çš„å¹³å‡é”™è¯¯æ¦‚ç‡ï¼Œä¹Ÿå³
$$\operatorname{Pr}(\mathcal{E})=\frac{1}{2^{n R}} \sum_{w=1}^{2^{n R}} \operatorname{Pr}(\mathcal{E} | W=w)$$

ç”±å¯¹ç§°æ€§ï¼Œæˆ‘ä»¬ä»¥è¯æ˜ä¿¡æ¯ä¸º1æ—¶çš„ç»“è®ºä¸ºä¾‹ã€‚

$$E_{i}=\left\{\left(\left(X^{n}(i), Y^{n}\right) \text { is in } A_{\epsilon}^{(n)}\right), i \in\left\{1,2, \ldots, 2^{n R}\right\}\right\}$$

å®šä¹‰é›†åˆ$E_{i}$ï¼Œé‚£ä¹ˆè§£ç é”™è¯¯çš„æ¦‚ç‡å¯ä»¥å½¢å¼åŒ–åœ°è¡¨è¾¾ä¸º

$$\begin{aligned}
\operatorname{Pr}(\mathcal{E} | W=1) &=P\left(E_{1}^{c} \cup E_{2} \cup E_{3} \cup \ldots \cup E_{2^{n} R} | W=1\right) \\
& \leq P\left(E_{1}^{c} | W=1\right)+\sum_{i=2}^{2^{n R}} P\left(E_{i} | W=1\right)
\end{aligned}$$

è¦ä¹ˆä¸åœ¨å…¸å‹é›†ä¸­ï¼Œè¦ä¹ˆåœ¨å…¶ä»–å…¸å‹é›†ä¸­ã€‚æˆ‘ä»¬ç›´æ¥å¯¹é›†åˆè¿›è¡Œæ”¾ç¼©ã€‚å¹¶$\rightarrow$ä¸ç›¸äº¤å¹¶ã€‚

æˆ‘ä»¬å…·ä½“åˆ†æä¸¤ä¸ªéƒ¨åˆ†æœ‰å¤šå¤§ã€‚
- ç”±è”åˆAEPï¼ŒBy Joint $\mathrm{AEP}, P\left(E_{1}^{c} | W=1\right) \rightarrow 0,$ and hence $\mathrm{P}\left(\mathrm{E}_{1}^{\mathrm{c}} | \mathrm{W}=1\right) \leq \epsilon,$ for $n$ sufficiently large
- For $i \geq 2,\left(E_{i} | W=1\right):$
  - since by the code generation process, $X^{n}(1)$ and $X^{n}(i)$ are independent for $i \neq 1,$ï¼Œæºç çš„ç”Ÿæˆç›¸äº’ç‹¬ç«‹
  - so are $Y^{n}$ and $X^{n}(i) .$
  - Hence, the probability that $X^{n}(i)$ and $Y^{n}$ are jointly typical is $\leq 2^{-n(I(X ; Y)-3 \epsilon)}$ by the joint AEP

$$\begin{aligned}
\operatorname{Pr}(\mathcal{E} | W=1) & \leq \epsilon+\sum_{i=2}^{2^{n R}} 2^{-n(I(X ; Y)-3 \epsilon)} \\
&=\epsilon+\left(2^{n R}-1\right) 2^{-n(I(X ; Y)-3 \epsilon)} \\
& \leq \epsilon+2^{n R} 2^{-n(I(X ; Y)-3 \epsilon)} \\
&=\epsilon+2^{-n(I(X ; Y)-R-3 \epsilon)}
\end{aligned}$$

If $n$ is sufficiently large and $R<I(X ; Y)-3 \epsilon$
$$
\begin{aligned}
\operatorname{Pr}(&\varepsilon | W=1) \leq 2 \epsilon \\
& \operatorname{Pr}(\mathcal{E}) \leq 2 \epsilon
\end{aligned}
$$

è¿™æ ·ï¼Œæˆ‘ä»¬å°±è¯æ˜äº†ï¼Œç»™å®š$p(x)$æƒ…å†µä¸‹ï¼Œéšæœºç¼–ç çš„é”™è¯¯æ¦‚ç‡$\le 2\epsilon$ã€‚è¿™æ ·ä¹Ÿå°±è¯æ˜äº†ä¸€å®šå­˜åœ¨è¿™æ ·çš„ç¼–ç ã€‚

æ³¨æ„åˆ°$p(x)$çš„é€‰æ‹©æ˜¯ä¸å—é™çš„ï¼Œå› æ­¤Choose $p(x)$ in the proof to be $p^{*}(x),$ the distribution on $X$ that achieving capacity. Then
$$
\begin{aligned}
R \leq I\left(X^{*} ; Y\right) &=C \\
\lambda^{(n)} \leq 4 \epsilon
\end{aligned}
$$

### $\operatorname{Pr}(\mathcal{E}) \rightarrow 0 \Rightarrow \lambda^{(n)} \rightarrow 0$
ä»å¹³å‡æ¦‚ç‡ä¸º0åˆ°æœ€å¤§é”™è¯¯æ¦‚ç‡ä¸º0.

åœ¨å‰é¢çš„è¯æ˜ä¸­ï¼Œæˆ‘ä»¬è¯´æ˜äº†There exists a best codebook $C^{*}$ such that
$$
\operatorname{Pr}\left(\varepsilon | C^{*}\right)=\frac{1}{2^{n R}} \sum_{i=1}^{2^{n R}} \lambda_{i}\left(C^{*}\right) \leq 2 \epsilon
$$

æ ¹æ®ä¸Šä¸€èŠ‚çš„å®šä¹‰ï¼ŒBy the definition of $\left(n, 2^{n R}\right)$ code, we need to further show that
$$
\lambda^{(n)} \rightarrow 0
$$

Without loss of generality, assume $\lambda_{1} \leq \lambda_{2} \leq \cdots \leq \lambda_{2} n_{R}$
By $\operatorname{Pr}\left(\varepsilon | C^{*}\right) \leq 2 \epsilon,$ we have æˆ‘ä»¬çŸ¥é“å‰ä¸€åŠä¸€å®šè¶³å¤Ÿå°
$$\lambda_{1} \leq \lambda_{2} \leq \cdots \leq \lambda_{2} n_{R-1} \leq 4 \epsilon$$
åè¯æ³•ï¼š $\left(\text { Or } \lambda_{2^{n R-1}}>4 \epsilon, \frac{1}{2^{n R}} \sum_{i=1+2^{n R-1}}^{2^{n R}} \lambda_{i}\left(C^{*}\right)>\frac{1}{2} 4 \epsilon=2 \epsilon, \text { contradiction! }\right)$

åªéœ€å¯¹ç æœ¬è¿›è¡Œå¾®è°ƒï¼Œå°±å¯ä»¥è¾¾åˆ°æœ€å¤§é”™è¯¯æ¦‚ç‡çš„é™ä½ã€‚æŠŠæœ€åçš„ä¸€åŠç åˆ¶ç›´æ¥æ‰”æ‰ã€‚ï¼ˆè€ƒè™‘å¹³å‡å†é€šè¿‡æ‰”æ‰ä¸€åŠè¯æ˜æœ€åï¼Œè¿™æ˜¯ä¸€ä¸ªå¾ˆgeneralçš„è¯æ˜åšæ³•ï¼Œåœ¨å¾ˆå¤šåœ°æ–¹éƒ½å¾ˆå¸¸è§ï¼‰Further refine the codebook $C^{*}$
- Throw away the worst half of the codewords in the best codebook $C^{*}$
- The best half of the codewords have a maximal probability of error less than $4 \epsilon$
- If we reindex these codewords, we **have $2^{n R-1}$ codewords**. Throwing out half the codewords has changed the rate from $R$ to $R-\frac{1}{n},$ which is negligible for large $n$ã€‚åœ¨æé™æƒ…å†µä¸‹ä¸ä¼šå¯¹ç ç‡äº§ç”Ÿå½±å“ã€‚

## (Introduction) Feedback Capacity

Recallï¼ŒDMCä¸­ï¼Œæ”¶åˆ°çš„ä¿¡æ¯æ˜¯ä»€ä¹ˆæƒ…å†µæˆ‘ä»¬æ˜¯ä¸çŸ¥æƒ…çš„ã€‚ä¿¡å·çš„å‘é€ç«¯ä¸æ¥æ”¶ç«¯åœ¨ç‰©ç†ä¸Šæ˜¯éš”ç¦»å¼€çš„ã€‚

![](img/04-13-09-17-27.png)

- We assume that **all the received symbols are sent back immediately and noiselessly** to the transmitter, which can then use them to decide **which symbol to send next** å³ï¼Œåœ¨æœ‰åé¦ˆçš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æœ‰æ›´é«˜çš„çµæ´»æ€§ç”Ÿæˆç åˆ¶ã€‚è¿™é‡Œæˆ‘ä»¬å‡è®¾è¾“å‡ºçš„ä¿¡å·é©¬ä¸Šä¼šå‘åˆ°è¾“å…¥ç«¯ï¼Œè¾“å…¥ç«¯å¯ä»¥æ ¹æ®è¯¥åé¦ˆç”Ÿæˆä¸‹ä¸€ä¸ªä¿¡å·ã€‚
- We define $a\left(2^{n R}, n\right)$ *feedback code* as a sequence of mappings $x_{i}\left(W, Y^{i-1}\right),$ where each $x_{i}$ is **a function only of** the message $W \in 2^{n R}$ and the previous received values, $Y_{1}, Y_{2}, \ldots, Y_{i-1},$ and a sequence of decoding functions $g: \mathcal{Y}^{n} \rightarrow\left\{1,2, \ldots, 2^{n R}\right\} .$ Thus, å®šä¹‰è§£ç å™¨æ²¡æœ‰å¾—åˆ°æ­£ç¡®ç»“æœçš„æ¦‚ç‡
$$
P_{e}^{(n)}=\operatorname{Pr}\left(g\left(Y^{n}\right) \neq W\right)
$$
when $W$ is uniformly distributed over $\left\{1,2, \ldots, 2^{n R}\right\}$

æˆ‘ä»¬å‘ç°ï¼šFeedback capacity
$$
C_{F B}=C=\max _{p(x)} I(X ; Y)
$$

> Feedback cannot increase capacity. ï¼ˆå› ä¸ºè¯¥é—®é¢˜ä¸­ï¼Œæ— **è®°å¿†æ€§ä»ç„¶æ˜¯ä¿ç•™çš„**ï¼‰

### TODO:çœ‹ä¸€ä¸‹è¯æ˜

## (Introduction) Source-Channel Separation
ä¿¡æºä¿¡é“åˆ†ç¦»å®šç†


Recallï¼Œæˆ‘ä»¬å­¦ä¹ äº†æ•°æ®çš„å‹ç¼©ä¸æ•°æ®çš„ä¼ è¾“

- In data compression: $R>H$
- In data transmission: $R<C$

> Is the condition $H<C$ sufficient and necessary?
> æ˜¯ä¸æ˜¯$H<C$å°±ä¿è¯äº†æˆ‘ä»¬å¯ä»¥ä»¥å¾ˆä½çš„é”™è¯¯æ¦‚ç‡ï¼Ÿ
> **YES**

![](img/04-13-09-23-37.png)

### Formal Problem

- We want to send the sequence of symbols $V^{n}=V_{1}, V_{2}, \ldots, V_{n}$ over the channel so that the receiver can reconstruct the sequence
- To do this, we map the sequence onto a codeword $X^{n}\left(V^{n}\right)$ and send the codeword over the channel
- The receiver looks at his received sequence $Y^{n}$ and makes an estimate $\hat{V}^{n}$ of the sequence $V^{n}$ that was sent. The receiver makes an error if $V^{n} \neq \hat{V}^{n} .$ We define the probability of error as
  $$
  \operatorname{Pr}\left(V^{n} \neq \hat{V}^{n}\right)=\sum_{y^{n}} \sum_{v^{n}} p\left(v^{n}\right) p\left(y^{n} | x^{n}\left(v^{n}\right)\right) I\left(g\left(y^{n}\right) \neq v^{n}\right)
  $$
  Where $I$ is the indicator function and $g\left(y^{n}\right)$ is the decoding function

### TODO: Theorem

**Theorem (Source-channel coding theorem).** If $V_{1}, V_{2}, \ldots, V_{n}$ is a finite alphabet stochastic process that satisfies the AEP and $H(\mathcal{V})<C,$ there exists a source-channel code with probability of error $\operatorname{Pr}\left(\hat{V}^{n} \neq V^{n}\right) \rightarrow 0 .$ Conversely, for any stationary stochastic process, if $H(v)>C,$ the probability of error is bounded away from zero, and it is not possible to send the process over the channel with arbitrarily low probability of error.

## Error Correction Code

The object of coding is to introduce redundancy so that even if some of the information is lost or corrupted, it will still be possible to recover the message at the receiver. åœ¨å‰é¢çš„å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬çŸ¥é“ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨å†—ä½™æŠµæŠ—å™ªå£°çš„å¹²æ‰°ï¼Œä¸‹é¢åˆ—å‡ºä¸€äº›å†—ä½™æ‰‹æ®µã€‚

- Repetition code: For example, to send a $1,$ we send 11111 , and to send a $0,$ we send 00000. The decoding scheme is to take the majority vote. æ¥æ”¶ç«¯æ•°0å¤šè¿˜æ˜¯1å¤šï¼Œå°‘æ•°æœä»å¤šæ•°ã€‚
- Parity check code: Starting with a block of $n-1$ information bits, we choose the $n$ th bit so that the parity of the entire block is 0. å¥‡å¶æ ¡éªŒä½ã€‚
- The code does not detect an even number of errors and does not give any information about how to correct the errors that occur.

## Hamming Code

RECALL: BSCä¿¡é“
ç”±å¤§æ•°å®šå¾‹ï¼Œå› ä¸ºæˆ‘ä»¬ä¼šä»¥$p$çš„æ¦‚ç‡ç¿»è½¬ï¼Œå¦‚æœä¿¡æ¯é‡å¤Ÿå¤§ï¼Œé‚£ä¹ˆä¼šæœ‰å¤§çº¦npä¸ªbitsè¢«ä¿®æ”¹ï¼Œæˆ‘ä»¬å¯ä»¥è¯æ˜ï¼Œ$d(x,y)\le np$ï¼ˆæ›¼å“ˆé¡¿è·ç¦»ï¼‰ã€‚å¦ä¸€ç§æ„ä¹‰ä¸Šï¼Œä¹Ÿå°±æ˜¯è¯´ï¼ŒAll the points $y$ are within the sphere with center $x$ and radius $n p$


- Deonte the codeword by $x,$ then the noisy version $y$ of $x$ stays inside the sphere with center $x$ and radius $r$
- Sphere packing: the art of error correction code
- https://en.wikipedia.org/wiki/Sphere_packing



# 0415 Differential Entropy (1)


## Differential Entropy

### Definition

æ¦‚ç‡è®ºä¸­çš„ä¸€äº›æ¦‚å¿µ

- Let $X$ be a random variable with **cumulative distribution function** $F(x)=\operatorname{Pr}(X \leq x)$
- If $F(x)$ is continuous, the random variable is said to be **continuous**.
- Let $f(x)=F^{\prime}(x)$ when the derivative is defined. If $\int_{-\infty}^{\infty} f(x)=1, f(x)$ is called the **probability density function** for X.
- The set where $f(x)>0$ is called the **support set** of $X .$

å¾®åˆ†ç†µçš„å®šä¹‰

> The differential entropy $h(X)$ of a continuous random variable $X$ with density $f(x)$ is defined as
> $$
> h(X)=-\int_{S} f(x) \log f(x) d x
> $$
> where $S$ is the support set of the random variable. The differential entropy is sometimes written as $h(f)$ rather than $h(X)$


å¹³ç§»ä¸æ”¹å˜å¾®åˆ†ç†µ.
$h(X+c)=h(X)$ (Translation does not change the differential entropy)

å¯¹æ¯”ç¦»æ•£ç†µ
$$\begin{aligned}
&p(x) \Rightarrow f(x)\\
&\sum=\int\\
&H(X) \Rightarrow h(X)
\end{aligned}$$

$H(X)$ is always non-negative. $h(X)$ may be negative

### Example

> Consider a random variable distributed uniformly from 0 to $a$, then $h(X)=\log a$.
> Let $X \sim \mathcal{N}\left(\mu, \sigma^{2}\right),$ then $\boldsymbol{h}(\boldsymbol{X})=\frac{1}{2} \log 2 \pi e \sigma^{2}$

When $X$ is uniformly distributed in $[0, a]$
$$
\begin{array}{c}
f(x)=1 / a \\
h(X)=-\int_{0}^{a} \frac{1}{a} \log \frac{1}{a} d x=\log a
\end{array}
$$
å‘ç°, å¾®åˆ†ç†µå·²ç»å¯ä»¥å°äº0äº†.

When $X$ is Gaussian $\mathcal{N}\left(\mu, \sigma^{2}\right),$ then
$$
f(x)=\frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{-\frac{(x-\mu)^{2}}{2 \sigma^{2}}}
$$
$$
\begin{aligned}
h(f(x))&=-\int f(x) \log f(x) d x \\
=&-\int f(x) \log \frac{1}{\sqrt{2 \pi \sigma^{2}}}+f(x)\left(-\frac{(x-\mu)^{2}}{2 \sigma^{2}}\right) d x \\
\end{aligned}
$$
$$
\int f(x) d x=1 \text { and } \operatorname{Var}(X)=\int(x-\mu)^{2} f(x) d x=\sigma^{2}
$$
ç”¨ä»£å…¥æ–¹å·®çš„æ–¹æ³•, ç®€åŒ–è¿ç®—.
$$
h(f(x))=\frac{1}{2} \log 2 \pi \sigma^{2}+\frac{1}{2}=\frac{1}{2} \log 2 \pi e \sigma^{2}
$$
æˆ‘ä»¬ç”¨eä¸ºåº•, æ–¹ä¾¿ä»£å…¥æ¦‚ç‡å¯†åº¦å‡½æ•°.


## h(X): infinite information

è¿ç»­å˜é‡ä¸åŠ ä»¥ä»»ä½•é™åˆ¶, å®ƒçš„ä¿¡æ¯æ˜¯æ— ç©·å¤§çš„. å¾®åˆ†ç†µæ— æ³•å†åº¦é‡ç³»ç»Ÿä¸­çš„ä¿¡æ¯é‡äº†.

- Differential entropy does not serve as a measure of the average amount of information contained in a continuous random variable.
- In fact, a continuous random variable generally contains an infinite amount of information

æˆ‘ä»¬è®¡ç®—è¿ç»­å‹éšæœºå˜é‡çš„ç¦»æ•£ç†µ:
Let $X$ be uniformly distributed on $[0,1) .$ Then we can write
$$
X=0 . X_{1} X_{2}, \dots
$$
The dyadic expansion of $X,$ where $X_{i}^{\prime} s$ is a sequence of i.i.d bits. Then
$$
\begin{aligned}
H(X) &=H\left(X_{1}, X_{2}, \ldots\right) \\
&=\sum_{i=1}^{\infty} H\left(X_{i}\right) \\
&=\sum_{i=1}^{\infty} 1 \\
&=\infty
\end{aligned}
$$

## h(aX): Stretching Random Variable

> $$\begin{array}{c}
> \boldsymbol{h}(\boldsymbol{a} \boldsymbol{X})=\boldsymbol{h}(\boldsymbol{X})+\log |\boldsymbol{a}| \\
> \boldsymbol{h}(\boldsymbol{A X})=\boldsymbol{h}(\boldsymbol{X})+\log |\operatorname{det} \boldsymbol{A}|
> \end{array}$$

Let $Y=a X .$ Then $f_{Y}(y)=\frac{1}{|a|} f_{X}\left(\frac{y}{a}\right),$ and
$$
\begin{aligned}
h(a X) &=-\int f_{Y}(y) \log f_{Y}(y) d y \\
&=-\int \frac{1}{|a|} f_{X}\left(\frac{y}{a}\right) \log \left(\frac{1}{|a|} f_{X}\left(\frac{y}{a}\right)\right) d y \\
&=-\int f_{X}(x) \log f_{X}(x) d x+\log |a| \\
&=h(X)+\log |a|
\end{aligned}
$$

## Differential and Discrete Entropy

![](img/04-15-10-22-58.png)

Suppose that we divide the range of $X$ into bins of length $\Delta .$ 
By the mean value theorem, there exists a value $x_{i}$ within each bin such that
$$f\left(x_{i}\right) \Delta=\int_{i \Delta}^{(i+1) \Delta} f(x) d x$$

Consider the quantized random variable $X^{\Delta}$, which is defined by
$$X^{\Delta}=x_{i}$ if $i \Delta \leq x<(i+1) \Delta$$

åŸºäºè¿ç»­éšæœºå˜é‡å®šä¹‰ä¸€ä¸ªåˆ‡å‰²æ„ä¹‰ä¸Šçš„ç¦»æ•£éšæœºå˜é‡. Then the probability that $X^{\Delta}=x_{i}$ is
$$p_{i}=\int_{i \Delta}^{(i+1) \Delta} f(x) d x=f\left(x_{i}\right) \Delta$$

$$H\left(X^{\Delta}\right)=-\sum \Delta f\left(x_{i}\right) \log f\left(x_{i}\right)-\log \Delta$$

ç»“è®º:
è¿™é‡Œè¯´çš„æ˜¯$-\infty +\infty \rightarrow h(f)$.
$$H\left(X^{\Delta}\right)+\log \Delta \rightarrow h(f)=h(X), \text { as } \Delta \rightarrow 0$$


## AEP For Continuous Random Variable

- AEP for continuous random variables: Let $X_{1}, X_{2}, \ldots, X_{n}$ be a sequence of random variables drawn i.i.d. according to the density $f(x) .$ Then
  $$
  -\frac{1}{n} \log f\left(X_{1}, X_{2}, \ldots, X_{n}\right) \rightarrow E(-\log f(X))=h(f)
  $$
  in probability

- For $\epsilon \geq 0$ and any $n,$ we define the typical set $A_{\epsilon}^{(n)}$ with respect to $f(x)$ as follows:
  $$
  A_{\epsilon}^{(n)}=\left\{\left(x_{1}, x_{2}, \ldots, x_{n}\right) \in S^{n}:\left|-\frac{1}{n} \log f\left(x_{1}, x_{2}, \ldots, x_{n}\right)-h(X)\right| \leq \epsilon\right\}
  $$
  where $f\left(x_{1}, x_{2}, \ldots, x_{n}\right)=\prod_{i=1}^{n} f\left(x_{i}\right)$

è¿ç»­æƒ…å†µ,è€ƒè™‘å…ƒç´ çš„ä½“ç§¯
The volume of a set $A \subset \mathcal{R}^{n}$ is defined as
$$
\operatorname{Vol}(A)=\int_{A} d x_{1} d x_{2} \ldots d x_{n}
$$
åœ¨è¿™é‡Œ, $2^{n h(X)}$ is the volume

The typical set $A_{\epsilon}^{(n)}$ has the following properties:
1. $\operatorname{Pr}\left(A_{\epsilon}^{(n)}\right)>1-\epsilon$ for $n$ sufficiently large.
2. Vol $\left(A_{\epsilon}^{(n)}\right) \leq 2^{n(h(X)+\epsilon)}$ for all $n$
3. Vol $\left(A_{\epsilon}^{(n)}\right) \geq(1-\epsilon) 2^{n(h(X)-\epsilon)}$ for $n$ sufficiently large.




## Joint and Conditional Differential Entropy

> - The differential entropy of a set $X_{1}, X_{2}, \ldots, X_{n}$ of random variables with density $f\left(x_{1}, x_{2}, \ldots, x_{n}\right)$ is defined as
>   $$
>   h\left(X_{1}, X_{2}, \dots, X_{n}\right)=-\int f\left(x^{n}\right) \log f\left(x^{n}\right) d x^{n}
>   $$
> - If $X, Y$ have a ioint density function $f(x, y),$ we can define the conditional differential entropy $h(X | Y)$ as
>   $$
>   \begin{array}{c}
>   h(X) | Y)=-\int f(x, y) \log f(x | y) d x d y \\
>   h(X | Y)=h(X, Y)-h(Y)
>   \end{array}
>   $$

è€ƒè™‘åˆ°,å¾ˆå¤šæƒ…å†µä¸‹,ç§¯åˆ†æ˜¯æ²¡æ³•æ±‚/ä¸å­˜åœ¨çš„,æˆ‘ä»¬åœ¨æœ¬è¯¾ç¨‹ä¸­ä»…å‡è®¾ç§¯åˆ†æ˜¯å­˜åœ¨çš„. ä½†æ˜¯è¿™åœ¨ç ”ç©¶ä¸­æ˜¯éœ€è¦ç‰¹åˆ«è°¨æ…çš„åœ°æ–¹.

å¯ä»¥è¯æ˜æ¡ä»¶å‡å°‘ç†µ, chain rule, è”åˆç†µçš„ç»“è®ºä¾ç„¶æˆç«‹
Pf by expectation (è™½ç„¶å®šä¹‰çš„æ–¹å¼ä¸åŒ,ä½†æœŸæœ›çš„å†™æ³•ä¾ç„¶æˆç«‹)

- $h(X | Y) \leq h(X)$
  with equality iff $X$ and $Y$ are independent.
- (Chain rule for differential entropy)
  $$
  h\left(X_{1}, X_{2}, \ldots, X_{n}\right)=\sum_{i=1}^{n} h\left(X_{i} | X_{1}, X_{2}, \ldots, X_{i-1}\right)
  $$
- $h\left(X_{1}, X_{2}, \ldots, X_{n}\right) \leq \sum_{i=1}^{n} h\left(X_{i}\right)$
  with equality iff $X_{1}, X_{2}, \ldots, X_{n}$ are independent.

## Entropy of Multivariate Normal Distribution

### Covariance Matrix

> - The **covariance** between two random variables $X$ and $Y$ is defined as
>   $$
>   \operatorname{cov}(X ; Y)=E(X-E X)(Y-E Y)=E(X Y)-(E X)(E Y)
>   $$
> - For a random vector $X=\left[X_{1}, X_{2}, \ldots, X_{n}\right]^{T},$ the **covariance matrix** ç›¸å…³çŸ©é˜µ is defined a $=$
>   $$
>   K_{x}=E(X-E X)(X-E X)^{T}=\left[\operatorname{cov}\left(X_{i} ; X_{j}\right)\right]
>   $$
>   and the **correlation matrix** is defined as $\widetilde{K}_{X}=E X X^{T}=\left[{E} {X}_{{i}} {X}_{{j}}\right]$
> - $K_{X}=E X X^{T}-(E X)\left(E X^{T}\right)=\widetilde{K}_{X}-(E X)\left(E X^{T}\right)$


- A covariance matrix is both symmetric and positive semidefinite. åæ–¹å·®çŸ©é˜µåŠæ­£å®š
  - The eigenvalues of a positive semidefinite matrix are non-negative. ç‰¹å¾å€¼éè´Ÿ
- çº¿æ€§å˜æ¢ä½œç”¨äºåæ–¹å·®çŸ©é˜µå’Œå…³è”çŸ©é˜µ. Let $Y=A X,$ where $X$ and $Y$ are column vectors of $n$ random variables and $A$ is an $n \times n$ matrix. Then
  $$K_{Y}=A K_{X} A^{T}$$
  and
  $$\widetilde{K}_{Y}=A \tilde{K}_{Y} A^{T}$$


> A set of correlated random variables can be regarded as an orthogonal
> transformation of a set of uncorrelated random variables. (Ref : Ch. 10.1 Yeung, Information theory and network coding)

### Multivariate Normal Distribution

> - In probability theory and statistics, the multivariate normal distribution, multivariate Gaussian distribution, or joint normal distribution is a generalization of the onedimensional (univariate) normal distribution to higher dimensions.
> - åŸå§‹å®šä¹‰: å‘é‡çš„ä»»æ„çº¿æ€§ç»„åˆéƒ½æ˜¯é«˜æ–¯åˆ†å¸ƒ ~ å‘é‡æœä»å¤šå…ƒé«˜æ–¯åˆ†å¸ƒ
> - More generally, let $\mathcal{N}(\mu, K)$ denote the multivariate Gaussian distribution with mean $\mu$ and covariance matrix $K,$ i.e., the joint pdf of the distribution is given by
>   $$f(x)=\frac{1}{(\sqrt{2 \pi})^{n}|K|^{1 / 2}} e^{-\frac{1}{2}(x-\mu)^{T} K^{-1}(x-\mu)}$$
> - One definition is that a random vector is said to be **k-variate normally** distributed if every linear combination of its k components has a univariate normal distribution. 

å¤šå…ƒé«˜æ–¯åˆ†å¸ƒè‰¯å¥½çš„æ€§è´¨
- In general, random variables may be uncorrelated but statistically dependent. 
- ä¸¤ä¸ªäº‹ä»¶ä¸ç›¸å…³ä¸ä»£è¡¨ä¸¤ä¸ªäº‹ä»¶ç‹¬ç«‹. ä½†å¯¹å¤šå…ƒé«˜æ–¯åˆ†å¸ƒ ä¸€å®šæˆç«‹. But if a random vector has a multivariate normal distribution then any two or more of its components that are uncorrelated are independent .
- ä¸¤ä¸¤ç‹¬ç«‹æ¨å‡ºè”åˆç‹¬ç«‹. This implies that any two or more of its components that are pairwise independent are independent.

### Entropy

> (Entropy of a multivariate normal distribution) Let $X_{1}, X_{2}, \ldots, X_{n}$ have a multivariate normal distribution with mean $\mu$ and covariance matrix $K$
> $$h\left(X_{1}, X_{2}, \dots, X_{n}\right)=h(\mathcal{N}(\mu, K))=\frac{1}{2} \log (2 \pi e)^{n}|K|$$
> where $|K|$ denotes the determinant of $K .$
è®°ä½,å¯¹äºå¤šå…ƒé«˜æ–¯åˆ†å¸ƒ,å¾®åˆ†ç†µæ˜¯å¯ä»¥è®¡ç®—å‡ºæ¥çš„, ä¸”å’Œåæ–¹å·®çŸ©é˜µå¯¹æ•°å€¼ç›¸å…³å³å¯.
$$f(\mathbf{x})=\frac{1}{(\sqrt{2 \pi})^{n}|K|^{\frac{1}{2}}} e^{-\frac{1}{2}(\mathbf{x}-\mu)^{T} K^{-1}(\mathbf{x}-\mu)}$$

è¯æ˜ä¸ä½œå…·ä½“è¦æ±‚, ä¸ä¸€å…ƒæƒ…å†µç±»ä¼¼.
æŠ€å·§:æœŸæœ›å’Œæ±‚å’Œè¿›è¡Œäº¤æ¢
$$\begin{aligned}
h(f) &=-\int f(\mathbf{x})\left[-\frac{1}{2}(\mathbf{x}-\mu)^{T} K^{-1}(\mathbf{x}-\mu)-\ln (\sqrt{2 \pi})^{n}|K|^{\frac{1}{2}}\right] d \mathbf{x} \\
&=\frac{1}{2} E\left[\sum_{i, j}\left(X_{i}-\mu_{i}\right)\left(X_{j}-\mu_{j}\right)\left(K^{-1}\right)_{i j}\right]+\frac{1}{2} \ln (2 \pi)^{n}|K| \\
&=\frac{1}{2} \sum_{i, j} E\left[\left(X_{j}-\mu_{j}\right)\left(X_{i}-\mu_{i}\right)\right]\left(K^{-1}\right)_{i j}+\frac{1}{2} \ln (2 \pi)^{n}|K| \\
&=\frac{1}{2} \sum_{j} \sum_{i} K_{j i}\left(K^{-1}\right)_{i j}+\frac{1}{2} \ln (2 \pi)^{n}|K| \\
&=\frac{1}{2} \sum_{j}\left(K K^{-1}\right)_{j j}+\frac{1}{2} \ln (2 \pi)^{n}|K| \\
&=\frac{1}{2} \sum_{j} I_{j j}+\frac{1}{2} \ln (2 \pi)^{n}|K|\\
&=\frac{n}{2}+\frac{1}{2} \ln (2 \pi)^{n}|K|\\
&=\frac{1}{2} \ln (2 \pi e)^{n}|K|\\
&=\frac{1}{2} \log (2 \pi e)^{n}|K|
\end{aligned}$$


## Relative Entropy and Mutual Information

> - The **relative entropy (or Kullback-Leibler distance) $D(f \| g)$** between two densities $f$ and $g$ is defined by
>   $$D(f \| g)=\int f \log \frac{f}{g}$$
> - The **mutual information $I(X ; Y)$** between two random variables with joint density $f(x, y)$ is defined as
>   $$I(X ; Y)=\int f(x, y) \log \frac{f(x, y)}{f(x) f(y)} d x d y$$

æœ‰å…³ç›¸å¯¹ç†µå’Œäº’ä¿¡æ¯çš„æ€§è´¨:
- $I(X ; Y)=h(X)-h(X | Y)=h(Y)-h(Y | X)=h(X)+h(Y)-h(X, Y)$,
  $I(X ; Y)=D(f(x, y) \| f(x) f(y))$
  ä¿¡æ¯å›¾ä¾ç„¶å¯ç”¨,ä½†å¯¹éè´Ÿéƒ¨åˆ†éœ€è¦è€ƒè™‘.
- $D(f \| g) \geq 0$ with equality iff $f=g$ almost everywhere (a.e.). ä¸æ­¤å‰è¯æ˜ç±»ä¼¼
- $I(X ; Y) \geq 0$ with equality iff $X$ and $Y$ are independent. ä»ç›¸å¯¹ç†µæ¨å‡º

### Mutual Information: Master Definition

äº’ä¿¡æ¯å’Œç†µè¿˜æ˜¯æœ‰å¾ˆå¤§åŒºåˆ«çš„. æˆ‘ä»¬è€ƒè™‘è¿›é˜¶çš„å®šä¹‰æ–¹å¼:

The mutual information between two random variables is the limit of the mutual information between their quantized versions
$$
\begin{aligned}
I\left(X^{\Delta} ; Y^{\Delta}\right) &=H\left(X^{\Delta}\right)-H\left(X^{\Delta} | Y^{\Delta}\right) \\
& \approx h(X)-\log \Delta-(h(x | y)-\log \Delta) \\
&=I(X ; Y)
\end{aligned}
$$

æˆ‘ä»¬å‘ç°, ç¦»æ•£å‹éšæœºå˜é‡ä¸è¿ç»­å‹éšæœºå˜é‡çš„äº’ä¿¡æ¯(ç¦»æ•£åŒ–)æ˜¯è¿‘ä¼¼ç›¸ç­‰çš„.

> **Definition**. The mutual information between two random variables $X$ and $Y$ is given by
> $$I(X ; Y)=\sup _{\mathcal{P}, Q} I\left([X]_{\mathcal{P} ;}[Y]_{Q}\right)$$
> where the supremum is over all finite partitions $\mathcal{P}$ and $Q$
> è¿™é‡Œ, $X$ å’Œ $Y$ æ—¢å¯ä»¥æ˜¯ç¦»æ•£çš„ä¹Ÿå¯ä»¥æ˜¯è¿ç»­çš„(4ç§æƒ…å†µ), ä½†å®šä¹‰æ˜¯é€šç”¨çš„.

- Let $\mathcal{X}$ be the range of a random variable $X .$ A partition $\mathcal{P}$ of $\mathcal{X}$ is a finite collection of disjoint sets $P_{i}$ such that $U_{i} P_{i}=\mathcal{X}$. The quantization of $X$ by $\mathcal{P}$ (denoted $[X]_{\mathcal{P}}$ ) is the discrete random variable defined by
  $$\operatorname{Pr}\left([X]_{P}=i\right)=\operatorname{Pr}\left(X \in P_{i}\right)=\int_{P_{i}} d F(x)$$
- For two random variables $X$ and $Y$ with partitions $\mathcal{P}$ and $Q,$ we can calculate the mutual information between the quantized versions of $X$ and $Y$

> This is the master definition of mutual information that always applies, even to joint distributions with atoms, densities, and singular parts.

# 0420 Differential Entropy (2)


## Correlated Gaussian
æˆ‘ä»¬è®¡ç®—ä¸¤ä¸ªéšæœºå˜é‡çš„äº’ä¿¡æ¯ï¼Œè¿™é‡Œä»¥è”åˆé«˜æ–¯åˆ†å¸ƒä¸ºä¾‹ã€‚
(Mutual information between correlated Gaussian random variables with correlation $\rho$ ) Let $(X, Y) \sim \mathcal{N}(0, K),$ where
$$
\boldsymbol{K}=\left[\begin{array}{cc}
\boldsymbol{\sigma}^{2} & \boldsymbol{\rho} \boldsymbol{\sigma}^{2} \\
\boldsymbol{\rho} \boldsymbol{\sigma}^{2} & \boldsymbol{\sigma}^{2}
\end{array}\right]
$$
$I(X ; Y) ?$

$$
\begin{array}{c}
h(X)=h(Y)=\frac{1}{2} \log 2 \pi e \sigma^{2} \\
h(X, Y)=\frac{1}{2} \log (2 \pi e)^{2}|K|=\frac{1}{2} \log (2 \pi e)^{2} \sigma^{4}\left(1-\rho^{2}\right) \\
I(X ; Y)=h(X)+h(Y)-h(X, Y)=-\frac{1}{2} \log \left(1-\rho^{2}\right)
\end{array}
$$
- $\rho=0, X$ and $Y$ are independent and $I$ is 0
- $\rho=\pm 1, X$ and $Y$ are perfectly correlated and $I$ is $\infty$

## Maximum Entropy with Constraints

$E(X^2),Var(X)$ç»™å®šçš„æƒ…å†µä¸‹ï¼Œé«˜æ–¯åˆ†å¸ƒæœ€å¤§åŒ–å¾®åˆ†ç†µã€‚

> - Let the random variable $X \in R$ have mean $\mu$ and variance $\sigma^{2}$. Then
>   $$
>   h(X) \leq \frac{1}{2} \log 2 \pi e \sigma^{2}
>   $$
>   with equality iff $X \sim \mathcal{N}\left(\boldsymbol{\mu}, \boldsymbol{\sigma}^{2}\right)$
> - Let the random variable $X \in R$ satisfy $E X^{2} \leq \sigma^{2} .$ Then
>   $$
>   h(x) \leq \frac{1}{2} \log 2 \pi e \sigma^{2}
>   $$
>   with equality iff $X \sim \mathcal{N}\left(0, \sigma^{2}\right)$

è¯æ˜åŸŸå¹³å‡åˆ†å¸ƒæœ€å¤§åŒ–ç¦»æ•£ç†µçš„è¯æ˜å¦‚ä¸‹ï¼Œæˆ‘ä»¬ç”¨ç›¸å¯¹ç†µæ¨å‡ºã€‚

1. Let $X_{G} \sim \mathcal{N}\left(\mu, \sigma^{2}\right) .$ Consider 
   $$\boldsymbol{D}\left(X \| X_{G}\right) \geqq \mathbf{0}$$
    Then
$$
  \int f \log \frac{f}{g} \geq 0
$$
  æŠŠå¯¹æ•°å‡½æ•°å±•å¼€ï¼Œç”±äº$g$æ˜¯é«˜æ–¯åˆ†å¸ƒï¼Œå¯ä»¥è¿›ä¸€æ­¥å±•å¼€ã€‚
  $$h(X)=h(f) \leq-\int f \log g=-\int f \log \frac{1}{\sqrt{2 \pi \sigma^{2}}}+f\left(-\frac{(x-\mu)^{2}}{2 \sigma^{2}}\right)$$
  ç”±äºå³ä¾§éƒ½æ˜¯å¸¸æ•°ï¼Œå¯ä»£å…¥åŒ–ç®€ã€‚
  $$h(X) \leq \frac{1}{2} \log 2 \pi \sigma^{2}+\frac{1}{2}=\frac{1}{2} \log 2 \pi e \sigma^{2}$$
2. $\operatorname{Var}(X)=E\left(X^{2}\right)-E(X)^{2} \leq \sigma^{2} \cdot \Rightarrow$ Case 1

ä½¿ç”¨è¿™ä¸¤ä¸ªç»“è®ºæ—¶ä¸€å®šè¦æ³¨æ„æ˜¯å¦å­˜åœ¨ç¡®å®šçš„å‡å€¼ã€æ–¹å·®æˆ–äºŒé˜¶çŸ©æ˜¯å¦å­˜åœ¨ä¸Šç•Œã€‚

## Maximum Entropy

æœ€å¤§ç†µåŸç†åœ¨ä¸åŒçš„é™åˆ¶ä¸‹å¯ä»¥å¾—åˆ°ä¸åŒçš„ç»“è®ºã€‚ï¼ˆè¯¦è§Cover Ch.12ï¼‰

Consider the following problem: Maximize the entropy $h(f)$ over all probability densities $f$ satisfying ï¼ˆ$++$æ¡ä»¶ï¼‰
1. $f(x) \geq 0,$ with equality outside the support éè´Ÿæ€§
2. $\int_{s} f(x) d x=1$ è§„èŒƒæ€§
3. $\int_{S} f(x) r_{i}(x) d x=\alpha_{i}$ for $1 \leq i \leq m .\left(r_{i}(x) \text { is a function of } \right) x$. Thus, $f$ is a density on support set $S$ meeting certain moment constraints $\alpha_{1}, \alpha_{2}, \ldots, \alpha_{m}$ å³æŸäº›å…³äº$x$çš„å‡½æ•°çš„å‡å€¼æ˜¯ä¸€å®šçš„ã€‚

> Theorem 12.1.1 (Maximum entropy distribution) Let 
> $$f^{*}(x)=f_{\lambda}(x)=e^{\lambda_{0}+\sum_{i=1}^{m} \lambda_{i} r_{i}(x)}$$
> $x \in S,$ where $\lambda_{0}, \ldots, \lambda_{m}$ are chosen so that $f^{*}$ satisfies $(++) .$ Then $f^{*}$ uniquely maximizes $h(f)$ over all probability densities $f$ satisfying constraints $(++)$
> æœ€å¤§ç†µçš„åˆ†å¸ƒæ˜¯$f^{*}$å–åˆ°çš„ï¼Œä½†å…¶ä¸­ä¸€äº›ç³»æ•°éœ€è¦é€šè¿‡$\lambda_i$ä½œä¸ºå¾…å®šç³»æ•°ï¼Œè¿˜éœ€æ›´å¤šæ¡ä»¶å¯ç¡®å®šå¾…å®šç³»æ•°ã€‚

**Proof.**
$$\begin{aligned}
h(g) &=-\int_{S} g \ln g \\
&=-\int_{S} g \ln \frac{g}{f^{*}} f^{*} \\
&=-D\left(g \| f^{*}\right)-\int_{S} g \ln f^{*} \\
& \stackrel{(a)}{\leq}-\int_{S} g \ln f^{*} \\
& \stackrel{(b)}{=}-\int_{S} g\left(\lambda_{0}+\sum \lambda_{i} r_{i}\right) \\
& \stackrel{(c)}{=}-\int_{S} f^{*}\left(\lambda_{0}+\sum \lambda_{i} r_{i}\right) \\
&=-\int_{S} f^{*} \ln f^{*} \\
&=h\left(f^{*}\right)
\end{aligned}$$

where (a) follows from the nonnegativity of relative entropy, (b) follows from the definition of $f^{*},$ and $(\mathrm{c})$ follows from the fact that both $f^{*}$ and $g$ satisfy the constraints. Note that equality holds in (a) if and only if $g(x)=f^{*}(x)$ for all $x,$ except for a set of measure $0,$ thus proving uniqueness.

The same approach holds for discrete entropies and for multivariate distributions.

**Examples.**
- Let $S=[a, b],$ with no other constraints. Then the maximum entropy distribution is the uniform distribution over this range.
- $S=[0, \infty)$ and $E X=\mu .$ Then the entropy-maximizing distribution is
  $$
  f(x)=\frac{1}{\mu} e^{-\frac{x}{\mu}}, \quad x \geq 0
  $$
- $S=(-\infty, \infty), E X=\alpha_{1},$ and $E X^{2}=\alpha_{2} .$ The maximum entropy distribution is $\mathcal{N}\left(\alpha_{1}, \alpha_{2}-\alpha_{1}^{2}\right)$

## Hadamard's Inequality

$K$ is a nonnegative definite symmetric $n \times n$ matrix. Let $|K|$ denote the determinant of $K$
> Theorem (Hadamard) $|K| \leq \prod K_{i i},$ with equality iff $K_{i j}=0, \quad i \neq j$

**Proof.**
Let $X \sim \mathcal{N}(0, K) .$ Then
$$
\frac{1}{2} \log (2 \pi e)^{n}|K|=h\left(X_{1}, X_{2}, \ldots, X_{n}\right) \leqq \Sigma h\left(X_{i}\right)=\sum_{i=1}^{n} \frac{1}{2} \log 2 \pi e\left|K_{i i}\right|
$$
with equality iff $\left.X_{1}, X_{2}, \ldots, X_{n} \text { are independent (i.e., } K_{i j}=0, i \neq j\right)$

ideaï¼šçŸ©é˜µè½¬åŒ–ä¸ºå¤šå…ƒé«˜æ–¯åˆ†å¸ƒï¼Œè”åˆåˆ†å¸ƒç†µå°äºè¾¹ç¼˜åˆ†å¸ƒç†µçš„å’Œã€‚
remarkï¼šç†µæ˜¯ä¸€ä¸ªåŸºç¡€çš„ç‰©ç†é‡ï¼Œå¯ä»¥ç”¨æ¥è¯æ˜å¾ˆå¤šä¸ç­‰å¼(Cover Ch 17.9, 17.10)ï¼Œæ¯”å¦‚ä¸€ç³»åˆ—æœ‰å…³æ­£å®šçŸ©é˜µçš„æ€§è´¨
- $\log |K|$ is concave
- $\log \left(\left|K_{n}\right| /\left|K_{n-p}\right|\right)$ is concave in $K_{n}$
- $\left|K_{n}\right| /\left|K_{n-1}\right|$ is concave in $K_{n}$

## Balanced Information Inequality

å¹³è¡¡ä¿¡æ¯ä¸ç­‰å¼ï¼šç¦»æ•£ç†µä¸å¾®åˆ†ç†µçš„åŒå’Œä¸åŒ
Differences between inequalities of the discrete entropy and differential entropy
- Both $H(X, Y) \leq H(X)+H(Y)$ and $h(X, Y) \leq h(X)+h(Y)$ are valid
- $H(X, Y) \geq H(X)$ but neither $h(X, Y) \geq h(X)$ nor $h(X, Y) \leq h(X)$ is valid
Take $H(X, Y, Z) \leq \frac{1}{4} H(X)+\frac{1}{2} H(Y, Z)+\frac{3}{4} H(Z, X)$ for example.
Count the weights of random variables $X, Y, Z$ in both sides $X: 1,1 ; Y: 1, \frac{1}{2} ; Z: 1, \frac{5}{4}$ å®šä¹‰$X,Y,Z$çš„å‡€æƒé‡ã€‚
The net weights of $X, Y, Z$ are $0, \frac{1}{2},-\frac{1}{4}$

æ¯”å¦‚ï¼Œä¸‹é¢çš„ä¸ç­‰å¼æ˜¯å¹³è¡¡çš„ï¼š
Balanced: If the net weights of $X, Y, Z$ are all zero.
$$
h(X, Y) \leq h(X)+h(Y) \text { and } h(X, Y, Z) \leq \frac{1}{2} h(X, Y)+\frac{1}{2} h(Y, Z)+\frac{1}{2} h(Z, X)
$$

> å¯¹æ›´ä¸ºä¸€èˆ¬çš„æƒ…å†µï¼Œ
> Let $[n]:=\{1,2, \ldots, n\} .$ For any $\alpha \subseteq[n],$ denote $\left(X_{i}: i \in \alpha\right)$ by $X_{\alpha} .$ For example, $\alpha=\{1,3,4\},$ we denote $X_{1}, X_{3}, X_{4}$ by $X_{(1,3,4)}$ for simplicity.
> - We could write any information inequality in the form $\Sigma_{\alpha} w_{\alpha} H\left(X_{\alpha}\right) \geq 0$ or $\Sigma_{\alpha} w_{\alpha} h\left(X_{\alpha}\right) \geq 0$
> - An information inequality is called balanced if for any $i \in[n]$, the net weight of $X_{i}$ is zero.
> - The linear continuous inequality $\Sigma_{\alpha} w_{\alpha} h\left(X_{\alpha}\right) \geq 0$ is valid if and only if its corresponding discrete counterpart $\Sigma_{\mathrm{g}} w_{\mathrm{g}} H\left(X_{\mathrm{g}}\right) \geq 0$ is valid and balanced.
> ç”±æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å»ºç«‹å¾®åˆ†ç†µä¸ç­‰å¼å’Œç¦»æ•£ç†µä¸ç­‰å¼çš„å…³ç³»ã€‚è¿™ä¸ªä¸ç­‰å¼æ˜¯æ­£ç¡®çš„**å½“ä¸”ä»…å½“**å®ƒå¯¹åº”çš„ç¦»æ•£ç†µä¸ç­‰å¼æ˜¯æ­£ç¡®çš„ä¸”å¹³è¡¡çš„ã€‚

Ref: Balanced Information Inequalities, T. H. Chan, IEEE Transactions
on Information Theory, Vol. 49 , No. 12 , December 2003

## Hanâ€™s Inequality

Let $\left(X_{1}, X_{2}, \ldots, X_{n}\right)$ have a density, and for every $S \subseteq\{1,2, \ldots, n\},$ denoted by $X(S)$ the subset $\left\{X_{i}: i \in S\right\} .$ Let
$$
\begin{array}{c}
h_{k}^{(n)}=\frac{1}{\left(\begin{array}{c}
n \\
k
\end{array}\right)} \sum_{S:[S]=k} \frac{h(X(S))}{k} \\
g_{k}^{(n)}=\frac{1}{\left(\begin{array}{l}
n \\
k
\end{array}\right)} \sum_{S:|S|=k} \frac{h\left(X(S) | X\left(S^{c}\right)\right)}{k}
\end{array}
$$

ä»nä¸ªéšæœºå˜é‡ä¸­å–kä¸ªï¼Œæ±‚è”åˆç†µã€æ¡ä»¶ç†µã€‚

When $n=3$,
$$\begin{aligned}
h_{1}^{(3)} &=\frac{H\left(X_{1}\right)+H\left(X_{2}\right)+H\left(X_{3}\right)}{3} \geq h_{2}^{(3)}=\frac{H\left(X_{1}, X_{2}\right)+H\left(X_{2}, X_{3}\right)+H\left(X_{3}, X_{1}\right)}{3} \\
& \geq h_{3}^{(3)}=H\left(X_{1}, X_{2}, X_{3}\right)
\end{aligned}$$

$$\begin{aligned}
g_{1}^{(3)} &=\frac{H\left(X_{1} | X_{2}, X_{3}\right)+H\left(X_{2} | X_{1}, X_{3}\right)+H\left(X_{3} | X_{1}, X_{2}\right)}{3} \\
& \leq g_{2}^{(3)}=\frac{H\left(X_{1}, X_{2} | X_{3}\right)+H\left(X_{2}, X_{3} | X_{1}\right)+H\left(X_{3}, X_{1} | X_{2}\right)}{3} \\
& \leq g_{3}^{(3)}=H\left(X_{1}, X_{2}, X_{3}\right)
\end{aligned}$$

> Han's inequality:
> $h_{1}^{(n)} \geq h_{2}^{(n)} \ldots \geq h_{n}^{(n)}=H\left(X_{1}, X_{2}, \ldots, X_{n}\right)=g_{n}^{(n)} \geq \cdots \geq g_{2}^{(n)} \geq g_{1}^{(n)}$



## Information Heat


### Heat Equation

- Heat equation (Fourier): Let $x$ be the position and $t$ be the time, çƒ­ä¼ å¯¼æ–¹ç¨‹ã€‚**ï¼ˆå®ƒä¸é«˜æ–¯ä¿¡é“æ˜¯ç­‰ä»·çš„ï¼‰**
  $$\frac{\partial}{\partial t} f(x, t)=\frac{1}{2} \frac{\partial^{2}}{\partial x^{2}} f(x, t)$$
- Let $X$ be any random variable with a density $f(x)$. Let $Z$ be an independent normally distributed random variable with zero mean and unit variance, $Z \sim \mathcal{N}(0,1) .$ Let
  $$Y_{t}=X+\sqrt{t} Z$$
  The **probability density function** $f(y ; t)(f(y ; t) \text { is a function in } y, \text { not } t)$ of $Y_{t}$ **satisfies heat equation**
  $$f(y ; t)=\int f(x) \frac{1}{\sqrt{2 \pi t}} e^{\frac{(y-x)^{2}}{2 t}} d x$$
  é«˜æ–¯ä¿¡é“çš„è¾“å‡ºä¿¡å·ä¸çƒ­ä¼ å¯¼æ–¹ç¨‹å…·æœ‰ä¸€ä¸€å¯¹åº”çš„å…³ç³»ã€‚

### Entropy and Fisher Information
å¯¹è¿ç»­å‹éšæœºå˜é‡å®šä¹‰ä¸€ä¸ªæ–°çš„ä¿¡æ¯é‡ï¼Œ Fisher Information

> Fisher information: Let $X$ be any random variable with density $f(x)$. Its Fisher information is given by
> $$
> I(x)=\int_{-\infty}^{+\infty} f(x)\left[\frac{\frac{\partial}{\partial x} f(x)}{f(x)}\right]^{2} d x
> $$
- Let $X$ be any random variable with a density $f(x)$. Let $Z$ be an independent normally distributed random variable with zero mean and unit variance. Let $Y_{t}=X+\sqrt{t} Z$
  $$\frac{\partial}{\partial t} h\left(Y_{t}\right)=\frac{1}{2} I\left(Y_{t}\right)$$ è¡¨æ˜ä¿¡æ¯é‡ä¸ç»Ÿè®¡é‡ä¹‹é—´ä¹Ÿå­˜åœ¨å…³è”ã€‚
- Let $f(y, t) \text { (or } f)$ be the p.d.f of $Y_{t}$
  $$\begin{array}{c}
  \frac{\partial}{\partial t} h\left(Y_{t}\right)=\frac{1}{2} I\left(Y_{t}\right)=\frac{1}{2} \int \frac{f_{y}^{2}}{f} d y \geq 0 \\
  \frac{\partial^{2}}{\partial t^{2}} h\left(Y_{t}\right)=-\frac{1}{2} \int f\left(\frac{f_{y y}}{f}-\frac{f_{y}^{2}}{f^{2}}\right)^{2} d y \leq 0
  \end{array}$$
  $h(Y_t)$å…³äºtæ˜¯ä¸€ä¸ªé€’å¢çš„å‡¹å‡½æ•°ã€‚

- When $X$ is Gaussian $\mathcal{N}(0,1)$
  $$h\left(Y_{t}\right)=\frac{1}{2} \log 2 \pi e(1+t)$$
  å¯¹é«˜æ–¯åˆ†å¸ƒçš„è¾“å…¥ï¼Œné˜¶å¯¼æ•°å¯æ±‚ï¼Œä¸”ç¬¦å·ä¸º
  All the derivatives alternate in signs: $+,-,+,-, \dots$

### Higher Order Derivatives of â„(ğ‘Œğ‘¡)

(Cheng 2015) Let $X$ be any random variable with a density $f(x)$. Let $Z$ be an independent normally distributed random variable with zero mean and unit variance. Let $Y_{t}=X+\sqrt{t} Z$ and $f(y, t) \text { (or } f)$ be the p.d.f of $Y_{t} .$ Then
$$
\frac{\partial^{3}}{\partial t^{3}} h\left(Y_{t}\right) \geq 0 \text { and } \frac{\partial^{4}}{\partial t^{4}} h\left(Y_{t}\right) \leq 0
$$
Conjecture: When $n$ is even, $\frac{\partial^{n}}{\partial t^{n}} h\left(Y_{t}\right) \leq 0,$ otherwise $\frac{\partial^{n}}{\partial t^{n}} h\left(Y_{t}\right) \geq 0$

Ref: F. Cheng and Y. Geng, Higher Order Derivatives in Costa's Entropy Power Inequality

## EPI and FII

> (Shannon 1948, Entropy power inequality (EPI)) If $X$ and $Y$ are independent random $n$ -vectors with densities, then
> $$e^{\frac{2}{n} h(X+Y)} \geq e^{\frac{2}{n} h(X)}+e^{\frac{2}{n} h(Y)}$$
è‹¥å¯¹éšæœºå˜é‡ï¼š
$$e^{2 h(X+Y)} \geq e^{2 h(X)}+e^{2 h(Y)}$$
- ä¹Ÿå¯ä»¥äº’æ¨FIIä¸ç­‰å¼ï¼ŒFisher information inequality (FII)
  $$\frac{1}{I(X+Y)} \geq \frac{1}{I(X)}+\frac{1}{I(Y)}$$
- Most profound result in Shannon's 1948 paper
- EPI can imply some very fundamental results 
  - Uncertainty principle in quantom physics
  - Young's inequality 
  - Nash's inequality 
  - Cramer-Rao bound

References:
- T. Cover, Information theoretic inequalities, 1990
- O. Rioul , â€œInformation Theoretic Proofs of Entropy Power Inequalities,â€ 2011


# 0422 Gaussian Channel


## Gaussian Channel
è¿ç»­ä¿¡é“ä¸­æœ€å¸¸è§çš„æ˜¯é«˜æ–¯ä¿¡é“ï¼ŒèƒŒæ™¯å™ªå£°æœä»é«˜æ–¯åˆ†å¸ƒã€‚
![](img/04-22-10-03-09.png)

- the most important continuous alphabet channel is the Gaussian channel. For example, wireless telephone channels and satellite links
- The noise $Z_{i}$ is drawn i.i.d. from **a Gaussian distribution** with variance $N$
- The noise $Z_{i}$ is assumed to be **independent** of the signal $X_{i}$
- This is a time-discrete channel with output $Y_{i}$ at time $i,$ where $Y_{i}$ is the sum of the input $X_{i}$ and the noise $Z_{i}$ ï¼Œ
  $$Y_{i}=X_{i}+Z_{i}, \quad Z_{i} \sim \mathcal{N}(0, N)$$
  å¯¹è¿ç»­éšæœºå˜é‡ï¼Œä¸¤ä¸ªéšæœºå˜é‡çš„å’Œçš„æ¦‚ç‡å¯†åº¦å‡½æ•°æ˜¯å®ƒä»¬çš„å·ç§¯ã€‚
- Without further conditions, the capacity of this channel may be $\infty$.
  - The values of $X$ may be very sparse
  - ä¸€ä¸ªä¾‹å­ï¼šAssume the variance of noise $N$ is neglected compared to the distances of the values of $X$ Then $Y=X+Z \approx X .$ Thus $I(X ; Y) \approx H(X),$ which may be $\infty$ã€‚è¿ç»­å‹éšæœºå˜é‡çš„ç¦»æ•£ç†µæœ‰å¯èƒ½æ˜¯æ— ç©·å¤§çš„ï¼Œä¿¡é“å®¹é‡æ­¤æ—¶ä¹Ÿå¤±å»äº†æ„ä¹‰ã€‚

ä¸ºè¿›ä¸€æ­¥ç ”ç©¶ä¿¡é“å®¹é‡ï¼Œæˆ‘ä»¬éœ€è¦ä»å®é™…å‡ºå‘ï¼Œä¸ºä¿¡é“å¢åŠ ä¸€äº›åŠŸèƒ½ã€é™åˆ¶ã€‚

## Energy Constraint

é«˜æ–¯ä¿¡é“ä¸­å¸¸è§çš„ä¸€ä¸ªé™åˆ¶æ˜¯èƒ½é‡é™åˆ¶ã€‚èƒ½é‡é€šå¸¸ä¸æ–¹å·®ç›¸å…³ã€‚åœ¨ä¸€äº›ç ”ç©¶ä¸­ï¼Œæœ‰å…³â€œèƒ½é‡â€å¯ä»¥ç”¨å…¶ä»–æ›´å¤æ‚æˆ–æ›´ç²¾ç»†çš„æ¨¡å‹è¿›è¡Œå®šä¹‰ï¼Œä»è€Œå¾—åˆ°ç±»ä¼¼çš„æ¨å¯¼å’Œç»“è®ºã€‚

- The most common limitation on the input is **an energy or power constraint**
- We assume an average power constraint. For any codeword $\left(x_{1}, x_{2}, \ldots, x_{n}\right)$ transmitted over the channel, we require that ï¼ˆå‡è®¾ç åˆ¶ä¸­çš„ç¬¦å·å‡åŒ€åˆ†å¸ƒï¼‰
  $$\frac{1}{n} \sum_{i=1}^{n} x_{i}^{2} \leq P$$
  - within the sphere $\sqrt{n P}$
  - $P$ per channel use æ¯æ¬¡ä¿¡é“ä½¿ç”¨åŠŸç‡æ¶ˆè€—æœ€å¤§ä¸ºP
  - å¦‚æœç åˆ¶ä¸å‡åŒ€åˆ†å¸ƒï¼Œåªéœ€æ”¹æˆ$EX^2 \leq P$
- This communication channel models many practical channels, including radio and satellite links.

> The information capacity of the Gaussian channel with power constraint $P$ is
> $$C=\max _{f(x): E X^{2} \leq P} I(X ; Y)$$

ä¸‹é¢æ±‚è§£è¯¥ä¼˜åŒ–é—®é¢˜ã€‚
$$\begin{aligned}
I(X ; Y) &=h(Y)-h(Y | X) \\
&=h(Y)-h(X+Z | X) \\
&=h(Y)-h(Z | X) \\
&=h(Y)-h(Z) \\
h(Z)&=\frac{1}{2} \log 2 \pi e N
\end{aligned}$$

$$\begin{array}{l}
E Y^{2}=E(X+Z)^{2}=E X^{2}+2 E X E Z+E Z^{2}\leq P+N \\
h(Y) \leq \frac{1}{2} \log 2 \pi e(P+N)
\end{array}$$

$$\begin{aligned}
I(X ; Y) &=h(Y)-h(Z) \leq \frac{1}{2} \log 2 \pi e(P+N)-\frac{1}{2} \log 2 \pi e N \\
&=\frac{1}{2} \log \left(1+\frac{P}{N}\right)
\end{aligned}$$

ç»¼ä¸Šï¼š
$$
C=\frac{1}{2} \log \left(1+\frac{P}{N}\right)
$$
The maximum is attained when $X \sim \mathcal{N}(\mathbf{0}, P)$ï¼Œ$P\over N$ä¹Ÿè¢«ç§°ä¸ºä¿¡é“æ¯”ã€‚


## Intuition

æ¯ä¸ªè¾“å…¥ä¿¡å·éƒ½ä¼šåœ¨æ¥æ”¶ç«¯äº§ç”Ÿä¸€å®šçš„åŒºåŸŸã€‚è€ƒè™‘ä¸€ä¸ªç»™å®šçš„ç åˆ¶$x_n$
![](img/04-22-10-24-11.png)

- The received vector is **normally distributed** with mean equal to the true codeword and variance equal to the noise variance.
- With high probability, the received vector is contained **in a sphere of radius $\sqrt{n(N+\epsilon)}$ around the true codeword**.
- ä»»ä½•ç åˆ¶åœ¨é«˜æ–¯å™ªå£°çš„å½±å“ä¸‹ï¼Œæ¥æ”¶ç«¯éƒ½äº§ç”Ÿä¸€ä¸ªçƒä½“
- æˆ‘ä»¬åœ¨è§£ç æ—¶ï¼Œåªè¦è·å¾—çš„$Y_n$åœ¨çƒä½“å†…ï¼Œæˆ‘ä»¬å°±è®¤ä¸º$Y_n$å¯¹åº”$X_n$ï¼Œä¸ºäº†é™ä½é”™è¯¯ç‡ï¼Œæˆ‘ä»¬å¸Œæœ›ä»»æ„ä¸¤ä¸ªçƒéƒ½æ˜¯ä¸ç›¸äº¤çš„ã€‚
- If we assign everything within this sphere to the given codeword, when this codeword is sen there will be an error only if the received vector falls outside the sphere, which has low probability.
  - Each codeword is represented by a sphere 
  - Low decoding error requires no intersection between any spheres
é‚£ä¹ˆå¡«å……æ»¡çš„å°çƒç©ºé—´ç‡è¶Šå¤§ï¼Œç ç‡å°±è¶Šé«˜

![](img/04-22-10-29-03.png)

- The received vectors $(Y=X+Z)$ have energy no greater than $n(P+N),$ so they lie in a sphere of radius $\sqrt{n(P+N)}$
- The volume of an n-dimensional sphere is of the form $C_{n} r^{n}$, where $r$ is the radius of the sphere.
  $$2 \pi r, \pi r^{2} \text { and } \frac{4}{3} \pi r^{3}$$
- The volumes are approximated by
  $$C_{n}(n N)^{\frac{n}{2}} \text { and } C_{n}(n(P+N))^{\frac{n}{2}}$$
  æˆ‘ä»¬ä¸éœ€è¦æŠŠçƒä½“çš„å…¬å¼ç®—çš„ç‰¹åˆ«ç»†ï¼Œå¿½ç•¥å‰é¢çš„ç³»æ•°ï¼Œè®¡ç®—é«˜é˜¶çš„æƒ…å†µã€‚
  æˆ‘ä»¬å®Œæˆäº†ä¸Šç•Œçš„æ¨ç®—ã€‚å½“ç„¶ï¼Œé«˜ç»´ç©ºé—´å°çƒçš„å †æ”¾é—®é¢˜ï¼Œå…¶å®é™…çš„æ„é€ æ˜¯ä¸€ä¸ªéå¸¸å¤æ‚çš„é—®é¢˜ã€‚

## Theorems

è¯æ˜æ€è·¯ä¸DMCä¸€è‡´ï¼Œå®šä¹‰ç¼–ç ï¼Œå®šä¹‰é”™è¯¯æ¦‚ç‡ï¼Œå®šä¹‰å¯è¾¾åŒºåŸŸï¼Œè¯æ˜converseå’Œachievability

### Definition

å¯¹èƒ½é‡é™åˆ¶Pçš„ä¿¡é“å®šä¹‰ç¼–ç å‡½æ•°ã€‚

**Definition**. An $(M, n)$ code for the Gaussian channel with power constraint $P$ consists of the following:
1. An index set $\{1,2, \ldots, M\}$
2. An encoding function $x:\{1,2, \ldots, M\} \rightarrow X^{n}$, yielding codewords $x^{n}(1), x^{n}(2), \ldots$ $x^{n}(M),$ satisfying the power constraint $P$; that is, for every codeword
   $$\sum_{i=1}^{n} x_{i}^{2}(w) \leq n P, \quad w=1,2, \dots, M$$
3. A decoding function
   $$g: y^{n} \rightarrow\{1,2, \ldots, M\}$$

The arithmetic average of the probability of error is defined by
$$P_{e}^{(n)}=\frac{1}{2^{n R}} \sum \lambda_{i}$$

A rate $R$ is said to be achievable for a Gaussian channel with a power constraint $P$ if there exists a sequence of $\left(2^{n R}, n\right)$ codes with codewords satisfying the power constraint such that the maximal probability of error $\lambda^{(n)}$ tends to zero. The capacity of the channel is the supremum of the achievable rates.

### Code Construction

#### Generation of the codebook
We generate the codewords $\left(x_{1}, x_{2}, \ldots, x_{n}\right)$ with each element i.i.d. according to a normal distribution with variance $P-\epsilon .$ since for large $n$
$$
\frac{1}{n} \sum x_{i}^{2} \rightarrow P-\epsilon
$$
The probability that a codeword does not satisfy the power constraint will be small. æ ¹æ®æ–¹å·®çš„å®šä¹‰ï¼Œè¶…è¿‡èƒ½é‡é™åˆ¶çš„æ¦‚ç‡æ˜¯éå¸¸å°çš„ã€‚
Let $X_{i}(w), \quad i=1,2, \ldots, n, w=1,2, \ldots, 2^{n R}$ be i.i.d. $\sim \mathcal{N}(0, P-\epsilon),$ forming codewords $X^{n}(1), X^{n}(2), \ldots, X^{n}\left(2^{n R}\right) \in \mathcal{R}^{n}$ å®Œæˆäº†ç æœ¬çš„ç”Ÿæˆ
#### Encoding
- The codebook is revealed to both the sender and the receiver.
- To send the message index $w$, sends the $w$ th codeword $X^{n}(w)$ in the codebook.
#### Decoding:
æ ¹æ®è”åˆå…¸å‹æ€§è§£ç ï¼ŒThe receiver looks down the list of codewords $\left\{X^{n}(w)\right\}$ and searches for one that is jointly typical with the received vector.
- If there is one and only one such codeword $X^{n}(w)$, the receiver declares $\widehat{W}=w$ to be the transmitted codeword.
- Otherwise, the receiver declares an error. The receiver also declares an error **if the chosen codeword does not satisfy the power constraint.**ï¼ˆé™¤äº†ä¸æ»¡è¶³å…¸å‹æ€§ï¼Œé”™è¯¯è¿˜å¯èƒ½æ˜¯è¶…è¿‡äº†èƒ½é‡é™åˆ¶ï¼‰


### Probability of Error

WLOGï¼Œå‡è®¾æˆ‘ä»¬å‘é€äº†ç åˆ¶1.

Without loss of generality, assume that codeword 1 was sent. Thus,
$$
Y^{n}=X^{n}(1)+Z^{n}
$$
ç°åœ¨æˆ‘ä»¬è¦åˆ†æä¸¤ç§é™åˆ¶ã€‚Define the following events:
$$E_{0}=\left\{\frac{1}{n} \sum_{j=1}^{n} X_{j}^{2}(1)>P\right\}$$
and
$$E_{i}=\left\{\left(X^{n}(i), Y^{n}\right) \text { is in } A_{\epsilon}^{(n)}\right\}$$

$$\operatorname{Pr}(\mathcal{E} | W=1)=P\left(E_{0} \cup E_{1}^{c} \cup E_{2} \cup E_{3} \ldots \cup E_{2^{n R}}\right) \leq P\left(E_{0}\right)+P\left(E_{1}^{c}\right)+\sum_{i=2}^{2^{n R}} P\left(E_{i}\right)$$

$E_0$è¡¨ç¤ºè¿èƒŒäº†èƒ½é‡çº¦æŸã€‚

ä½†æ ¹æ®æˆ‘ä»¬å‰é¢çš„åˆ†æï¼Œ$P\left(E_{0}\right) \rightarrow 0$ï¼Œ $P\left(E_{1}^{c}\right) \leq \epsilon$ï¼Œåˆæ ¹æ®è”åˆå…¸å‹æ€§ï¼Œ

$$\sum_{i=2}^{2^{n R}} P\left(E_{i}\right)=\left(2^{n R}-1\right) 2^{-n(I(X ; Y)-3 \epsilon)} \leq 2^{-n(I(X ; Y)-R-3 \epsilon)}$$

æ‰€ä»¥æˆ‘ä»¬æœ‰$P_{e}^{(n)} \leq 3 \epsilon$ã€‚åˆ©ç”¨DMCä¸­ç±»ä¼¼çš„å‡ä¸€åŠæ–¹æ³•ï¼Œæˆ‘ä»¬å¯ä»¥è¯æ˜æœ€å¤§é”™è¯¯æ¦‚ç‡ä¹Ÿæ˜¯è¶‹å‘äº0çš„ã€‚


### Converse

Let $W$ be distributed uniformly over $\left\{1,2, \ldots, 2^{n R}\right\}$
$$
W \rightarrow X^{n}(W) \rightarrow Y^{n} \rightarrow \widehat{W}
$$

By Fano's inequality
$$H(W | \widehat{W}) \leq 1+n R P_{e}^{(n)}=n \epsilon_{n}$$
where $\epsilon_{n} \rightarrow 0$ as $P_{e}^{(n)} \rightarrow 0$

$$\begin{aligned}
n R &=H(W)=I(W ; \widehat{W})+H(W | \widehat{W}) \\
& \leq I(W ; \widehat{W})+n \epsilon_{n} \\
& \leq I\left(X^{n} ; Y^{n}\right)+n \epsilon_{n} \\
&=h\left(Y^{n}\right)-h\left(Y^{n} | X^{n}\right)+n \epsilon_{n} \\
&=h\left(Y^{n}\right)-h\left(Z^{n}\right)+n \epsilon_{n} \quad \text{å»é™¤ä¿¡å·}Y_n = X_n + Z_n \\
& \leq \sum_{i=1}^{n} h\left(Y_{i}\right)-\sum_{i=1}^{n} h\left(Z_{i}\right)+n \epsilon_{n}
\end{aligned}$$

Let $P_{i}$ be the average power of the $i$ th column of the codebook
$$
P_{i}=\frac{1}{2^{n R}} \sum_{w} x_{i}^{2}(w) \text { and } \frac{1}{n} \sum_{i} P_{i} \leq P
$$

since $X_{i}$ and $Z_{i}$ are independent, then å»ºç«‹å¾®åˆ†ç†µ$h(Y_i)$çš„ä¸Šç•Œ
$$
E Y_{i}^{2}=P_{i}+N, h\left(Y_{i}\right) \leq \frac{1}{2} \log 2 \pi e\left(P_{i}+N\right)
$$

$$\begin{aligned}
n R &\leq \sum_{i=1}^{n} h\left(Y_{i}\right)-\sum_{i=1}^{n} h\left(Z_{i}\right)+n \epsilon_{n} \\
&\leq \sum\left(\frac{1}{2} \log 2 \pi e\left(P_{i}+N\right)-\frac{1}{2} \log 2 \pi e N\right)+n \epsilon_{n} \\
&=\sum \frac{1}{2} \log 2 \pi e\left(1+\frac{P_{i}}{N}\right)+n \epsilon_{n}
\end{aligned}$$

åˆ©ç”¨å‡¹å‡½æ•°çš„æ€§è´¨ï¼Œ$f(x)= \frac{1}{2} \log (1+x) \text { is concave }$

$$\begin{aligned}
& \frac{1}{n} \sum_{i=1}^{n} \frac{1}{2} \log \left(1+\frac{P_{i}}{N}\right) \\
& \leq \frac{1}{2} \log \left(1+\frac{1}{n} \sum_{i=1}^{n} \frac{P_{i}}{N}\right) \leq \frac{1}{2} \log \left(1+\frac{P}{N}\right)
\end{aligned}$$

æ‰€ä»¥
$$R \leq \frac{1}{2} \log \left(1+\frac{P}{N}\right)+\epsilon_{n}$$

è¿™é‡Œæˆ‘ä»¬å€Ÿç”¨å‡¹å‡½æ•°çš„æ€§è´¨å¤„ç†äº†èƒ½é‡çš„é—®é¢˜ï¼Œå®Œæˆäº†converseçš„è¯æ˜ã€‚

## Parallel Gaussian Channel

### Problem

![](img/04-22-11-03-00.png)

é«˜æ–¯ä¿¡é“çš„æ‰©å±•ï¼šnä¸ªä¿¡é“ï¼Œå¯ä»¥åŒæ—¶ä½¿ç”¨ã€‚ä»ç„¶å­˜åœ¨èƒ½é‡çš„é™åˆ¶ã€‚

Assume that we have a set of Gaussian channels in parallel. The output of each channel is the sum of the input and Gaussian noise. For channel $j$
$$Y_{j}=X_{j}+Z_{j}, \quad j=1,2, \dots, k$$
The noise is assumed to be independent from channel to channel. We assume that there is a common power constraint on the total power used, that is
$$E \sum_{j=1}^{k} X_{j}^{2} \leq P$$
We wish to **distribute the power among the various channels** so as to maximize the total capacity. å°†é—®é¢˜æ›´ç»†åŒ–ä¸€æ­¥ï¼Œæˆ‘ä»¬éœ€è¦å¯¹å•ä¸ªä¿¡é“çš„èƒ½é‡åšä¸€å®šçš„åˆ†é…ã€‚
$$ P_{i}=E X_{i}^{2},\text{ and }\sum P_{i} \leq P$$

### Solution

$$\begin{aligned}
& I\left(X_{1}, X_{2}, \ldots, X_{k} ; Y_{1}, Y_{2}, \ldots, Y_{k}\right) \\
=& h\left(Y_{1}, Y_{2}, \ldots, Y_{k}\right)-h\left(Y_{1}, Y_{2}, \ldots, Y_{k} | X_{1}, X_{2}, \ldots, X_{k}\right) \\
=& h\left(Y_{1}, Y_{2}, \ldots, Y_{k}\right)-h\left(Z_{1}, Z_{2}, \ldots, Z_{k} | X_{1}, X_{2}, \ldots, X_{k}\right) \\
=& h\left(Y_{1}, Y_{2}, \ldots, Y_{k}\right)-h\left(Z_{1}, Z_{2}, \ldots, Z_{k}\right) \\
=& h\left(Y_{1}, Y_{2}, \ldots, Y_{k}\right)-\sum_{i} h\left(Z_{i}\right) \\
\leq & \sum h\left(Y_{i}\right)-h\left(Z_{i}\right) \\
\leq & \sum_{i} \frac{1}{2} \log \left(1+\frac{P_{i}}{N_{i}}\right) \quad \text{é«˜æ–¯å™ªå£°æœ€å¤§åŒ–å¾®åˆ†ç†µ}
\end{aligned}$$

where $P_{i}=E X_{i}^{2},$ and $\sum P_{i}=P .$ ç­‰å·æ˜¯å¯ä»¥å–åˆ°çš„ï¼ŒEquality is achieved by
$$
\left(X_{1}, X_{2}, \ldots, X_{k}\right) \sim \mathcal{N}\left(0,\left[\begin{array}{cccc}
P_{1} & 0 & \cdots & 0 \\
0 & P_{2} & \cdots & \vdots \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & P_{k}
\end{array}\right]\right)
$$

ä¼˜åŒ–ç›®æ ‡ï¼š
$$\begin{array}{c}
\max \sum_{i} \log \left(1+\frac{P_{i}}{N_{i}}\right) \\
\sum P_{i}=P
\end{array}$$

å‡¸ä¼˜åŒ–é—®é¢˜ï¼Œå¯è§£ã€‚æˆ‘ä»¬å‘ç°ï¼Œåœ¨å¹¶è¡Œé«˜æ–¯ä¿¡é“ä¸­ï¼Œæˆ‘ä»¬éœ€è¦ä¸“é—¨åˆ†é…èƒ½é‡ã€‚
Extensionï¼šåˆ†é…èƒ½é‡ç®—æ³•ï¼šwater-filling



## Worst Additive Noise

æ¨å¹¿åˆ°å™ªå£°çš„ä¸€èˆ¬åˆ†å¸ƒï¼Œä¿¡é“å®¹é‡å®šç†ä¾ç„¶æˆç«‹ã€‚

![](img/04-22-11-16-48.png)

### Problem

- Under the energy constraint $P,$ the channel capacity of additive channel $Y=X+Z$ is
  $$
  \begin{aligned}
  C(Z) &=\max _{X: E X^{2} \leq P} I(X ; Y) \\
  &=\max _{X: E X^{2} \leq P} h(X+Z)-h(Z)
  \end{aligned}
  $$
  åä¸€é¡¹éœ€è¦æ ¹æ®å…·ä½“æƒ…å†µå…·ä½“åˆ†æã€‚

- åŠ ä¸€ä¸ªé™åˆ¶ï¼ŸWhat is the minimum of $C(Z),$ if we could choose $Z: E Z^{2} \leq N$
  - That is, to play a max-min game between $X$ and $Z$ åœ¨Zç»™å®šçš„æƒ…å†µä¸‹è®¾å®šX
    $$\begin{aligned}
    \max _{Z: E Z^{2} \leq N} C(Z)&:=\min _{Z: E Z^{2} \leq N} \max _{X: E X^{2} \leq P} I(X ; X+Z)\\
    &=\min _{Z: E Z^{2} \leq N}\left(\max _{X: E X^{2} \leq P} I(X ; X+Z)\right)
    \end{aligned}
    $$
  - å¯¹å¤šé‡ä¼˜åŒ–é—®é¢˜ï¼ŒæŠŠå†…éƒ¨çœ‹æˆä¸€ä¸ªå‡½æ•°ï¼Œåˆ†ä¸¤æ­¥èµ°ã€‚We need to find a $Z^{*} .$ When $C\left(Z^{*}\right)$ is attained by $X^{*}$
    $$
    I\left(X^{*} ; X^{*}+Z^{*}\right) \leq \max _{X: E X^{2} \leq P} I(X ; X+Z)
    $$

- The $\min _{Z: E Z^{2} \leq N} C(Z)$ is attained iff $Z=Z_{G} \sim \mathcal{N}\left(0, \sigma^{2}\right)$ (Shannon, 1948)

### Entropy power inequality
åœ¨ç»™å®šä¿¡é“èƒ½é‡çš„æƒ…å†µä¸‹ï¼Œé«˜æ–¯å™ªå£°æ˜¯æœ€åçš„åŠ æ€§å™ªå£°ã€‚EPIï¼Œç†µå¹‚ä¸ç­‰å¼

> Entropy power inequality (EPI, Shannon 1948 ): If $X$ and $Y$ are independent random $n$ vectors with densities, then
> $$
> e^{\frac{2}{n} h(X+Y)} \geq e^{\frac{2}{n} h(X)}+e^{\frac{2}{n} h(Y)}
> $$
> è¯æ˜ç•¥

åˆ©ç”¨EPIï¼Œæˆ‘ä»¬è¯æ˜å‰ä¸€èŠ‚çš„å®šç†
- Recall $I(X ; X+Z)=h(X+Z)-h(Z)$
- By EPI, $h(X+Z) \geq \frac{1}{2} \log \left(e^{2 h(X)}+e^{2 h(Z)}\right)$
- æˆ‘ä»¬æœ‰$I(X ; X+Z) \geq \frac{1}{2} \log \left(e^{2 h(X)}+e^{2 h(Z)}\right)-h(Z)$
- $f(t, s)=\frac{1}{2} \log \left(e^{2 t}+e^{2 s}\right)-s,$ where

$$\begin{array}{l}
t=h(X) \leq \frac{1}{2} \log 2 \pi e P \\
s=h(Z) \leq \frac{1}{2} \log 2 \pi e N
\end{array}$$

- In $f(t, s)$ is increasing and convex in $t,$ and is decreasing and convex in $S$
- Fix $s, f(t, s)$ is maximized if $t=\frac{1}{2} \log 2 \pi e P$
- Fix $t, f(t, s)$ is minimized if $s=\frac{1}{2} \log 2 \pi e N$
- $X^{*} \sim \mathcal{N}(0, P), Z \sim \mathcal{N}\left(0, N^{*}\right)$

In Gaussian channel

$$I\left(X ; X+Z^{*}\right) \leq I\left(X^{*} ; X^{*}+Z^{*}\right)=C\left(Z^{*}\right)$$

$$\begin{aligned}
& I\left(X^{*} ; X^{*}+Z\right) \\
=& h\left(X^{*}+Z\right)-h(Z) \\
\geq & \frac{1}{2} \log \left(e^{2 h\left(X^{*}\right)}+e^{2 h(Z)}\right)-h(Z) \\
\geq & \min f(t, s) \\
=& I\left(X^{*} ; X^{*}+Z^{*}\right)
\end{aligned}$$

ç»¼åˆèµ·æ¥ï¼Œæˆ‘ä»¬è·å¾—äº†ä¸€ä¸ªä¸ç­‰å¼é“¾ï¼Œé«˜æ–¯åˆ†å¸ƒä½äºä¸­é—´ã€‚åŒæ—¶ä½œä¸ºä¸‹ç•Œå’Œä¸Šç•Œè€Œå­˜åœ¨ã€‚è¿™ä¹Ÿæ­ç¤ºäº†æˆ‘ä»¬åœ¨å¾ˆå¤šé—®é¢˜ä¸­ï¼Œä»¥é«˜æ–¯ä¿¡é“ä¸ºä¾‹çš„æ„ä¹‰ã€‚

$$\boldsymbol{I}\left(\boldsymbol{X} ; \boldsymbol{X}+\boldsymbol{Z}^{*}\right) \leq \boldsymbol{I}\left(\boldsymbol{X}^{*} ; \boldsymbol{X}^{*}+\boldsymbol{Z}^{*}\right) \leq \boldsymbol{I}\left(\boldsymbol{X}^{*} ; \boldsymbol{X}^{*}+\boldsymbol{Z}\right)$$

$$\begin{aligned}
\min _{Z} \max _{X} I(X ; X+Z) &=\max _{X} \min _{Z} I(X ; X+Z) \\
&=\frac{1}{2} \log \left(1+\frac{P}{N}\right)
\end{aligned}$$


å®Œç»“æ’’èŠ±~


